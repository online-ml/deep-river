{"config":{"lang":["en"],"separator":"[\\s\\u200b\\-_,:!=\\[\\]()\"`/]+|\\.(?!\\d)|&[lg]t;|(?!\\b)(?=[A-Z][a-z])","pipeline":["stopWordFilter"]},"docs":[{"location":"getting_started/","title":"Getting started","text":"<p>We build the development of neural networks on top of the river API and refer to the rivers design principles. The following example creates a simple MLP architecture based on PyTorch and incrementally predicts and trains on the website phishing dataset. For further examples check out the Documentation.</p>"},{"location":"getting_started/#installation","title":"\ud83d\udc88Installation","text":"<p>River is meant to work with Python 3.8 and above. Installation can be done via <code>pip</code>:</p> <p><pre><code>pip install deep-river\n</code></pre> or <pre><code>pip install \"river[deep]\"\n</code></pre></p> <p>You can install the latest development version from GitHub, as so:</p> <pre><code>pip install git+https://github.com/online-ml/deep-river --upgrade\n</code></pre> <p>Or, through SSH:</p> <pre><code>pip install git+ssh://git@github.com/online-ml/deep-river.git --upgrade\n</code></pre> <p>Feel welcome to open an issue on GitHub if you are having any trouble.</p>"},{"location":"getting_started/#usage","title":"\ud83d\udcbb Usage","text":""},{"location":"getting_started/#classification","title":"Classification","text":"<pre><code>&gt;&gt;&gt; from river import metrics, datasets, preprocessing, compose\n&gt;&gt;&gt; from deep_river import classification\n&gt;&gt;&gt; from torch import nn\n&gt;&gt;&gt; from torch import optim\n&gt;&gt;&gt; from torch import manual_seed\n\n&gt;&gt;&gt; _ = manual_seed(42)\n\n&gt;&gt;&gt; class MyModule(nn.Module):\n...     def __init__(self, n_features):\n...         super(MyModule, self).__init__()\n...         self.dense0 = nn.Linear(n_features, 5)\n...         self.nonlin = nn.ReLU()\n...         self.dense1 = nn.Linear(5, 2)\n...         self.softmax = nn.Softmax(dim=-1)\n...\n...     def forward(self, X, **kwargs):\n...         X = self.nonlin(self.dense0(X))\n...         X = self.nonlin(self.dense1(X))\n...         X = self.softmax(X)\n...         return X\n\n&gt;&gt;&gt; model_pipeline = compose.Pipeline(\n...     preprocessing.StandardScaler(),\n...     classification.Classifier(module=MyModule, loss_fn='binary_cross_entropy', optimizer_fn='adam')\n...     )\n\n&gt;&gt;&gt; dataset = datasets.Phishing()\n&gt;&gt;&gt; metric = metrics.Accuracy()\n\n&gt;&gt;&gt; for x, y in dataset:\n...     y_pred = model_pipeline.predict_one(x)  # make a prediction\n...     metric = metric.update(y, y_pred)  # update the metric\n...     model_pipeline = model_pipeline.learn_one(x, y)  # make the model learn\n&gt;&gt;&gt;     print(f\"Accuracy: {metric.get():.4f}\")\nAccuracy: 0.6728\n</code></pre>"},{"location":"getting_started/#regression","title":"Regression","text":"<pre><code>&gt;&gt;&gt; from river import metrics, compose, preprocessing, datasets\n&gt;&gt;&gt; from deep_river.regression import Regressor\n&gt;&gt;&gt; from torch import nn\n&gt;&gt;&gt; from pprint import pprint\n&gt;&gt;&gt; from tqdm import tqdm\n\n&gt;&gt;&gt; dataset = datasets.Bikes()\n&gt;&gt;&gt; metric = metrics.MAE()\n\n&gt;&gt;&gt; class MyModule(nn.Module):\n...     def __init__(self, n_features):\n...         super(MyModule, self).__init__()\n...         self.dense0 = nn.Linear(n_features, 5)\n...         self.nonlin = nn.ReLU()\n...         self.dense1 = nn.Linear(5, 1)\n...         self.softmax = nn.Softmax(dim=-1)\n...\n...     def forward(self, X, **kwargs):\n...         X = self.nonlin(self.dense0(X))\n...         X = self.nonlin(self.dense1(X))\n...         X = self.softmax(X)\n...         return X\n\n&gt;&gt;&gt; model_pipeline = compose.Select('clouds', 'humidity', 'pressure', 'temperature', 'wind')\n&gt;&gt;&gt; model_pipeline |= preprocessing.StandardScaler()\n&gt;&gt;&gt; model_pipeline |= Regressor(module=MyModule, loss_fn=\"mse\", optimizer_fn='sgd')\n&gt;&gt;&gt; for x, y in dataset.take(5000):\n...     y_pred = model_pipeline.predict_one(x)\n...     metric.update(y_true=y, y_pred=y_pred)\n...     model_pipeline.learn_one(x=x, y=y)\nprint(f'MAE: {metric.get():.2f}')\nMAE: 6.83\n</code></pre>"},{"location":"getting_started/#anomaly-detection","title":"Anomaly Detection","text":"<pre><code>&gt;&gt;&gt; from deep_river.anomaly import Autoencoder\n&gt;&gt;&gt; from river import metrics\n&gt;&gt;&gt; from river.datasets import CreditCard\n&gt;&gt;&gt; from torch import nn\n&gt;&gt;&gt; import math\n&gt;&gt;&gt; from river.compose import Pipeline\n&gt;&gt;&gt; from river.preprocessing import MinMaxScaler\n\n&gt;&gt;&gt; dataset = CreditCard().take(5000)\n&gt;&gt;&gt; metric = metrics.ROCAUC(n_thresholds=50)\n\n&gt;&gt;&gt; class MyAutoEncoder(nn.Module):\n\n...     def __init__(self, n_features, latent_dim=3):\n...         super(MyAutoEncoder, self).__init__()\n...         self.linear1 = nn.Linear(n_features, latent_dim)\n...         self.nonlin = nn.LeakyReLU()\n...         self.linear2 = nn.Linear(latent_dim, n_features)\n...         self.sigmoid = nn.Sigmoid()\n...\n...     def forward(self, X, **kwargs):\n...         X = self.linear1(X)\n...         X = self.nonlin(X)\n...         X = self.linear2(X)\n...         return self.sigmoid(X)\n\n&gt;&gt;&gt; ae = Autoencoder(module=MyAutoEncoder, lr=0.005)\n&gt;&gt;&gt; scaler = MinMaxScaler()\n&gt;&gt;&gt; model = Pipeline(scaler, ae)\n\n&gt;&gt;&gt; for x, y in dataset:\n...     score = model.score_one(x)\n...     model = model.learn_one(x=x)\n...     metric = metric.update(y, score)\n... \n\n&gt;&gt;&gt; print(f\"ROCAUC: {metric.get():.4f}\")\nROCAUC: 0.7447\n</code></pre>"},{"location":"benchmarks/","title":"Index","text":""},{"location":"benchmarks/#binary-classification","title":"Binary classification","text":""},{"location":"benchmarks/#bananas","title":"Bananas","text":""},{"location":"benchmarks/#elec2","title":"Elec2","text":""},{"location":"benchmarks/#phishing","title":"Phishing","text":""},{"location":"benchmarks/#datasets","title":"Datasets","text":"Bananas <pre>Bananas dataset.\n\nAn artificial dataset where instances belongs to several clusters with a banana shape.\nThere are two attributes that correspond to the x and y axis, respectively.\n\n    Name  Bananas                                                                                                        \n    Task  Binary classification                                                                                          \n Samples  5,300                                                                                                          \nFeatures  2                                                                                                              \n  Sparse  False                                                                                                          \n    Path  /Users/cedrickulbach/Documents/Projects/deep-river/.venv/lib/python3.10/site-packages/river/datasets/banana.zip</pre> Elec2 <pre>Electricity prices in New South Wales.\n\nThis is a binary classification task, where the goal is to predict if the price of electricity\nwill go up or down.\n\nThis data was collected from the Australian New South Wales Electricity Market. In this market,\nprices are not fixed and are affected by demand and supply of the market. They are set every\nfive minutes. Electricity transfers to/from the neighboring state of Victoria were done to\nalleviate fluctuations.\n\n      Name  Elec2                                                      \n      Task  Binary classification                                      \n   Samples  45,312                                                     \n  Features  8                                                          \n    Sparse  False                                                      \n      Path  /Users/cedrickulbach/river_data/Elec2/electricity.csv      \n       URL  https://maxhalford.github.io/files/datasets/electricity.zip\n      Size  2.95 MiB                                                   \nDownloaded  True                                                       </pre> Phishing <pre>Phishing websites.\n\nThis dataset contains features from web pages that are classified as phishing or not.\n\n    Name  Phishing                                                                                                            \n    Task  Binary classification                                                                                               \n Samples  1,250                                                                                                               \nFeatures  9                                                                                                                   \n  Sparse  False                                                                                                               \n    Path  /Users/cedrickulbach/Documents/Projects/deep-river/.venv/lib/python3.10/site-packages/river/datasets/phishing.csv.gz</pre>"},{"location":"benchmarks/#models","title":"Models","text":"Logistic regression <pre>Pipeline (\n  StandardScaler (\n    with_std=True\n  ),\n  LogisticRegression (\n    optimizer=SGD (\n      lr=Constant (\n        learning_rate=0.005\n      )\n    )\n    loss=Log (\n      weight_pos=1.\n      weight_neg=1.\n    )\n    l2=0.\n    l1=0.\n    intercept_init=0.\n    intercept_lr=Constant (\n      learning_rate=0.01\n    )\n    clip_gradient=1e+12\n    initializer=Zeros ()\n  )\n)</pre> Deep River Logistic <pre>Pipeline (\n  StandardScaler (\n    with_std=True\n  ),\n  LogisticRegressionInitialized (\n    n_features=10\n    n_init_classes=2\n    loss_fn=\"cross_entropy\"\n    optimizer_fn=\"sgd\"\n    lr=0.005\n    output_is_logit=True\n    is_feature_incremental=True\n    is_class_incremental=True\n    device=\"cpu\"\n    seed=42\n    gradient_clip_value=None\n  )\n)</pre> Deep River MLP <pre>Pipeline (\n  StandardScaler (\n    with_std=True\n  ),\n  MultiLayerPerceptronInitialized (\n    n_features=10\n    n_width=5\n    n_layers=5\n    n_init_classes=2\n    loss_fn=\"cross_entropy\"\n    optimizer_fn=\"sgd\"\n    lr=0.005\n    output_is_logit=True\n    is_feature_incremental=True\n    is_class_incremental=True\n    device=\"cpu\"\n    seed=42\n    gradient_clip_value=None\n  )\n)</pre> Deep River LSTM <pre>Pipeline (\n  StandardScaler (\n    with_std=True\n  ),\n  LSTMClassifier (\n    n_features=10\n    hidden_size=32\n    n_init_classes=2\n    loss_fn=\"cross_entropy\"\n    optimizer_fn=\"adam\"\n    lr=0.001\n    output_is_logit=True\n    is_feature_incremental=True\n    is_class_incremental=True\n    device=\"cpu\"\n    seed=42\n    gradient_clip_value=None\n  )\n)</pre> Deep River RNN <pre>Pipeline (\n  StandardScaler (\n    with_std=True\n  ),\n  RNNClassifier (\n    n_features=10\n    hidden_size=32\n    num_layers=1\n    nonlinearity=\"tanh\"\n    n_init_classes=2\n    loss_fn=\"cross_entropy\"\n    optimizer_fn=\"adam\"\n    lr=0.001\n    output_is_logit=True\n    is_feature_incremental=True\n    is_class_incremental=True\n    device=\"cpu\"\n    seed=42\n    gradient_clip_value=None\n  )\n)</pre> [baseline] Prior class <pre>PriorClassifier ()</pre>"},{"location":"benchmarks/#multiclass-classification","title":"Multiclass classification","text":""},{"location":"benchmarks/#hyperplane-limited-5000","title":"Hyperplane (limited 5000)","text":""},{"location":"benchmarks/#led-limited-5000","title":"LED (limited 5000)","text":""},{"location":"benchmarks/#randomrbf-limited-5000","title":"RandomRBF (limited 5000)","text":""},{"location":"benchmarks/#datasets_1","title":"Datasets","text":"Hyperplane (limited 5000) <pre>Hyperplane(limited n=5000)</pre> LED (limited 5000) <pre>LED(limited n=5000)</pre> RandomRBF (limited 5000) <pre>RandomRBF(limited n=5000)</pre>"},{"location":"benchmarks/#models_1","title":"Models","text":"Logistic regression <pre>Pipeline (\n  StandardScaler (\n    with_std=True\n  ),\n  LogisticRegression (\n    optimizer=SGD (\n      lr=Constant (\n        learning_rate=0.005\n      )\n    )\n    loss=Log (\n      weight_pos=1.\n      weight_neg=1.\n    )\n    l2=0.\n    l1=0.\n    intercept_init=0.\n    intercept_lr=Constant (\n      learning_rate=0.01\n    )\n    clip_gradient=1e+12\n    initializer=Zeros ()\n  )\n)</pre> Deep River Logistic <pre>Pipeline (\n  StandardScaler (\n    with_std=True\n  ),\n  LogisticRegressionInitialized (\n    n_features=10\n    n_init_classes=2\n    loss_fn=\"cross_entropy\"\n    optimizer_fn=\"sgd\"\n    lr=0.005\n    output_is_logit=True\n    is_feature_incremental=True\n    is_class_incremental=True\n    device=\"cpu\"\n    seed=42\n    gradient_clip_value=None\n  )\n)</pre> Deep River MLP <pre>Pipeline (\n  StandardScaler (\n    with_std=True\n  ),\n  MultiLayerPerceptronInitialized (\n    n_features=10\n    n_width=5\n    n_layers=5\n    n_init_classes=2\n    loss_fn=\"cross_entropy\"\n    optimizer_fn=\"sgd\"\n    lr=0.005\n    output_is_logit=True\n    is_feature_incremental=True\n    is_class_incremental=True\n    device=\"cpu\"\n    seed=42\n    gradient_clip_value=None\n  )\n)</pre> Deep River LSTM <pre>Pipeline (\n  StandardScaler (\n    with_std=True\n  ),\n  LSTMClassifier (\n    n_features=10\n    hidden_size=32\n    n_init_classes=2\n    loss_fn=\"cross_entropy\"\n    optimizer_fn=\"adam\"\n    lr=0.001\n    output_is_logit=True\n    is_feature_incremental=True\n    is_class_incremental=True\n    device=\"cpu\"\n    seed=42\n    gradient_clip_value=None\n  )\n)</pre> Deep River RNN <pre>Pipeline (\n  StandardScaler (\n    with_std=True\n  ),\n  RNNClassifier (\n    n_features=10\n    hidden_size=32\n    num_layers=1\n    nonlinearity=\"tanh\"\n    n_init_classes=2\n    loss_fn=\"cross_entropy\"\n    optimizer_fn=\"adam\"\n    lr=0.001\n    output_is_logit=True\n    is_feature_incremental=True\n    is_class_incremental=True\n    device=\"cpu\"\n    seed=42\n    gradient_clip_value=None\n  )\n)</pre> [baseline] Last Class <pre>NoChangeClassifier ()</pre> [baseline] Prior Class <pre>PriorClassifier ()</pre>"},{"location":"benchmarks/#regression","title":"Regression","text":""},{"location":"benchmarks/#chickweights","title":"ChickWeights","text":""},{"location":"benchmarks/#trumpapproval","title":"TrumpApproval","text":""},{"location":"benchmarks/#datasets_2","title":"Datasets","text":"ChickWeights <pre>Chick weights along time.\n\nThe stream contains 578 items and 3 features. The goal is to predict the weight of each chick\nalong time, according to the diet the chick is on. The data is ordered by time and then by\nchick.\n\n    Name  ChickWeights                                                                                                          \n    Task  Regression                                                                                                            \n Samples  578                                                                                                                   \nFeatures  3                                                                                                                     \n  Sparse  False                                                                                                                 \n    Path  /Users/cedrickulbach/Documents/Projects/deep-river/.venv/lib/python3.10/site-packages/river/datasets/chick-weights.csv</pre> TrumpApproval <pre>Donald Trump approval ratings.\n\nThis dataset was obtained by reshaping the data used by FiveThirtyEight for analyzing Donald\nTrump's approval ratings. It contains 5 features, which are approval ratings collected by\n5 polling agencies. The target is the approval rating from FiveThirtyEight's model. The goal of\nthis task is to see if we can reproduce FiveThirtyEight's model.\n\n    Name  TrumpApproval                                                                                                             \n    Task  Regression                                                                                                                \n Samples  1,001                                                                                                                     \nFeatures  6                                                                                                                         \n  Sparse  False                                                                                                                     \n    Path  /Users/cedrickulbach/Documents/Projects/deep-river/.venv/lib/python3.10/site-packages/river/datasets/trump_approval.csv.gz</pre>"},{"location":"benchmarks/#models_2","title":"Models","text":"Linear regression <pre>Pipeline (\n  StandardScaler (\n    with_std=True\n  ),\n  LinearRegression (\n    optimizer=SGD (\n      lr=Constant (\n        learning_rate=0.005\n      )\n    )\n    loss=Squared ()\n    l2=0.\n    l1=0.\n    intercept_init=0.\n    intercept_lr=Constant (\n      learning_rate=0.01\n    )\n    clip_gradient=1e+12\n    initializer=Zeros ()\n  )\n)</pre> Deep River Linear <pre>Pipeline (\n  StandardScaler (\n    with_std=True\n  ),\n  LinearRegressionInitialized (\n    n_features=10\n    loss_fn=\"mse\"\n    optimizer_fn=\"sgd\"\n    lr=0.005\n    is_feature_incremental=True\n    device=\"cpu\"\n    seed=42\n    gradient_clip_value=None\n  )\n)</pre> Deep River MLP <pre>Pipeline (\n  StandardScaler (\n    with_std=True\n  ),\n  MultiLayerPerceptron (\n    n_features=10\n    n_width=5\n    n_layers=5\n    loss_fn=\"mse\"\n    optimizer_fn=\"sgd\"\n    lr=0.005\n    is_feature_incremental=True\n    device=\"cpu\"\n    seed=42\n    gradient_clip_value=None\n  )\n)</pre> Deep River LSTM <pre>Pipeline (\n  StandardScaler (\n    with_std=True\n  ),\n  LSTMRegressor (\n    n_features=10\n    hidden_size=64\n    num_layers=1\n    dropout=0.1\n    gradient_clip_value=1.\n    loss_fn=\"mse\"\n    optimizer_fn=\"adam\"\n    lr=0.001\n    is_feature_incremental=True\n    device=\"cpu\"\n    seed=42\n  )\n)</pre> Deep River RNN <pre>Pipeline (\n  StandardScaler (\n    with_std=True\n  ),\n  RNNRegressor (\n    n_features=10\n    hidden_size=64\n    num_layers=1\n    nonlinearity=\"tanh\"\n    dropout=0.1\n    gradient_clip_value=1.\n    loss_fn=\"mse\"\n    optimizer_fn=\"adam\"\n    lr=0.001\n    is_feature_incremental=True\n    device=\"cpu\"\n    seed=42\n  )\n)</pre> River MLP <pre>Pipeline (\n  StandardScaler (\n    with_std=True\n  ),\n  MLPRegressor (\n    hidden_dims=(10,)\n    activations=(&lt;class 'river.neural_net.activations.ReLU'&gt;, &lt;class 'river.neural_net.activations.ReLU'&gt;, &lt;class 'river.neural_net.activations.Identity'&gt;)\n    loss=Squared ()\n    optimizer=SGD (\n      lr=Constant (\n        learning_rate=0.005\n      )\n    )\n    seed=42\n  )\n)</pre> [baseline] Mean predictor <pre>StatisticRegressor (\n  statistic=Mean ()\n)</pre>"},{"location":"benchmarks/#environment","title":"Environment","text":"<pre>Python implementation: CPython\nPython version       : 3.10.11\nIPython version      : 8.37.0\n\nriver       : 0.22.0\nnumpy       : 1.26.4\nscikit-learn: 1.5.2\npandas      : 2.2.3\nscipy       : 1.15.3\nplotly      : 6.3.1\n\nCompiler    : Clang 13.0.0 (clang-1300.0.29.30)\nOS          : Darwin\nRelease     : 25.1.0\nMachine     : x86_64\nProcessor   : i386\nCPU cores   : 16\nArchitecture: 64bit\n</pre>"},{"location":"benchmarks/Binary%20classification/","title":"Binary classification","text":""},{"location":"benchmarks/Binary%20classification/#bananas","title":"Bananas","text":""},{"location":"benchmarks/Binary%20classification/#summary","title":"Summary","text":"Model Accuracy F1 Memory in Mb Time in s Deep River LSTM 0.606604 0.36219 0.0321245 1183.4 Deep River Logistic 0.527925 0.222981 0.019639 151.192 Deep River MLP 0.548113 0.101313 0.0418444 278.597 Deep River RNN 0.541321 0.271938 0.0323954 444.79 Logistic regression 0.543208 0.197015 0.00424099 16.6572 [baseline] Prior class 0.551236 0.00335289 0.000611305 7.11342"},{"location":"benchmarks/Binary%20classification/#charts","title":"Charts","text":""},{"location":"benchmarks/Binary%20classification/#elec2","title":"Elec2","text":""},{"location":"benchmarks/Binary%20classification/#summary_1","title":"Summary","text":"Model Accuracy F1 Memory in Mb Time in s Deep River LSTM 0.835751 0.79717 0.0359993 3685.53 Deep River Logistic 0.837372 0.799445 0.0213318 1308.76 Deep River MLP 0.582715 0.291357 0.0435371 2416.23 Deep River RNN 0.833267 0.799565 0.0357666 3638.42 Logistic regression 0.822144 0.777086 0.005373 201.865 [baseline] Prior class 0.575335 0.00248834 0.000611305 86.623"},{"location":"benchmarks/Binary%20classification/#charts_1","title":"Charts","text":""},{"location":"benchmarks/Binary%20classification/#phishing","title":"Phishing","text":""},{"location":"benchmarks/Binary%20classification/#summary_2","title":"Summary","text":"Model Accuracy F1 Memory in Mb Time in s Deep River LSTM 0.8696 0.843118 0.0370378 383.293 Deep River Logistic 0.8488 0.832 0.0215311 45.0264 Deep River MLP 0.5528 0.231087 0.0437365 71.7688 Deep River RNN 0.8784 0.86055 0.0373087 110.05 Logistic regression 0.8872 0.871233 0.00556469 6.14968 [baseline] Prior class 0.554844 0.0794702 0.000611305 2.30399"},{"location":"benchmarks/Binary%20classification/#charts_2","title":"Charts","text":""},{"location":"benchmarks/Binary%20classification/#datasets","title":"Datasets","text":"Bananas <p>Bananas dataset.</p> <p>An artificial dataset where instances belongs to several clusters with a banana shape. There are two attributes that correspond to the x and y axis, respectively.</p> <pre><code>Name  Bananas                                                                                                        \nTask  Binary classification\n</code></pre> <p>Samples  5,300                                                                                                         Features  2                                                                                                               Sparse  False                                                                                                             Path  /Users/cedrickulbach/Documents/Projects/deep-river/.venv/lib/python3.10/site-packages/river/datasets/banana.zip</p> <p></p> Elec2 <p>Electricity prices in New South Wales.</p> <p>This is a binary classification task, where the goal is to predict if the price of electricity will go up or down.</p> <p>This data was collected from the Australian New South Wales Electricity Market. In this market, prices are not fixed and are affected by demand and supply of the market. They are set every five minutes. Electricity transfers to/from the neighboring state of Victoria were done to alleviate fluctuations.</p> <pre><code>  Name  Elec2                                                      \n  Task  Binary classification\n</code></pre> <p>Samples  45,312                                                      Features  8                                                             Sparse  False                                                           Path  /Users/cedrickulbach/river_data/Elec2/electricity.csv            URL  https://maxhalford.github.io/files/datasets/electricity.zip       Size  2.95 MiB                                                  Downloaded  True                                                       </p> <p></p> Phishing <p>Phishing websites.</p> <p>This dataset contains features from web pages that are classified as phishing or not.</p> <pre><code>Name  Phishing                                                                                                            \nTask  Binary classification\n</code></pre> <p>Samples  1,250                                                                                                              Features  9                                                                                                                    Sparse  False                                                                                                                  Path  /Users/cedrickulbach/Documents/Projects/deep-river/.venv/lib/python3.10/site-packages/river/datasets/phishing.csv.gz</p> <p></p>"},{"location":"benchmarks/Binary%20classification/#models","title":"Models","text":"Logistic regression <p><pre>Pipeline (\n  StandardScaler (\n    with_std=True\n  ),\n  LogisticRegression (\n    optimizer=SGD (\n      lr=Constant (\n        learning_rate=0.005\n      )\n    )\n    loss=Log (\n      weight_pos=1.\n      weight_neg=1.\n    )\n    l2=0.\n    l1=0.\n    intercept_init=0.\n    intercept_lr=Constant (\n      learning_rate=0.01\n    )\n    clip_gradient=1e+12\n    initializer=Zeros ()\n  )\n)</pre></p> <p></p> Deep River Logistic <p><pre>Pipeline (\n  StandardScaler (\n    with_std=True\n  ),\n  LogisticRegressionInitialized (\n    n_features=10\n    n_init_classes=2\n    loss_fn=\"cross_entropy\"\n    optimizer_fn=\"sgd\"\n    lr=0.005\n    output_is_logit=True\n    is_feature_incremental=True\n    is_class_incremental=True\n    device=\"cpu\"\n    seed=42\n    gradient_clip_value=None\n  )\n)</pre></p> <p></p> Deep River MLP <p><pre>Pipeline (\n  StandardScaler (\n    with_std=True\n  ),\n  MultiLayerPerceptronInitialized (\n    n_features=10\n    n_width=5\n    n_layers=5\n    n_init_classes=2\n    loss_fn=\"cross_entropy\"\n    optimizer_fn=\"sgd\"\n    lr=0.005\n    output_is_logit=True\n    is_feature_incremental=True\n    is_class_incremental=True\n    device=\"cpu\"\n    seed=42\n    gradient_clip_value=None\n  )\n)</pre></p> <p></p> Deep River LSTM <p><pre>Pipeline (\n  StandardScaler (\n    with_std=True\n  ),\n  LSTMClassifier (\n    n_features=10\n    hidden_size=32\n    n_init_classes=2\n    loss_fn=\"cross_entropy\"\n    optimizer_fn=\"adam\"\n    lr=0.001\n    output_is_logit=True\n    is_feature_incremental=True\n    is_class_incremental=True\n    device=\"cpu\"\n    seed=42\n    gradient_clip_value=None\n  )\n)</pre></p> <p></p> Deep River RNN <p><pre>Pipeline (\n  StandardScaler (\n    with_std=True\n  ),\n  RNNClassifier (\n    n_features=10\n    hidden_size=32\n    num_layers=1\n    nonlinearity=\"tanh\"\n    n_init_classes=2\n    loss_fn=\"cross_entropy\"\n    optimizer_fn=\"adam\"\n    lr=0.001\n    output_is_logit=True\n    is_feature_incremental=True\n    is_class_incremental=True\n    device=\"cpu\"\n    seed=42\n    gradient_clip_value=None\n  )\n)</pre></p> <p></p> [baseline] Prior class <p><pre>PriorClassifier ()</pre></p> <p></p>"},{"location":"benchmarks/Binary%20classification/#environment","title":"Environment","text":"<pre>Python implementation: CPython\nPython version       : 3.12.12\nIPython version      : 9.6.0\n\nriver       : 0.22.0\nnumpy       : 1.26.4\nscikit-learn: 1.5.2\npandas      : 2.2.3\nscipy       : 1.16.2\n\nCompiler    : Clang 21.1.4 \nOS          : Linux\nRelease     : 6.11.0-1018-azure\nMachine     : x86_64\nProcessor   : x86_64\nCPU cores   : 4\nArchitecture: 64bit\n</pre>"},{"location":"benchmarks/Multiclass%20classification/","title":"Multiclass classification","text":""},{"location":"benchmarks/Multiclass%20classification/#hyperplane-limited-5000","title":"Hyperplane (limited 5000)","text":""},{"location":"benchmarks/Multiclass%20classification/#summary","title":"Summary","text":"Model Accuracy MicroF1 MacroF1 Memory in Mb Time in s Deep River LSTM 0.8882 0.8882 0.888163 0.048192 1192.18 Deep River Logistic 0.9084 0.9084 0.908396 0.0271883 157.744 Deep River MLP 0.5004 0.5004 0.498917 0.049367 291.074 Logistic regression 0.9108 0.9108 0.9108 0.00967312 36.7864 [baseline] Last Class 0.503301 0.503301 0.503278 0.000510216 14.4975 [baseline] Prior Class 0.494699 0.494699 0.49095 0.000611305 14.912"},{"location":"benchmarks/Multiclass%20classification/#charts","title":"Charts","text":""},{"location":"benchmarks/Multiclass%20classification/#led-limited-5000","title":"LED (limited 5000)","text":""},{"location":"benchmarks/Multiclass%20classification/#summary_1","title":"Summary","text":"Model Accuracy MicroF1 MacroF1 Memory in Mb Time in s Deep River LSTM 0.338 0.338 0.328551 0.03547 1147.47 Deep River Logistic 0.3606 0.3606 0.31801 0.0215616 148.098 Deep River MLP 0.1048 0.1048 0.0525165 0.0437403 279.831 Deep River RNN 0.3524 0.3524 0.313258 0.0357409 433.301 Logistic regression 0.0928 0.0928 0.016987 0.00505733 18.9079 [baseline] Last Class 0.0980196 0.0980196 0.0975498 0.00121212 7.60275 [baseline] Prior Class 0.105421 0.105421 0.0468459 0.00116062 8.76123"},{"location":"benchmarks/Multiclass%20classification/#charts_1","title":"Charts","text":""},{"location":"benchmarks/Multiclass%20classification/#randomrbf-limited-5000","title":"RandomRBF (limited 5000)","text":""},{"location":"benchmarks/Multiclass%20classification/#summary_2","title":"Summary","text":"Model Accuracy MicroF1 MacroF1 Memory in Mb Time in s Deep River LSTM 0.5368 0.5368 0.489181 0.0331888 1075.22 Deep River Logistic 0.5066 0.5066 0.314396 0.0202723 146.062 Deep River MLP 0.3482 0.3482 0.112211 0.0424776 282.092 Deep River RNN 0.5136 0.5136 0.353501 0.0334597 423.671 Logistic regression 0.3628 0.3628 0.157195 0.00439358 17.8173 [baseline] Last Class 0.276855 0.276855 0.243844 0.000563622 7.77983 [baseline] Prior Class 0.34907 0.34907 0.13395 0.000718117 8.54999"},{"location":"benchmarks/Multiclass%20classification/#charts_2","title":"Charts","text":""},{"location":"benchmarks/Multiclass%20classification/#datasets","title":"Datasets","text":"Hyperplane (limited 5000) <p>Hyperplane(limited n=5000)</p> <p></p> LED (limited 5000) <p>LED(limited n=5000)</p> <p></p> RandomRBF (limited 5000) <p>RandomRBF(limited n=5000)</p> <p></p>"},{"location":"benchmarks/Multiclass%20classification/#models","title":"Models","text":"Logistic regression <p><pre>Pipeline (\n  StandardScaler (\n    with_std=True\n  ),\n  LogisticRegression (\n    optimizer=SGD (\n      lr=Constant (\n        learning_rate=0.005\n      )\n    )\n    loss=Log (\n      weight_pos=1.\n      weight_neg=1.\n    )\n    l2=0.\n    l1=0.\n    intercept_init=0.\n    intercept_lr=Constant (\n      learning_rate=0.01\n    )\n    clip_gradient=1e+12\n    initializer=Zeros ()\n  )\n)</pre></p> <p></p> Deep River Logistic <p><pre>Pipeline (\n  StandardScaler (\n    with_std=True\n  ),\n  LogisticRegressionInitialized (\n    n_features=10\n    n_init_classes=2\n    loss_fn=\"cross_entropy\"\n    optimizer_fn=\"sgd\"\n    lr=0.005\n    output_is_logit=True\n    is_feature_incremental=True\n    is_class_incremental=True\n    device=\"cpu\"\n    seed=42\n    gradient_clip_value=None\n  )\n)</pre></p> <p></p> Deep River MLP <p><pre>Pipeline (\n  StandardScaler (\n    with_std=True\n  ),\n  MultiLayerPerceptronInitialized (\n    n_features=10\n    n_width=5\n    n_layers=5\n    n_init_classes=2\n    loss_fn=\"cross_entropy\"\n    optimizer_fn=\"sgd\"\n    lr=0.005\n    output_is_logit=True\n    is_feature_incremental=True\n    is_class_incremental=True\n    device=\"cpu\"\n    seed=42\n    gradient_clip_value=None\n  )\n)</pre></p> <p></p> Deep River LSTM <p><pre>Pipeline (\n  StandardScaler (\n    with_std=True\n  ),\n  LSTMClassifier (\n    n_features=10\n    hidden_size=32\n    n_init_classes=2\n    loss_fn=\"cross_entropy\"\n    optimizer_fn=\"adam\"\n    lr=0.001\n    output_is_logit=True\n    is_feature_incremental=True\n    is_class_incremental=True\n    device=\"cpu\"\n    seed=42\n    gradient_clip_value=None\n  )\n)</pre></p> <p></p> Deep River RNN <p><pre>Pipeline (\n  StandardScaler (\n    with_std=True\n  ),\n  RNNClassifier (\n    n_features=10\n    hidden_size=32\n    num_layers=1\n    nonlinearity=\"tanh\"\n    n_init_classes=2\n    loss_fn=\"cross_entropy\"\n    optimizer_fn=\"adam\"\n    lr=0.001\n    output_is_logit=True\n    is_feature_incremental=True\n    is_class_incremental=True\n    device=\"cpu\"\n    seed=42\n    gradient_clip_value=None\n  )\n)</pre></p> <p></p> [baseline] Last Class <p><pre>NoChangeClassifier ()</pre></p> <p></p> [baseline] Prior Class <p><pre>PriorClassifier ()</pre></p> <p></p>"},{"location":"benchmarks/Multiclass%20classification/#environment","title":"Environment","text":"<pre>Python implementation: CPython\nPython version       : 3.12.12\nIPython version      : 9.6.0\n\nriver       : 0.22.0\nnumpy       : 1.26.4\nscikit-learn: 1.5.2\npandas      : 2.2.3\nscipy       : 1.16.2\n\nCompiler    : Clang 21.1.4 \nOS          : Linux\nRelease     : 6.11.0-1018-azure\nMachine     : x86_64\nProcessor   : x86_64\nCPU cores   : 4\nArchitecture: 64bit\n</pre>"},{"location":"benchmarks/Regression/","title":"Regression","text":""},{"location":"benchmarks/Regression/#chickweights","title":"ChickWeights","text":""},{"location":"benchmarks/Regression/#summary","title":"Summary","text":"Model MAE RMSE R2 Memory in Mb Time in s Deep River Attention 79.3499 93.1322 -0.720105 0.0645237 134.626 Deep River LSTM 100.133 117.539 -1.7398 0.0351133 138.265 Deep River Linear 119.58 138.47 -2.80249 0.0177469 20.0831 Deep River MLP 24.8823 38.4367 0.707013 0.0374966 29.0197 Deep River RNN 100.316 117.739 -1.74914 0.0329514 54.5518 Linear regression 24.488 37.1301 0.726594 0.00423336 2.12884 River MLP 121.818 141.004 -2.94294 0.0126944 21.2278 [baseline] Mean predictor 50.2509 71.1144 -0.00292947 0.000490189 0.755343"},{"location":"benchmarks/Regression/#charts","title":"Charts","text":""},{"location":"benchmarks/Regression/#trumpapproval","title":"TrumpApproval","text":""},{"location":"benchmarks/Regression/#summary_1","title":"Summary","text":"Model MAE RMSE R2 Memory in Mb Time in s Deep River Attention 6.49609 13.1981 -58.5337 0.067049 189.508 Deep River LSTM 10.7062 16.3447 -90.3047 0.0376387 194.99 Deep River Linear 36.6534 36.7298 -460.078 0.0187769 31.5477 Deep River MLP 1.31522 4.85733 -7.06366 0.0385265 45.7201 Deep River RNN 10.936 16.4744 -91.7587 0.0354767 88.6672 Linear regression 1.62028 4.53607 -6.0323 0.0049963 3.96432 River MLP 5.50408 10.9984 -40.3426 0.0139608 37.746 [baseline] Mean predictor 1.56755 2.20286 -0.658483 0.000490189 1.44011"},{"location":"benchmarks/Regression/#charts_1","title":"Charts","text":""},{"location":"benchmarks/Regression/#datasets","title":"Datasets","text":"ChickWeights <p>Chick weights along time.</p> <p>The stream contains 578 items and 3 features. The goal is to predict the weight of each chick along time, according to the diet the chick is on. The data is ordered by time and then by chick.</p> <pre><code>Name  ChickWeights                                                                                                          \nTask  Regression\n</code></pre> <p>Samples  578                                                                                                                  Features  3                                                                                                                      Sparse  False                                                                                                                    Path  /Users/cedrickulbach/Documents/Projects/deep-river/.venv/lib/python3.10/site-packages/river/datasets/chick-weights.csv</p> <p></p> TrumpApproval <p>Donald Trump approval ratings.</p> <p>This dataset was obtained by reshaping the data used by FiveThirtyEight for analyzing Donald Trump's approval ratings. It contains 5 features, which are approval ratings collected by 5 polling agencies. The target is the approval rating from FiveThirtyEight's model. The goal of this task is to see if we can reproduce FiveThirtyEight's model.</p> <pre><code>Name  TrumpApproval                                                                                                             \nTask  Regression\n</code></pre> <p>Samples  1,001                                                                                                                    Features  6                                                                                                                          Sparse  False                                                                                                                        Path  /Users/cedrickulbach/Documents/Projects/deep-river/.venv/lib/python3.10/site-packages/river/datasets/trump_approval.csv.gz</p> <p></p>"},{"location":"benchmarks/Regression/#models","title":"Models","text":"Linear regression <p><pre>Pipeline (\n  StandardScaler (\n    with_std=True\n  ),\n  LinearRegression (\n    optimizer=SGD (\n      lr=Constant (\n        learning_rate=0.005\n      )\n    )\n    loss=Squared ()\n    l2=0.\n    l1=0.\n    intercept_init=0.\n    intercept_lr=Constant (\n      learning_rate=0.01\n    )\n    clip_gradient=1e+12\n    initializer=Zeros ()\n  )\n)</pre></p> <p></p> Deep River Linear <p><pre>Pipeline (\n  StandardScaler (\n    with_std=True\n  ),\n  LinearRegressionInitialized (\n    n_features=10\n    loss_fn=\"mse\"\n    optimizer_fn=\"sgd\"\n    lr=0.005\n    is_feature_incremental=True\n    device=\"cpu\"\n    seed=42\n    gradient_clip_value=None\n  )\n)</pre></p> <p></p> Deep River MLP <p><pre>Pipeline (\n  StandardScaler (\n    with_std=True\n  ),\n  MultiLayerPerceptron (\n    n_features=10\n    n_width=5\n    n_layers=5\n    loss_fn=\"mse\"\n    optimizer_fn=\"sgd\"\n    lr=0.005\n    is_feature_incremental=True\n    device=\"cpu\"\n    seed=42\n    gradient_clip_value=None\n  )\n)</pre></p> <p></p> Deep River LSTM <p><pre>Pipeline (\n  StandardScaler (\n    with_std=True\n  ),\n  LSTMRegressor (\n    n_features=10\n    hidden_size=64\n    num_layers=1\n    dropout=0.1\n    gradient_clip_value=1.\n    loss_fn=\"mse\"\n    optimizer_fn=\"adam\"\n    lr=0.001\n    is_feature_incremental=True\n    device=\"cpu\"\n    seed=42\n  )\n)</pre></p> <p></p> Deep River RNN <p><pre>Pipeline (\n  StandardScaler (\n    with_std=True\n  ),\n  RNNRegressor (\n    n_features=10\n    hidden_size=64\n    num_layers=1\n    nonlinearity=\"tanh\"\n    dropout=0.1\n    gradient_clip_value=1.\n    loss_fn=\"mse\"\n    optimizer_fn=\"adam\"\n    lr=0.001\n    is_feature_incremental=True\n    device=\"cpu\"\n    seed=42\n  )\n)</pre></p> <p></p> River MLP <p><pre>Pipeline (\n  StandardScaler (\n    with_std=True\n  ),\n  MLPRegressor (\n    hidden_dims=(10,)\n    activations=(, , )\n    loss=Squared ()\n    optimizer=SGD (\n      lr=Constant (\n        learning_rate=0.005\n      )\n    )\n    seed=42\n  )\n)\n\n<p></p>\n\n[baseline] Mean predictor\n<p><pre>StatisticRegressor (\n  statistic=Mean ()\n)</pre></p>\n\n<p></p>"},{"location":"benchmarks/Regression/#environment","title":"Environment","text":"<pre>Python implementation: CPython\nPython version       : 3.12.12\nIPython version      : 9.6.0\n\nriver       : 0.22.0\nnumpy       : 1.26.4\nscikit-learn: 1.5.2\npandas      : 2.2.3\nscipy       : 1.16.2\n\nCompiler    : Clang 21.1.4 \nOS          : Linux\nRelease     : 6.11.0-1018-azure\nMachine     : x86_64\nProcessor   : x86_64\nCPU cores   : 4\nArchitecture: 64bit\n</pre>"},{"location":"examples/anomaly/example_autoencoder/","title":"Example autoencoder","text":"In\u00a0[1]: Copied! <pre>from river import compose, preprocessing, metrics, datasets\nfrom deep_river.anomaly import Autoencoder\nfrom torch import nn, manual_seed\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\n</pre> from river import compose, preprocessing, metrics, datasets from deep_river.anomaly import Autoencoder from torch import nn, manual_seed import numpy as np import matplotlib.pyplot as plt import pandas as pd In\u00a0[2]: Copied! <pre>_ = manual_seed(42)\ndataset = datasets.CreditCard().take(5000)\n</pre> _ = manual_seed(42) dataset = datasets.CreditCard().take(5000) In\u00a0[3]: Copied! <pre>dataset = datasets.CreditCard().take(5000)\nmetric = metrics.RollingROCAUC(window_size=5000)\nclass MyAutoEncoder(nn.Module):\n    def __init__(self, n_features, latent_dim=3):\n        super(MyAutoEncoder, self).__init__()\n        self.linear1 = nn.Linear(n_features, latent_dim)\n        self.nonlin = nn.LeakyReLU()\n        self.linear2 = nn.Linear(latent_dim, n_features)\n        self.sigmoid = nn.Sigmoid()\n\n    def forward(self, X, **kwargs):\n        X = self.linear1(X)\n        X = self.nonlin(X)\n        X = self.linear2(X)\n        return self.sigmoid(X)\n\n\nmodel_pipeline = compose.Pipeline(\n    preprocessing.MinMaxScaler(), \n    Autoencoder(module=MyAutoEncoder(30), lr=0.005)\n)\nmodel_pipeline\n</pre> dataset = datasets.CreditCard().take(5000) metric = metrics.RollingROCAUC(window_size=5000) class MyAutoEncoder(nn.Module):     def __init__(self, n_features, latent_dim=3):         super(MyAutoEncoder, self).__init__()         self.linear1 = nn.Linear(n_features, latent_dim)         self.nonlin = nn.LeakyReLU()         self.linear2 = nn.Linear(latent_dim, n_features)         self.sigmoid = nn.Sigmoid()      def forward(self, X, **kwargs):         X = self.linear1(X)         X = self.nonlin(X)         X = self.linear2(X)         return self.sigmoid(X)   model_pipeline = compose.Pipeline(     preprocessing.MinMaxScaler(),      Autoencoder(module=MyAutoEncoder(30), lr=0.005) ) model_pipeline Out[3]: <pre>MinMaxScaler</pre><code>MinMaxScaler () </code><pre>Autoencoder</pre><code>Autoencoder (   module=MyAutoEncoder(   (linear1): Linear(in_features=30, out_features=3, bias=True)   (nonlin): LeakyReLU(negative_slope=0.01)   (linear2): Linear(in_features=3, out_features=30, bias=True)   (sigmoid): Sigmoid() )   loss_fn=\"mse\"   optimizer_fn=\"sgd\"   lr=0.005   is_feature_incremental=False   device=\"cpu\"   seed=42 ) </code> In\u00a0[4]: Copied! <pre>for x, y in dataset:\n    score = model_pipeline.score_one(x)\n    metric.update(y_true=y, y_pred=score)\n    model_pipeline.learn_one(x=x)\nprint(f\"ROCAUC: {metric.get():.4f}\")\n</pre> for x, y in dataset:     score = model_pipeline.score_one(x)     metric.update(y_true=y, y_pred=score)     model_pipeline.learn_one(x=x) print(f\"ROCAUC: {metric.get():.4f}\") <pre>ROCAUC: 0.8901\n</pre> In\u00a0[5]: Copied! <pre>latent_dims = [2, 5, 10, 20, 30, 40]  # You can adjust this range\nresults = []\n</pre> latent_dims = [2, 5, 10, 20, 30, 40]  # You can adjust this range results = [] In\u00a0[6]: Copied! <pre>for latent_dim in latent_dims:\n    # Initialize a fresh metric\n    dataset = datasets.CreditCard().take(5000)\n    metric = metrics.RollingROCAUC(window_size=5000)\n\n    # Initialize pipeline with current latent_dim\n    model_pipeline = compose.Pipeline(\n        preprocessing.MinMaxScaler(),\n        Autoencoder(module=MyAutoEncoder(30, latent_dim), lr=0.005)\n    )\n\n    # Train and evaluate model\n    for x, y in dataset:\n        score = model_pipeline.score_one(x)\n        metric.update(y_true=y, y_pred=score)\n        model_pipeline.learn_one(x=x)\n\n    # Store the result\n    results.append((latent_dim, metric.get()))\n\n# Convert results to DataFrame for visualization\ndf_results = pd.DataFrame(results, columns=[\"latent_dim\", \"ROCAUC\"])\n\n# Plot results\nplt.figure(figsize=(8, 5))\nplt.plot(df_results[\"latent_dim\"], df_results[\"ROCAUC\"], marker=\"o\", linestyle=\"-\", color=\"b\")\nplt.xlabel(\"Latent Dimension\")\nplt.ylabel(\"ROCAUC Score\")\nplt.title(\"Hyperparameter Optimization: Latent Dimension vs. ROCAUC\")\nplt.grid(True)\nplt.show()\n</pre> for latent_dim in latent_dims:     # Initialize a fresh metric     dataset = datasets.CreditCard().take(5000)     metric = metrics.RollingROCAUC(window_size=5000)      # Initialize pipeline with current latent_dim     model_pipeline = compose.Pipeline(         preprocessing.MinMaxScaler(),         Autoencoder(module=MyAutoEncoder(30, latent_dim), lr=0.005)     )      # Train and evaluate model     for x, y in dataset:         score = model_pipeline.score_one(x)         metric.update(y_true=y, y_pred=score)         model_pipeline.learn_one(x=x)      # Store the result     results.append((latent_dim, metric.get()))  # Convert results to DataFrame for visualization df_results = pd.DataFrame(results, columns=[\"latent_dim\", \"ROCAUC\"])  # Plot results plt.figure(figsize=(8, 5)) plt.plot(df_results[\"latent_dim\"], df_results[\"ROCAUC\"], marker=\"o\", linestyle=\"-\", color=\"b\") plt.xlabel(\"Latent Dimension\") plt.ylabel(\"ROCAUC Score\") plt.title(\"Hyperparameter Optimization: Latent Dimension vs. ROCAUC\") plt.grid(True) plt.show() In\u00a0[\u00a0]: Copied! <pre>\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"examples/anomaly/example_autoencoder/#simple-fully-connected-autoencoder","title":"Simple Fully Connected Autoencoder\u00b6","text":"<p>This example demonstrate how to create and use a simple fully connected autoencoder for anomaly detection on data streams. The example uses the <code>CreditCard</code> dataset from the river library and shows how the module integrates into the river ecosystem.</p>"},{"location":"examples/anomaly/example_autoencoder/#analyse","title":"Analyse\u00b6","text":"<p>This code initializes a reproducible environment and loads a subset of the CreditCard dataset, selecting 5000 transactions for analysis. It then converts the dataset into a Pandas DataFrame and generates a summary report using TableReport(), which provides an overview of the dataset's structure, feature distributions, and potential missing values. This step helps to understand the data before applying preprocessing or modeling techniques.</p>"},{"location":"examples/anomaly/example_autoencoder/#model-creation","title":"Model Creation\u00b6","text":"<p>This block sets up an anomaly detection pipeline using an autoencoder-based neural network. It begins by loading a subset of the CreditCard dataset and defining a rolling ROC AUC metric to evaluate model performance dynamically over a window of 5000 samples. The <code>MyAutoEncoder</code> class constructs a simple autoencoder with a bottleneck layer of configurable dimensionality (<code>latent_dim=3</code>), using LeakyReLU activation for encoding and a sigmoid function for reconstruction. A pipeline is then created, consisting of a <code>MinMaxScaler</code> for feature normalization and the autoencoder wrapped within <code>AutoencoderInitialized</code>, optimizing it with a learning rate of 0.005. This setup enables online learning, where the model updates continuously as new data arrives.</p>"},{"location":"examples/anomaly/example_autoencoder/#run-the-datastream","title":"Run the Datastream\u00b6","text":"<p>This block performs hyperparameter optimization by evaluating the impact of different latent dimensions on the model's performance. It iterates over a predefined set of <code>latent_dim</code> values, initializing a fresh dataset and rolling ROC AUC metric for each experiment. A pipeline is created for each latent dimension, consisting of a <code>MinMaxScaler</code> for normalization and an autoencoder trained using online learning. The model is then trained and evaluated sequentially on the dataset, updating the metric with its predictions. After processing all data points, the final ROC AUC score is recorded for the current latent dimension. Once all experiments are completed, the results are stored in a DataFrame and visualized using a line plot, where the x-axis represents the latent dimension values and the y-axis represents the corresponding ROC AUC scores. This visualization helps identify the optimal latent dimension for anomaly detection.</p>"},{"location":"examples/anomaly/example_autoencoder/#hyperparameter-optimization","title":"Hyperparameter Optimization\u00b6","text":"<p>This block processes the dataset sequentially, evaluating and updating the model in an online learning manner. For each data point, the model generates an anomaly score using <code>score_one(x)</code>, which is then used to update the rolling ROC AUC metric by comparing it to the true label <code>y</code>. After scoring, the model is trained incrementally using <code>learn_one(x)</code>, allowing it to adapt continuously as new data arrives. Once all data points have been processed, the final ROC AUC score is printed, providing a performance measure of the model's ability to distinguish between fraudulent and non-fraudulent transactions.</p>"},{"location":"examples/anomaly/example_lstm_autoencoder/","title":"Example lstm autoencoder","text":"In\u00a0[1]: Copied! <pre>from river import preprocessing, metrics, datasets\n\nfrom deep_river.anomaly import RollingAutoencoder\nfrom torch import nn, manual_seed\nimport torch\n</pre> from river import preprocessing, metrics, datasets  from deep_river.anomaly import RollingAutoencoder from torch import nn, manual_seed import torch <p>LSTM Encoder-Decoder architecture by Sutskever et al. 2014 (https://arxiv.org/abs/1409.3215). The decoder only gets access to its own prediction of the previous timestep. Decoding also takes performed backwards.</p> In\u00a0[2]: Copied! <pre>class LSTMDecoder(nn.Module):\n    def __init__(\n        self,\n        input_size,\n        hidden_size,\n        sequence_length=None,\n        predict_backward=True,\n        num_layers=1,\n    ):\n        super().__init__()\n\n        self.cell = nn.LSTMCell(input_size, hidden_size)\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n\n        self.predict_backward = predict_backward\n        self.sequence_length = sequence_length\n        self.num_layers = num_layers\n        self.lstm = (\n            None\n            if num_layers &lt;= 1\n            else nn.LSTM(\n                input_size=hidden_size,\n                hidden_size=hidden_size,\n                num_layers=num_layers - 1,\n            )\n        )\n        self.linear = (\n            None\n            if input_size == hidden_size\n            else nn.Linear(hidden_size, input_size)\n        )\n\n    def forward(self, h, sequence_length=None):\n        \"\"\"Computes the forward pass.\n\n        Parameters\n        ----------\n        x:\n            Input of shape (batch_size, input_size)\n\n        Returns\n        -------\n        Tuple[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]\n            Decoder outputs (output, (h, c)) where output has the shape (sequence_length, batch_size, input_size).\n        \"\"\"\n\n        if sequence_length is None:\n            sequence_length = self.sequence_length\n        x_hat = torch.empty(sequence_length, h.shape[0], self.hidden_size)\n        for t in range(sequence_length):\n            if t == 0:\n                h, c = self.cell(h)\n            else:\n                input = h if self.linear is None else self.linear(h)\n                h, c = self.cell(input, (h, c))\n            t_predicted = -t if self.predict_backward else t\n            x_hat[t_predicted] = h\n\n        if self.lstm is not None:\n            x_hat = self.lstm(x_hat)\n\n        return x_hat, (h, c)\n\n\nclass LSTMAutoencoderSutskever(nn.Module):\n    def __init__(self, n_features, hidden_size=30, n_layers=1):\n        super().__init__()\n        self.n_features = n_features\n        self.hidden_size = hidden_size\n        self.n_layers = n_layers\n        self.encoder = nn.LSTM(\n            input_size=n_features, hidden_size=hidden_size, num_layers=n_layers\n        )\n        self.decoder = LSTMDecoder(\n            input_size=hidden_size,\n            hidden_size=n_features,\n            predict_backward=True,\n        )\n\n    def forward(self, x):\n        _, (h, _) = self.encoder(x)\n        x_hat, _ = self.decoder(h[-1], x.shape[0])\n        return x_hat\n</pre> class LSTMDecoder(nn.Module):     def __init__(         self,         input_size,         hidden_size,         sequence_length=None,         predict_backward=True,         num_layers=1,     ):         super().__init__()          self.cell = nn.LSTMCell(input_size, hidden_size)         self.input_size = input_size         self.hidden_size = hidden_size          self.predict_backward = predict_backward         self.sequence_length = sequence_length         self.num_layers = num_layers         self.lstm = (             None             if num_layers &lt;= 1             else nn.LSTM(                 input_size=hidden_size,                 hidden_size=hidden_size,                 num_layers=num_layers - 1,             )         )         self.linear = (             None             if input_size == hidden_size             else nn.Linear(hidden_size, input_size)         )      def forward(self, h, sequence_length=None):         \"\"\"Computes the forward pass.          Parameters         ----------         x:             Input of shape (batch_size, input_size)          Returns         -------         Tuple[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]             Decoder outputs (output, (h, c)) where output has the shape (sequence_length, batch_size, input_size).         \"\"\"          if sequence_length is None:             sequence_length = self.sequence_length         x_hat = torch.empty(sequence_length, h.shape[0], self.hidden_size)         for t in range(sequence_length):             if t == 0:                 h, c = self.cell(h)             else:                 input = h if self.linear is None else self.linear(h)                 h, c = self.cell(input, (h, c))             t_predicted = -t if self.predict_backward else t             x_hat[t_predicted] = h          if self.lstm is not None:             x_hat = self.lstm(x_hat)          return x_hat, (h, c)   class LSTMAutoencoderSutskever(nn.Module):     def __init__(self, n_features, hidden_size=30, n_layers=1):         super().__init__()         self.n_features = n_features         self.hidden_size = hidden_size         self.n_layers = n_layers         self.encoder = nn.LSTM(             input_size=n_features, hidden_size=hidden_size, num_layers=n_layers         )         self.decoder = LSTMDecoder(             input_size=hidden_size,             hidden_size=n_features,             predict_backward=True,         )      def forward(self, x):         _, (h, _) = self.encoder(x)         x_hat, _ = self.decoder(h[-1], x.shape[0])         return x_hat In\u00a0[3]: Copied! <pre>_ = manual_seed(42)\ndataset = datasets.CreditCard().take(5000)\nmetric = metrics.RollingROCAUC(window_size=5000)\n\nmodule = LSTMAutoencoderSutskever(30)  # Set this variable to your architecture of choice\nae = RollingAutoencoder(module=module, lr=0.005)\nscaler = preprocessing.StandardScaler()\n</pre> _ = manual_seed(42) dataset = datasets.CreditCard().take(5000) metric = metrics.RollingROCAUC(window_size=5000)  module = LSTMAutoencoderSutskever(30)  # Set this variable to your architecture of choice ae = RollingAutoencoder(module=module, lr=0.005) scaler = preprocessing.StandardScaler() In\u00a0[4]: Copied! <pre>for x, y in list(dataset):\n    scaler.learn_one(x)\n    x = scaler.transform_one(x)\n    score = ae.score_one(x)\n    metric.update(y_true=y, y_pred=score)\n    ae.learn_one(x=x, y=None)\nprint(f\"ROCAUC: {metric.get():.4f}\")\n</pre> for x, y in list(dataset):     scaler.learn_one(x)     x = scaler.transform_one(x)     score = ae.score_one(x)     metric.update(y_true=y, y_pred=score)     ae.learn_one(x=x, y=None) print(f\"ROCAUC: {metric.get():.4f}\") <pre>ROCAUC: 0.6839\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"examples/anomaly/example_lstm_autoencoder/#example-for-anomaly-detection-with-lstm-autoencoder-architectures","title":"Example for anomaly detection with LSTM autoencoder architectures\u00b6","text":"<p>There is a multitude of successful architecture. In the following we demonstrate the implementation of 3 possible architecture types.</p>"},{"location":"examples/anomaly/example_lstm_autoencoder/#models","title":"Models\u00b6","text":""},{"location":"examples/anomaly/example_lstm_autoencoder/#testing","title":"Testing\u00b6","text":"<p>The models can be tested with the code in the following cells. Since River currently does not feature any anomaly detection datasets with temporal dependencies, the results should be expected to be somewhat inaccurate.</p>"},{"location":"examples/anomaly/example_probability_weighted_autoencoder/","title":"Example probability weighted autoencoder","text":"In\u00a0[1]: Copied! <pre>from river import compose, preprocessing, metrics, datasets\nfrom deep_river.anomaly import ProbabilityWeightedAutoencoder\nfrom torch import nn, manual_seed\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\n</pre> from river import compose, preprocessing, metrics, datasets from deep_river.anomaly import ProbabilityWeightedAutoencoder from torch import nn, manual_seed import numpy as np import matplotlib.pyplot as plt import pandas as pd In\u00a0[2]: Copied! <pre>_ = manual_seed(42)\ndataset = datasets.CreditCard().take(5000)\n</pre> _ = manual_seed(42) dataset = datasets.CreditCard().take(5000) In\u00a0[3]: Copied! <pre>dataset = datasets.CreditCard().take(5000)\nmetric = metrics.RollingROCAUC(window_size=5000)\nclass MyAutoEncoder(nn.Module):\n    def __init__(self, n_features, latent_dim=3):\n        super(MyAutoEncoder, self).__init__()\n        self.linear1 = nn.Linear(n_features, latent_dim)\n        self.nonlin = nn.LeakyReLU()\n        self.linear2 = nn.Linear(latent_dim, n_features)\n        self.sigmoid = nn.Sigmoid()\n\n    def forward(self, X, **kwargs):\n        X = self.linear1(X)\n        X = self.nonlin(X)\n        X = self.linear2(X)\n        return self.sigmoid(X)\n\n\nmodel_pipeline = compose.Pipeline(\n    preprocessing.MinMaxScaler(), \n    ProbabilityWeightedAutoencoder(module=MyAutoEncoder(30), lr=0.005)\n)\nmodel_pipeline\n</pre> dataset = datasets.CreditCard().take(5000) metric = metrics.RollingROCAUC(window_size=5000) class MyAutoEncoder(nn.Module):     def __init__(self, n_features, latent_dim=3):         super(MyAutoEncoder, self).__init__()         self.linear1 = nn.Linear(n_features, latent_dim)         self.nonlin = nn.LeakyReLU()         self.linear2 = nn.Linear(latent_dim, n_features)         self.sigmoid = nn.Sigmoid()      def forward(self, X, **kwargs):         X = self.linear1(X)         X = self.nonlin(X)         X = self.linear2(X)         return self.sigmoid(X)   model_pipeline = compose.Pipeline(     preprocessing.MinMaxScaler(),      ProbabilityWeightedAutoencoder(module=MyAutoEncoder(30), lr=0.005) ) model_pipeline Out[3]: <pre>MinMaxScaler</pre><code>MinMaxScaler () </code><pre>ProbabilityWeightedAutoencoder</pre><code>ProbabilityWeightedAutoencoder (   module=MyAutoEncoder(   (linear1): Linear(in_features=30, out_features=3, bias=True)   (nonlin): LeakyReLU(negative_slope=0.01)   (linear2): Linear(in_features=3, out_features=30, bias=True)   (sigmoid): Sigmoid() )   loss_fn=\"mse\"   optimizer_fn=\"sgd\"   lr=0.005   device=\"cpu\"   seed=42   skip_threshold=0.9   window_size=250 ) </code> In\u00a0[4]: Copied! <pre>for x, y in dataset:\n    score = model_pipeline.score_one(x)\n    metric.update(y_true=y, y_pred=score)\n    model_pipeline.learn_one(x=x)\nprint(f\"ROCAUC: {metric.get():.4f}\")\n</pre> for x, y in dataset:     score = model_pipeline.score_one(x)     metric.update(y_true=y, y_pred=score)     model_pipeline.learn_one(x=x) print(f\"ROCAUC: {metric.get():.4f}\") <pre>ROCAUC: 0.8530\n</pre> In\u00a0[5]: Copied! <pre>latent_dims = [2, 5, 10, 20, 30, 40]  # You can adjust this range\nresults = []\n</pre> latent_dims = [2, 5, 10, 20, 30, 40]  # You can adjust this range results = [] In\u00a0[6]: Copied! <pre>for latent_dim in latent_dims:\n    # Initialize a fresh metric\n    dataset = datasets.CreditCard().take(5000)\n    metric = metrics.RollingROCAUC(window_size=5000)\n\n    # Initialize pipeline with current latent_dim\n    model_pipeline = compose.Pipeline(\n        preprocessing.MinMaxScaler(),\n        ProbabilityWeightedAutoencoder(module=MyAutoEncoder(30, latent_dim), lr=0.005)\n    )\n\n    # Train and evaluate model\n    for x, y in dataset:\n        score = model_pipeline.score_one(x)\n        metric.update(y_true=y, y_pred=score)\n        model_pipeline.learn_one(x=x)\n\n    # Store the result\n    results.append((latent_dim, metric.get()))\n\n# Convert results to DataFrame for visualization\ndf_results = pd.DataFrame(results, columns=[\"latent_dim\", \"ROCAUC\"])\n\n# Plot results\nplt.figure(figsize=(8, 5))\nplt.plot(df_results[\"latent_dim\"], df_results[\"ROCAUC\"], marker=\"o\", linestyle=\"-\", color=\"b\")\nplt.xlabel(\"Latent Dimension\")\nplt.ylabel(\"ROCAUC Score\")\nplt.title(\"Hyperparameter Optimization: Latent Dimension vs. ROCAUC\")\nplt.grid(True)\nplt.show()\n</pre> for latent_dim in latent_dims:     # Initialize a fresh metric     dataset = datasets.CreditCard().take(5000)     metric = metrics.RollingROCAUC(window_size=5000)      # Initialize pipeline with current latent_dim     model_pipeline = compose.Pipeline(         preprocessing.MinMaxScaler(),         ProbabilityWeightedAutoencoder(module=MyAutoEncoder(30, latent_dim), lr=0.005)     )      # Train and evaluate model     for x, y in dataset:         score = model_pipeline.score_one(x)         metric.update(y_true=y, y_pred=score)         model_pipeline.learn_one(x=x)      # Store the result     results.append((latent_dim, metric.get()))  # Convert results to DataFrame for visualization df_results = pd.DataFrame(results, columns=[\"latent_dim\", \"ROCAUC\"])  # Plot results plt.figure(figsize=(8, 5)) plt.plot(df_results[\"latent_dim\"], df_results[\"ROCAUC\"], marker=\"o\", linestyle=\"-\", color=\"b\") plt.xlabel(\"Latent Dimension\") plt.ylabel(\"ROCAUC Score\") plt.title(\"Hyperparameter Optimization: Latent Dimension vs. ROCAUC\") plt.grid(True) plt.show() In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"examples/anomaly/example_probability_weighted_autoencoder/#probability-weighted-autoencoder","title":"Probability weighted Autoencoder\u00b6","text":"<p>A Probability-Weighted Autoencoder (PWAE) is a variant of the traditional autoencoder designed to incorporate probabilistic weighting into the reconstruction process. Unlike standard autoencoders that treat all inputs equally, a PWAE assigns different importance to each data point based on its probability distribution, enhancing its ability to detect anomalies and rare events. The model minimizes a weighted reconstruction loss, ensuring that more critical samples (e.g., minority class instances like fraud) contribute more to learning.</p>"},{"location":"examples/anomaly/example_probability_weighted_autoencoder/#analyse","title":"Analyse\u00b6","text":"<p>This code initializes a reproducible environment and loads a subset of the CreditCard dataset, selecting 5000 transactions for analysis. It then converts the dataset into a Pandas DataFrame and generates a summary report using TableReport(), which provides an overview of the dataset's structure, feature distributions, and potential missing values. This step helps to understand the data before applying preprocessing or modeling techniques.</p>"},{"location":"examples/anomaly/example_probability_weighted_autoencoder/#model-creation","title":"Model Creation\u00b6","text":"<p>While the model stays the same as the fully connected autoencoder, the wrapper class changes to the probability weighted autoencoder.</p>"},{"location":"examples/anomaly/example_probability_weighted_autoencoder/#run-the-datastream","title":"Run the Datastream\u00b6","text":"<p>This block performs hyperparameter optimization by evaluating the impact of different latent dimensions on the model's performance. It iterates over a predefined set of <code>latent_dim</code> values, initializing a fresh dataset and rolling ROC AUC metric for each experiment. A pipeline is created for each latent dimension, consisting of a <code>MinMaxScaler</code> for normalization and an autoencoder trained using online learning. The model is then trained and evaluated sequentially on the dataset, updating the metric with its predictions. After processing all data points, the final ROC AUC score is recorded for the current latent dimension. Once all experiments are completed, the results are stored in a DataFrame and visualized using a line plot, where the x-axis represents the latent dimension values and the y-axis represents the corresponding ROC AUC scores. This visualization helps identify the optimal latent dimension for anomaly detection.</p>"},{"location":"examples/anomaly/example_probability_weighted_autoencoder/#hyperparameter-optimization","title":"Hyperparameter Optimization\u00b6","text":"<p>This block processes the dataset sequentially, evaluating and updating the model in an online learning manner. For each data point, the model generates an anomaly score using <code>score_one(x)</code>, which is then used to update the rolling ROC AUC metric by comparing it to the true label <code>y</code>. After scoring, the model is trained incrementally using <code>learn_one(x)</code>, allowing it to adapt continuously as new data arrives. Once all data points have been processed, the final ROC AUC score is printed, providing a performance measure of the model's ability to distinguish between fraudulent and non-fraudulent transactions.</p>"},{"location":"examples/catastrophic_forgetting/label_shift/","title":"Label shift","text":"In\u00a0[1]: Copied! <pre>from river.datasets import ImageSegments\nfrom river.preprocessing import MinMaxScaler\nfrom river.tree import HoeffdingTreeClassifier\nfrom deep_river.classification import Classifier\nfrom torch import nn\nfrom tqdm import tqdm\nimport matplotlib.pyplot as plt\nimport torch\nimport numpy as np\nimport random\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.utils import resample\nfrom sklearn.manifold import TSNE\n\nrandom.seed(0)\n</pre> from river.datasets import ImageSegments from river.preprocessing import MinMaxScaler from river.tree import HoeffdingTreeClassifier from deep_river.classification import Classifier from torch import nn from tqdm import tqdm import matplotlib.pyplot as plt import torch import numpy as np import random from sklearn.model_selection import train_test_split from sklearn.metrics import accuracy_score from sklearn.utils import resample from sklearn.manifold import TSNE  random.seed(0) In\u00a0[2]: Copied! <pre>stream_size = 10_000\n\nx, y = list(zip(*ImageSegments()))\nx = np.array(x)\ny = np.array(y)\n\n# Divide classes into two task specific sets\ntask_classes = [\n    [\"brickface\", \"window\", \"cement\"],\n    [\"sky\", \"foliage\", \"path\", \"grass\"],\n]\n\n# Divide all samples into their specific tasks\ndata_train = []\ndata_test = []\nfor classes_t in task_classes:\n    x_t = np.concatenate([x[y == c] for c in classes_t])\n    y_t = np.concatenate([330 * [c] for c in classes_t])\n    x_train, x_test, y_train, y_test = train_test_split(\n        x_t, y_t, test_size=0.25, stratify=y_t\n    )\n    x_train, y_train = resample(\n        x_train, y_train, n_samples=int(stream_size / 2), stratify=y_train\n    )\n    data_train.append(list(zip(x_train, y_train)))\n    data_test.append(list(zip(x_test, y_test)))\n\ndata_train = data_train[0] + data_train[1]\n</pre> stream_size = 10_000  x, y = list(zip(*ImageSegments())) x = np.array(x) y = np.array(y)  # Divide classes into two task specific sets task_classes = [     [\"brickface\", \"window\", \"cement\"],     [\"sky\", \"foliage\", \"path\", \"grass\"], ]  # Divide all samples into their specific tasks data_train = [] data_test = [] for classes_t in task_classes:     x_t = np.concatenate([x[y == c] for c in classes_t])     y_t = np.concatenate([330 * [c] for c in classes_t])     x_train, x_test, y_train, y_test = train_test_split(         x_t, y_t, test_size=0.25, stratify=y_t     )     x_train, y_train = resample(         x_train, y_train, n_samples=int(stream_size / 2), stratify=y_train     )     data_train.append(list(zip(x_train, y_train)))     data_test.append(list(zip(x_test, y_test)))  data_train = data_train[0] + data_train[1] In\u00a0[3]: Copied! <pre>classnames = task_classes[0] + task_classes[1]\ntsne = TSNE(init=\"pca\", learning_rate=\"auto\", n_jobs=-1)\nx_array = np.array([list(x_i.values()) for x_i in x])\nx_viz = tsne.fit_transform(x_array)\n</pre> classnames = task_classes[0] + task_classes[1] tsne = TSNE(init=\"pca\", learning_rate=\"auto\", n_jobs=-1) x_array = np.array([list(x_i.values()) for x_i in x]) x_viz = tsne.fit_transform(x_array) <pre>/Users/cedrickulbach/Documents/Projects/deep-river/.venv/lib/python3.10/site-packages/threadpoolctl.py:1226: RuntimeWarning: \nFound Intel OpenMP ('libiomp') and LLVM OpenMP ('libomp') loaded at\nthe same time. Both libraries are known to be incompatible and this\ncan cause random crashes or deadlocks on Linux when loaded in the\nsame Python program.\nUsing threadpoolctl may cause crashes or deadlocks. For more\ninformation and possible workarounds, please see\n    https://github.com/joblib/threadpoolctl/blob/master/multiple_openmp.md\n\n  warnings.warn(msg, RuntimeWarning)\n</pre> In\u00a0[4]: Copied! <pre>fig, ax = plt.subplots()\ncm = [\n    \"goldenrod\",\n    \"gold\",\n    \"yellow\",\n    \"aquamarine\",\n    \"seagreen\",\n    \"limegreen\",\n    \"lawngreen\",\n]\nfor c_idx, x_c in enumerate([x_viz[y == c] for c in classnames]):\n    scatter = ax.scatter(\n        x_c[:, 0],\n        x_c[:, 1],\n        c=len(x_c) * [cm[c_idx]],\n        s=3,\n        alpha=0.8,\n        label=classnames[c_idx],\n    )\n\nax.legend(loc=(1.04, 0))\n</pre> fig, ax = plt.subplots() cm = [     \"goldenrod\",     \"gold\",     \"yellow\",     \"aquamarine\",     \"seagreen\",     \"limegreen\",     \"lawngreen\", ] for c_idx, x_c in enumerate([x_viz[y == c] for c in classnames]):     scatter = ax.scatter(         x_c[:, 0],         x_c[:, 1],         c=len(x_c) * [cm[c_idx]],         s=3,         alpha=0.8,         label=classnames[c_idx],     )  ax.legend(loc=(1.04, 0)) Out[4]: <pre>&lt;matplotlib.legend.Legend at 0x12a0473a0&gt;</pre> In\u00a0[5]: Copied! <pre># Define function to calculate accuracy on testing data for each task\ndef get_test_accuracy(model, data_test):\n    results = []\n    for data_test_i in data_test:\n        ys = []\n        y_preds = []\n        for x_test, y_test in data_test_i:\n            ys.append(y_test)\n            y_preds.append(model.predict_one(x_test))\n        accuracy = accuracy_score(ys, y_preds)\n        results.append(accuracy)\n    return results\n\n\n# Define training and testing loop\ndef eval_separate_testing(model, data_train, data_test):\n    scaler = MinMaxScaler()\n    step = 0\n    steps = []\n    results = [[] for task in data_test]\n    for x, y in tqdm(data_train):\n        step += 1\n        scaler.learn_one(x)\n        x = scaler.transform_one(x)\n        model.learn_one(x, y)\n\n        if step % 100 == 0:\n            test_accuracies = get_test_accuracy(model, data_test)\n            for idx, accuracy in enumerate(test_accuracies):\n                results[idx].append(accuracy)\n            steps.append(step)\n    return steps, results\n</pre> # Define function to calculate accuracy on testing data for each task def get_test_accuracy(model, data_test):     results = []     for data_test_i in data_test:         ys = []         y_preds = []         for x_test, y_test in data_test_i:             ys.append(y_test)             y_preds.append(model.predict_one(x_test))         accuracy = accuracy_score(ys, y_preds)         results.append(accuracy)     return results   # Define training and testing loop def eval_separate_testing(model, data_train, data_test):     scaler = MinMaxScaler()     step = 0     steps = []     results = [[] for task in data_test]     for x, y in tqdm(data_train):         step += 1         scaler.learn_one(x)         x = scaler.transform_one(x)         model.learn_one(x, y)          if step % 100 == 0:             test_accuracies = get_test_accuracy(model, data_test)             for idx, accuracy in enumerate(test_accuracies):                 results[idx].append(accuracy)             steps.append(step)     return steps, results In\u00a0[6]: Copied! <pre># Evaluate a simple MLP classifier\nclass SimpleMLP(nn.Module):\n    def __init__(self, n_features):\n        super(SimpleMLP, self).__init__()\n        self.dense0 = nn.Linear(n_features, 5)\n        self.nonlin = nn.ReLU()\n        self.dense1 = nn.Linear(5, 2)\n        self.softmax = nn.Softmax(dim=-1)\n\n    def forward(self, X, **kwargs):\n        X = self.nonlin(self.dense0(X))\n        X = self.nonlin(self.dense1(X))\n        X = self.softmax(X)\n        return X\n\n\nmlp = Classifier(\n    SimpleMLP(30),\n    loss_fn=\"binary_cross_entropy_with_logits\",\n    optimizer_fn=\"sgd\",\n    lr=0.05,\n    seed=42,\n)\nsteps, results_mlp = eval_separate_testing(mlp, data_train, data_test)\n</pre> # Evaluate a simple MLP classifier class SimpleMLP(nn.Module):     def __init__(self, n_features):         super(SimpleMLP, self).__init__()         self.dense0 = nn.Linear(n_features, 5)         self.nonlin = nn.ReLU()         self.dense1 = nn.Linear(5, 2)         self.softmax = nn.Softmax(dim=-1)      def forward(self, X, **kwargs):         X = self.nonlin(self.dense0(X))         X = self.nonlin(self.dense1(X))         X = self.softmax(X)         return X   mlp = Classifier(     SimpleMLP(30),     loss_fn=\"binary_cross_entropy_with_logits\",     optimizer_fn=\"sgd\",     lr=0.05,     seed=42, ) steps, results_mlp = eval_separate_testing(mlp, data_train, data_test) <pre>\r  0%|                                                                                                                                                                                                                                                                                             | 0/10000 [00:00&lt;?, ?it/s]</pre> <pre>\r  1%|\u2588\u2589                                                                                                                                                                                                                                                                                 | 69/10000 [00:00&lt;00:14, 685.95it/s]</pre> <pre>\r  1%|\u2588\u2588\u2588\u258a                                                                                                                                                                                                                                                                              | 138/10000 [00:00&lt;00:17, 570.05it/s]</pre> <pre>\r  2%|\u2588\u2588\u2588\u2588\u2588\u258d                                                                                                                                                                                                                                                                            | 200/10000 [00:00&lt;00:17, 545.43it/s]</pre> <pre>\r  3%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f                                                                                                                                                                                                                                                                         | 300/10000 [00:00&lt;00:15, 612.60it/s]</pre> <pre>\r  4%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589                                                                                                                                                                                                                                                                       | 400/10000 [00:00&lt;00:14, 652.04it/s]</pre> <pre>\r  5%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b                                                                                                                                                                                                                                                                    | 500/10000 [00:00&lt;00:14, 677.48it/s]</pre> <pre>\r  6%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d                                                                                                                                                                                                                                                                 | 600/10000 [00:00&lt;00:13, 697.60it/s]</pre> <pre>\r  7%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f                                                                                                                                                                                                                                                              | 700/10000 [00:01&lt;00:13, 706.66it/s]</pre> <pre>\r  8%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589                                                                                                                                                                                                                                                            | 800/10000 [00:01&lt;00:12, 707.89it/s]</pre> <pre>\r  9%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b                                                                                                                                                                                                                                                         | 900/10000 [00:01&lt;00:12, 710.96it/s]</pre> <pre>\r 10%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e                                                                                                                                                                                                                                                     | 1000/10000 [00:01&lt;00:12, 715.83it/s]</pre> <pre>\r 11%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588                                                                                                                                                                                                                                                   | 1100/10000 [00:01&lt;00:12, 719.11it/s]</pre> <pre>\r 12%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a                                                                                                                                                                                                                                                | 1200/10000 [00:01&lt;00:12, 714.52it/s]</pre> <pre>\r 13%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d                                                                                                                                                                                                                                             | 1300/10000 [00:01&lt;00:12, 720.39it/s]</pre> <pre>\r 14%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f                                                                                                                                                                                                                                          | 1400/10000 [00:02&lt;00:11, 725.33it/s]</pre> <pre>\r 15%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589                                                                                                                                                                                                                                        | 1500/10000 [00:02&lt;00:11, 729.43it/s]</pre> <pre>\r 16%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b                                                                                                                                                                                                                                     | 1600/10000 [00:02&lt;00:11, 724.98it/s]</pre> <pre>\r 17%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d                                                                                                                                                                                                                                  | 1700/10000 [00:02&lt;00:11, 730.61it/s]</pre> <pre>\r 18%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f                                                                                                                                                                                                                               | 1800/10000 [00:02&lt;00:11, 731.66it/s]</pre> <pre>\r 19%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a                                                                                                                                                                                                                             | 1900/10000 [00:02&lt;00:11, 731.59it/s]</pre> <pre>\r 20%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c                                                                                                                                                                                                                          | 2000/10000 [00:02&lt;00:11, 724.02it/s]</pre> <pre>\r 21%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e                                                                                                                                                                                                                       | 2100/10000 [00:02&lt;00:10, 725.22it/s]</pre> <pre>\r 22%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588                                                                                                                                                                                                                     | 2200/10000 [00:03&lt;00:10, 727.33it/s]</pre> <pre>\r 23%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a                                                                                                                                                                                                                  | 2300/10000 [00:03&lt;00:10, 723.26it/s]</pre> <pre>\r 24%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c                                                                                                                                                                                                               | 2400/10000 [00:03&lt;00:10, 724.41it/s]</pre> <pre>\r 25%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e                                                                                                                                                                                                            | 2500/10000 [00:03&lt;00:10, 724.99it/s]</pre> <pre>\r 26%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589                                                                                                                                                                                                          | 2600/10000 [00:03&lt;00:10, 718.48it/s]</pre> <pre>\r 27%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b                                                                                                                                                                                                       | 2700/10000 [00:03&lt;00:10, 715.51it/s]</pre> <pre>\r 28%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d                                                                                                                                                                                                    | 2800/10000 [00:03&lt;00:10, 711.16it/s]</pre> <pre>\r 29%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f                                                                                                                                                                                                 | 2900/10000 [00:04&lt;00:10, 707.47it/s]</pre> <pre>\r 30%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589                                                                                                                                                                                               | 3000/10000 [00:04&lt;00:09, 711.05it/s]</pre> <pre>\r 31%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b                                                                                                                                                                                            | 3100/10000 [00:04&lt;00:09, 720.73it/s]</pre> <pre>\r 32%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e                                                                                                                                                                                         | 3200/10000 [00:04&lt;00:09, 726.98it/s]</pre> <pre>\r 33%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588                                                                                                                                                                                       | 3300/10000 [00:04&lt;00:09, 725.48it/s]</pre> <pre>\r 34%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a                                                                                                                                                                                    | 3400/10000 [00:04&lt;00:09, 723.70it/s]</pre> <pre>\r 35%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c                                                                                                                                                                                 | 3500/10000 [00:04&lt;00:08, 723.37it/s]</pre> <pre>\r 36%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e                                                                                                                                                                              | 3600/10000 [00:05&lt;00:08, 726.41it/s]</pre> <pre>\r 37%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588                                                                                                                                                                            | 3700/10000 [00:05&lt;00:08, 725.35it/s]</pre> <pre>\r 38%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b                                                                                                                                                                         | 3800/10000 [00:05&lt;00:08, 725.04it/s]</pre> <pre>\r 39%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d                                                                                                                                                                      | 3900/10000 [00:05&lt;00:08, 726.09it/s]</pre> <pre>\r 40%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f                                                                                                                                                                   | 4000/10000 [00:05&lt;00:08, 727.37it/s]</pre> <pre>\r 41%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589                                                                                                                                                                 | 4100/10000 [00:05&lt;00:08, 718.48it/s]</pre> <pre>\r 42%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b                                                                                                                                                              | 4200/10000 [00:05&lt;00:08, 718.17it/s]</pre> <pre>\r 43%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d                                                                                                                                                           | 4300/10000 [00:06&lt;00:07, 714.67it/s]</pre> <pre>\r 44%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588                                                                                                                                                         | 4400/10000 [00:06&lt;00:07, 714.03it/s]</pre> <pre>\r 45%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a                                                                                                                                                      | 4500/10000 [00:06&lt;00:07, 715.29it/s]</pre> <pre>\r 46%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c                                                                                                                                                   | 4600/10000 [00:06&lt;00:07, 717.06it/s]</pre> <pre>\r 47%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e                                                                                                                                                | 4700/10000 [00:06&lt;00:07, 718.95it/s]</pre> <pre>\r 48%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588                                                                                                                                              | 4800/10000 [00:06&lt;00:07, 717.98it/s]</pre> <pre>\r 49%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a                                                                                                                                           | 4900/10000 [00:06&lt;00:07, 721.21it/s]</pre> <pre>\r 50%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c                                                                                                                                        | 5000/10000 [00:07&lt;00:06, 718.23it/s]</pre> <pre>\r 51%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f                                                                                                                                     | 5100/10000 [00:07&lt;00:06, 720.99it/s]</pre> <pre>\r 52%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589                                                                                                                                   | 5200/10000 [00:07&lt;00:06, 727.76it/s]</pre> <pre>\r 53%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b                                                                                                                                | 5300/10000 [00:07&lt;00:06, 733.60it/s]</pre> <pre>\r 54%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d                                                                                                                             | 5400/10000 [00:07&lt;00:06, 730.01it/s]</pre> <pre>\r 55%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f                                                                                                                          | 5500/10000 [00:07&lt;00:06, 728.19it/s]</pre> <pre>\r 56%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589                                                                                                                        | 5600/10000 [00:07&lt;00:06, 728.66it/s]</pre> <pre>\r 57%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c                                                                                                                     | 5700/10000 [00:07&lt;00:05, 734.48it/s]</pre> <pre>\r 58%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e                                                                                                                  | 5800/10000 [00:08&lt;00:05, 736.69it/s]</pre> <pre>\r 59%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588                                                                                                                | 5900/10000 [00:08&lt;00:05, 734.05it/s]</pre> <pre>\r 60%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a                                                                                                             | 6000/10000 [00:08&lt;00:05, 726.54it/s]</pre> <pre>\r 61%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c                                                                                                          | 6100/10000 [00:08&lt;00:05, 729.96it/s]</pre> <pre>\r 62%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e                                                                                                       | 6200/10000 [00:08&lt;00:05, 727.93it/s]</pre> <pre>\r 63%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589                                                                                                     | 6300/10000 [00:08&lt;00:05, 725.29it/s]</pre> <pre>\r 64%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b                                                                                                  | 6400/10000 [00:08&lt;00:04, 720.41it/s]</pre> <pre>\r 65%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d                                                                                               | 6500/10000 [00:09&lt;00:04, 716.23it/s]</pre> <pre>\r 66%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f                                                                                            | 6600/10000 [00:09&lt;00:04, 714.90it/s]</pre> <pre>\r 67%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589                                                                                          | 6700/10000 [00:09&lt;00:04, 719.91it/s]</pre> <pre>\r 68%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b                                                                                       | 6800/10000 [00:09&lt;00:04, 711.00it/s]</pre> <pre>\r 69%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e                                                                                    | 6900/10000 [00:09&lt;00:04, 714.57it/s]</pre> <pre>\r 70%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588                                                                                  | 7000/10000 [00:09&lt;00:04, 713.78it/s]</pre> <pre>\r 71%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a                                                                               | 7100/10000 [00:09&lt;00:04, 715.64it/s]</pre> <pre>\r 72%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c                                                                            | 7200/10000 [00:10&lt;00:03, 720.31it/s]</pre> <pre>\r 73%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e                                                                         | 7300/10000 [00:10&lt;00:03, 720.78it/s]</pre> <pre>\r 74%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588                                                                       | 7400/10000 [00:10&lt;00:03, 724.48it/s]</pre> <pre>\r 75%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a                                                                    | 7500/10000 [00:10&lt;00:03, 731.97it/s]</pre> <pre>\r 76%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d                                                                 | 7600/10000 [00:10&lt;00:03, 733.21it/s]</pre> <pre>\r 77%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f                                                              | 7700/10000 [00:10&lt;00:03, 728.36it/s]</pre> <pre>\r 78%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589                                                            | 7800/10000 [00:10&lt;00:03, 725.22it/s]</pre> <pre>\r 79%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b                                                         | 7900/10000 [00:11&lt;00:02, 730.16it/s]</pre> <pre>\r 80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d                                                      | 8000/10000 [00:11&lt;00:02, 730.76it/s]</pre> <pre>\r 81%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f                                                   | 8100/10000 [00:11&lt;00:02, 726.85it/s]</pre> <pre>\r 82%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a                                                 | 8200/10000 [00:11&lt;00:02, 729.65it/s]</pre> <pre>\r 83%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c                                              | 8300/10000 [00:11&lt;00:02, 731.08it/s]</pre> <pre>\r 84%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e                                           | 8400/10000 [00:11&lt;00:02, 728.56it/s]</pre> <pre>\r 85%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588                                         | 8500/10000 [00:11&lt;00:02, 726.54it/s]</pre> <pre>\r 86%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a                                      | 8600/10000 [00:11&lt;00:01, 726.78it/s]</pre> <pre>\r 87%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c                                   | 8700/10000 [00:12&lt;00:01, 722.58it/s]</pre> <pre>\r 88%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f                                | 8800/10000 [00:12&lt;00:01, 722.56it/s]</pre> <pre>\r 89%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589                              | 8900/10000 [00:12&lt;00:01, 729.12it/s]</pre> <pre>\r 90%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b                           | 9000/10000 [00:12&lt;00:01, 730.73it/s]</pre> <pre>\r 91%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d                        | 9100/10000 [00:12&lt;00:01, 724.52it/s]</pre> <pre>\r 92%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f                     | 9200/10000 [00:12&lt;00:01, 727.33it/s]</pre> <pre>\r 93%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589                   | 9300/10000 [00:12&lt;00:00, 732.37it/s]</pre> <pre>\r 94%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c                | 9400/10000 [00:13&lt;00:00, 724.89it/s]</pre> <pre>\r 95%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e             | 9500/10000 [00:13&lt;00:00, 724.31it/s]</pre> <pre>\r 96%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588           | 9600/10000 [00:13&lt;00:00, 725.36it/s]</pre> <pre>\r 97%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a        | 9700/10000 [00:13&lt;00:00, 734.79it/s]</pre> <pre>\r 98%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c     | 9800/10000 [00:13&lt;00:00, 732.25it/s]</pre> <pre>\r 99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e  | 9900/10000 [00:13&lt;00:00, 723.37it/s]</pre> <pre>\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10000/10000 [00:13&lt;00:00, 725.18it/s]</pre> <pre>\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10000/10000 [00:13&lt;00:00, 719.72it/s]</pre> <pre>\n</pre> In\u00a0[7]: Copied! <pre># Evaluate a Hoeffding Tree classifier\ntree = HoeffdingTreeClassifier(tau=0.05)\nsteps, results_tree = eval_separate_testing(tree, data_train, data_test)\n</pre> # Evaluate a Hoeffding Tree classifier tree = HoeffdingTreeClassifier(tau=0.05) steps, results_tree = eval_separate_testing(tree, data_train, data_test) <pre>\r  0%|                                                                                                                                                                                                                                                                                             | 0/10000 [00:00&lt;?, ?it/s]</pre> <pre>\r  2%|\u2588\u2588\u2588\u2588\u2588\u258d                                                                                                                                                                                                                                                                           | 200/10000 [00:00&lt;00:06, 1401.48it/s]</pre> <pre>\r  4%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589                                                                                                                                                                                                                                                                      | 400/10000 [00:00&lt;00:06, 1413.79it/s]</pre> <pre>\r  6%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d                                                                                                                                                                                                                                                                | 600/10000 [00:00&lt;00:06, 1417.47it/s]</pre> <pre>\r  8%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a                                                                                                                                                                                                                                                           | 800/10000 [00:00&lt;00:06, 1408.06it/s]</pre> <pre>\r 10%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f                                                                                                                                                                                                                                                    | 1000/10000 [00:00&lt;00:06, 1410.37it/s]</pre> <pre>\r 12%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b                                                                                                                                                                                                                                               | 1200/10000 [00:00&lt;00:06, 1426.35it/s]</pre> <pre>\r 14%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588                                                                                                                                                                                                                                          | 1400/10000 [00:00&lt;00:05, 1449.83it/s]</pre> <pre>\r 16%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c                                                                                                                                                                                                                                    | 1600/10000 [00:01&lt;00:05, 1470.42it/s]</pre> <pre>\r 18%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589                                                                                                                                                                                                                               | 1800/10000 [00:01&lt;00:05, 1461.60it/s]</pre> <pre>\r 20%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d                                                                                                                                                                                                                         | 2000/10000 [00:01&lt;00:05, 1465.54it/s]</pre> <pre>\r 22%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a                                                                                                                                                                                                                    | 2200/10000 [00:01&lt;00:05, 1467.96it/s]</pre> <pre>\r 24%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e                                                                                                                                                                                                              | 2400/10000 [00:01&lt;00:05, 1458.74it/s]</pre> <pre>\r 26%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b                                                                                                                                                                                                         | 2600/10000 [00:01&lt;00:05, 1438.14it/s]</pre> <pre>\r 28%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f                                                                                                                                                                                                   | 2800/10000 [00:01&lt;00:05, 1439.49it/s]</pre> <pre>\r 30%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c                                                                                                                                                                                              | 3000/10000 [00:02&lt;00:04, 1455.77it/s]</pre> <pre>\r 32%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588                                                                                                                                                                                         | 3200/10000 [00:02&lt;00:04, 1460.67it/s]</pre> <pre>\r 34%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d                                                                                                                                                                                   | 3400/10000 [00:02&lt;00:04, 1459.35it/s]</pre> <pre>\r 36%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589                                                                                                                                                                              | 3600/10000 [00:02&lt;00:04, 1462.91it/s]</pre> <pre>\r 38%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e                                                                                                                                                                        | 3800/10000 [00:02&lt;00:04, 1474.20it/s]</pre> <pre>\r 40%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a                                                                                                                                                                   | 4000/10000 [00:02&lt;00:04, 1471.10it/s]</pre> <pre>\r 42%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f                                                                                                                                                             | 4200/10000 [00:02&lt;00:03, 1474.92it/s]</pre> <pre>\r 44%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b                                                                                                                                                        | 4400/10000 [00:03&lt;00:03, 1481.42it/s]</pre> <pre>\r 46%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588                                                                                                                                                   | 4600/10000 [00:03&lt;00:03, 1482.57it/s]</pre> <pre>\r 48%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c                                                                                                                                             | 4800/10000 [00:03&lt;00:03, 1471.34it/s]</pre> <pre>\r 50%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588                                                                                                                                        | 5000/10000 [00:03&lt;00:03, 1473.30it/s]</pre> <pre>\r 51%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588                                                                                                                                    | 5148/10000 [00:03&lt;00:03, 1310.80it/s]</pre> <pre>\r 53%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b                                                                                                                                | 5281/10000 [00:03&lt;00:04, 1171.86it/s]</pre> <pre>\r 54%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d                                                                                                                             | 5400/10000 [00:04&lt;00:05, 852.98it/s]</pre> <pre>\r 55%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f                                                                                                                          | 5500/10000 [00:04&lt;00:05, 806.85it/s]</pre> <pre>\r 56%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589                                                                                                                        | 5600/10000 [00:04&lt;00:05, 774.61it/s]</pre> <pre>\r 57%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c                                                                                                                     | 5700/10000 [00:04&lt;00:05, 755.04it/s]</pre> <pre>\r 58%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e                                                                                                                  | 5800/10000 [00:04&lt;00:05, 737.85it/s]</pre> <pre>\r 59%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588                                                                                                                | 5900/10000 [00:04&lt;00:05, 727.15it/s]</pre> <pre>\r 60%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a                                                                                                             | 6000/10000 [00:04&lt;00:05, 717.58it/s]</pre> <pre>\r 61%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c                                                                                                          | 6100/10000 [00:05&lt;00:05, 722.86it/s]</pre> <pre>\r 62%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e                                                                                                       | 6200/10000 [00:05&lt;00:05, 715.20it/s]</pre> <pre>\r 63%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589                                                                                                     | 6300/10000 [00:05&lt;00:05, 714.08it/s]</pre> <pre>\r 64%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b                                                                                                  | 6400/10000 [00:05&lt;00:05, 709.24it/s]</pre> <pre>\r 65%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d                                                                                               | 6500/10000 [00:05&lt;00:04, 712.84it/s]</pre> <pre>\r 66%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f                                                                                            | 6600/10000 [00:05&lt;00:04, 709.04it/s]</pre> <pre>\r 67%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589                                                                                          | 6700/10000 [00:05&lt;00:04, 704.78it/s]</pre> <pre>\r 68%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b                                                                                       | 6800/10000 [00:06&lt;00:04, 691.27it/s]</pre> <pre>\r 69%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e                                                                                    | 6900/10000 [00:06&lt;00:04, 691.39it/s]</pre> <pre>\r 70%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588                                                                                  | 7000/10000 [00:06&lt;00:04, 689.06it/s]</pre> <pre>\r 71%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a                                                                               | 7100/10000 [00:06&lt;00:04, 695.43it/s]</pre> <pre>\r 72%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c                                                                            | 7200/10000 [00:06&lt;00:04, 699.39it/s]</pre> <pre>\r 73%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e                                                                         | 7300/10000 [00:06&lt;00:03, 703.98it/s]</pre> <pre>\r 74%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588                                                                       | 7400/10000 [00:06&lt;00:03, 697.86it/s]</pre> <pre>\r 75%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a                                                                    | 7500/10000 [00:07&lt;00:03, 704.38it/s]</pre> <pre>\r 76%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d                                                                 | 7600/10000 [00:07&lt;00:03, 704.23it/s]</pre> <pre>\r 77%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f                                                              | 7700/10000 [00:07&lt;00:03, 709.88it/s]</pre> <pre>\r 78%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589                                                            | 7800/10000 [00:07&lt;00:03, 709.48it/s]</pre> <pre>\r 79%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b                                                         | 7900/10000 [00:07&lt;00:02, 713.03it/s]</pre> <pre>\r 80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d                                                      | 8000/10000 [00:07&lt;00:02, 710.40it/s]</pre> <pre>\r 81%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f                                                   | 8100/10000 [00:07&lt;00:02, 704.92it/s]</pre> <pre>\r 82%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a                                                 | 8200/10000 [00:07&lt;00:02, 699.53it/s]</pre> <pre>\r 83%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c                                              | 8300/10000 [00:08&lt;00:02, 696.32it/s]</pre> <pre>\r 84%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e                                           | 8400/10000 [00:08&lt;00:02, 695.14it/s]</pre> <pre>\r 85%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588                                         | 8500/10000 [00:08&lt;00:02, 691.13it/s]</pre> <pre>\r 86%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a                                      | 8600/10000 [00:08&lt;00:02, 690.11it/s]</pre> <pre>\r 87%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c                                   | 8700/10000 [00:08&lt;00:01, 688.53it/s]</pre> <pre>\r 88%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f                                | 8800/10000 [00:08&lt;00:01, 685.43it/s]</pre> <pre>\r 89%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589                              | 8900/10000 [00:09&lt;00:01, 695.61it/s]</pre> <pre>\r 90%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b                           | 9000/10000 [00:09&lt;00:01, 699.90it/s]</pre> <pre>\r 91%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d                        | 9100/10000 [00:09&lt;00:01, 703.52it/s]</pre> <pre>\r 92%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f                     | 9200/10000 [00:09&lt;00:01, 701.34it/s]</pre> <pre>\r 93%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589                   | 9300/10000 [00:09&lt;00:00, 708.38it/s]</pre> <pre>\r 94%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c                | 9400/10000 [00:09&lt;00:00, 706.90it/s]</pre> <pre>\r 95%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e             | 9500/10000 [00:09&lt;00:00, 702.33it/s]</pre> <pre>\r 96%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588           | 9600/10000 [00:10&lt;00:00, 697.37it/s]</pre> <pre>\r 97%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a        | 9700/10000 [00:10&lt;00:00, 703.23it/s]</pre> <pre>\r 98%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c     | 9800/10000 [00:10&lt;00:00, 699.77it/s]</pre> <pre>\r 99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e  | 9900/10000 [00:10&lt;00:00, 706.80it/s]</pre> <pre>\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10000/10000 [00:10&lt;00:00, 705.44it/s]</pre> <pre>\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10000/10000 [00:10&lt;00:00, 945.89it/s]</pre> <pre>\n</pre> In\u00a0[8]: Copied! <pre>fig, axs = plt.subplots(nrows=2, figsize=(6, 6), sharex=True)\nresults = {\"MLP\": results_mlp, \"Hoeffding Tree\": results_tree}\nfor model_idx, (model_name, model_results) in enumerate(results.items()):\n    ax = axs[model_idx]\n    ax.plot(steps, model_results[0], label=\"Task 1\")\n    ax.plot(steps, model_results[1], label=\"Task 2\")\n    ax.axvline(5000, c=\"red\", label=\"Change of Training Task\")\n    ax.set_ylabel(\"Accuracy\")\n    ax.set_title(model_name)\naxs[-1].set_xlabel(\"Steps\")\naxs[-1].set_xlim(0, 10000)\naxs[0].legend(loc=(1.04, 0))\n</pre> fig, axs = plt.subplots(nrows=2, figsize=(6, 6), sharex=True) results = {\"MLP\": results_mlp, \"Hoeffding Tree\": results_tree} for model_idx, (model_name, model_results) in enumerate(results.items()):     ax = axs[model_idx]     ax.plot(steps, model_results[0], label=\"Task 1\")     ax.plot(steps, model_results[1], label=\"Task 2\")     ax.axvline(5000, c=\"red\", label=\"Change of Training Task\")     ax.set_ylabel(\"Accuracy\")     ax.set_title(model_name) axs[-1].set_xlabel(\"Steps\") axs[-1].set_xlim(0, 10000) axs[0].legend(loc=(1.04, 0)) Out[8]: <pre>&lt;matplotlib.legend.Legend at 0x12c439a50&gt;</pre>"},{"location":"examples/catastrophic_forgetting/label_shift/#create-stream","title":"Create Stream\u00b6","text":""},{"location":"examples/catastrophic_forgetting/label_shift/#visualize-data","title":"Visualize Data\u00b6","text":""},{"location":"examples/catastrophic_forgetting/label_shift/#define-evaluation-procedure","title":"Define evaluation procedure\u00b6","text":""},{"location":"examples/catastrophic_forgetting/label_shift/#evaluate-classifiers","title":"Evaluate Classifiers\u00b6","text":""},{"location":"examples/catastrophic_forgetting/label_shift/#visualize-accuracy-over-timesteps","title":"Visualize Accuracy over Timesteps\u00b6","text":""},{"location":"examples/catastrophic_forgetting/recurring_concepts/","title":"Recurring concepts","text":"In\u00a0[1]: Copied! <pre>from river.datasets.synth import FriedmanDrift\nfrom river.preprocessing import MinMaxScaler\nfrom river.metrics import MAE\nfrom river.utils import Rolling\nfrom river.tree import HoeffdingTreeRegressor\nfrom deep_river.regression import Regressor\nfrom torch import nn\nfrom tqdm import tqdm\nimport matplotlib.pyplot as plt\nimport torch\n</pre> from river.datasets.synth import FriedmanDrift from river.preprocessing import MinMaxScaler from river.metrics import MAE from river.utils import Rolling from river.tree import HoeffdingTreeRegressor from deep_river.regression import Regressor from torch import nn from tqdm import tqdm import matplotlib.pyplot as plt import torch In\u00a0[2]: Copied! <pre>n_samples = 12500\nchange_points = (5000, 7500)\nfriedman = FriedmanDrift(drift_type=\"gra\", position=change_points)\n\n\ndef test_train_eval(model, stream, update_interval=100):\n    results = []\n    steps = []\n    step = 0\n    metric = Rolling(MAE(), window_size=400)\n    scaler = MinMaxScaler()\n    for x, y in tqdm(list(stream)):\n        scaler.learn_one(x)\n        x = scaler.transform_one(x)\n        y_pred = model.predict_one(x)\n        model.learn_one(x, y)\n        metric.update(y, y_pred)\n        step += 1\n        if step % update_interval == 0:\n            results.append(metric.get())\n            steps.append(step)\n    return steps, results\n</pre> n_samples = 12500 change_points = (5000, 7500) friedman = FriedmanDrift(drift_type=\"gra\", position=change_points)   def test_train_eval(model, stream, update_interval=100):     results = []     steps = []     step = 0     metric = Rolling(MAE(), window_size=400)     scaler = MinMaxScaler()     for x, y in tqdm(list(stream)):         scaler.learn_one(x)         x = scaler.transform_one(x)         y_pred = model.predict_one(x)         model.learn_one(x, y)         metric.update(y, y_pred)         step += 1         if step % update_interval == 0:             results.append(metric.get())             steps.append(step)     return steps, results In\u00a0[3]: Copied! <pre>class SimpleMLP(nn.Module):\n    def __init__(self, n_features):\n        super().__init__()\n        self.hidden = nn.Linear(n_features, 20)\n        self.logit = nn.Linear(20, 1)\n\n    def forward(self, x):\n        h = self.hidden(x)\n        h = torch.relu(h)\n        return self.logit(h)\n\n\nmlp = Regressor(\n    SimpleMLP(10),\n    loss_fn=\"l1\",\n    optimizer_fn=\"adam\",\n    lr=0.005,\n    seed=42,\n)\nsteps, results_mlp = test_train_eval(mlp, friedman.take(n_samples))\n</pre> class SimpleMLP(nn.Module):     def __init__(self, n_features):         super().__init__()         self.hidden = nn.Linear(n_features, 20)         self.logit = nn.Linear(20, 1)      def forward(self, x):         h = self.hidden(x)         h = torch.relu(h)         return self.logit(h)   mlp = Regressor(     SimpleMLP(10),     loss_fn=\"l1\",     optimizer_fn=\"adam\",     lr=0.005,     seed=42, ) steps, results_mlp = test_train_eval(mlp, friedman.take(n_samples)) <pre>\r  0%|                                                                                                                                                                                                                                                                                             | 0/12500 [00:00&lt;?, ?it/s]</pre> <pre>\r  1%|\u2588\u258d                                                                                                                                                                                                                                                                                 | 66/12500 [00:00&lt;00:18, 659.97it/s]</pre> <pre>\r  2%|\u2588\u2588\u2588\u2588\u258d                                                                                                                                                                                                                                                                            | 205/12500 [00:00&lt;00:11, 1086.71it/s]</pre> <pre>\r  3%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c                                                                                                                                                                                                                                                                         | 346/12500 [00:00&lt;00:09, 1232.04it/s]</pre> <pre>\r  4%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c                                                                                                                                                                                                                                                                      | 485/12500 [00:00&lt;00:09, 1293.10it/s]</pre> <pre>\r  5%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a                                                                                                                                                                                                                                                                   | 630/12500 [00:00&lt;00:08, 1347.46it/s]</pre> <pre>\r  6%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589                                                                                                                                                                                                                                                                | 773/12500 [00:00&lt;00:08, 1372.12it/s]</pre> <pre>\r  7%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588                                                                                                                                                                                                                                                             | 918/12500 [00:00&lt;00:08, 1396.86it/s]</pre> <pre>\r  8%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588                                                                                                                                                                                                                                                         | 1062/12500 [00:00&lt;00:08, 1410.36it/s]</pre> <pre>\r 10%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f                                                                                                                                                                                                                                                     | 1204/12500 [00:00&lt;00:08, 1409.68it/s]</pre> <pre>\r 11%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e                                                                                                                                                                                                                                                  | 1348/12500 [00:01&lt;00:07, 1418.42it/s]</pre> <pre>\r 12%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d                                                                                                                                                                                                                                               | 1490/12500 [00:01&lt;00:07, 1401.28it/s]</pre> <pre>\r 13%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d                                                                                                                                                                                                                                            | 1631/12500 [00:01&lt;00:07, 1399.53it/s]</pre> <pre>\r 14%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c                                                                                                                                                                                                                                         | 1774/12500 [00:01&lt;00:07, 1406.71it/s]</pre> <pre>\r 15%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b                                                                                                                                                                                                                                      | 1915/12500 [00:01&lt;00:07, 1400.51it/s]</pre> <pre>\r 16%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a                                                                                                                                                                                                                                   | 2058/12500 [00:01&lt;00:07, 1407.15it/s]</pre> <pre>\r 18%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a                                                                                                                                                                                                                                | 2199/12500 [00:01&lt;00:07, 1403.78it/s]</pre> <pre>\r 19%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589                                                                                                                                                                                                                             | 2340/12500 [00:01&lt;00:07, 1403.49it/s]</pre> <pre>\r 20%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589                                                                                                                                                                                                                          | 2481/12500 [00:01&lt;00:07, 1401.76it/s]</pre> <pre>\r 21%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588                                                                                                                                                                                                                       | 2625/12500 [00:01&lt;00:06, 1412.09it/s]</pre> <pre>\r 22%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f                                                                                                                                                                                                                   | 2767/12500 [00:02&lt;00:06, 1411.19it/s]</pre> <pre>\r 23%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e                                                                                                                                                                                                                | 2909/12500 [00:02&lt;00:06, 1411.33it/s]</pre> <pre>\r 24%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d                                                                                                                                                                                                             | 3051/12500 [00:02&lt;00:06, 1411.86it/s]</pre> <pre>\r 26%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c                                                                                                                                                                                                          | 3196/12500 [00:02&lt;00:06, 1422.43it/s]</pre> <pre>\r 27%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b                                                                                                                                                                                                       | 3339/12500 [00:02&lt;00:06, 1411.65it/s]</pre> <pre>\r 28%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589                                                                                                                                                                                                    | 3487/12500 [00:02&lt;00:06, 1429.78it/s]</pre> <pre>\r 29%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588                                                                                                                                                                                                 | 3631/12500 [00:02&lt;00:06, 1424.97it/s]</pre> <pre>\r 30%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588                                                                                                                                                                                              | 3774/12500 [00:02&lt;00:06, 1416.45it/s]</pre> <pre>\r 31%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f                                                                                                                                                                                          | 3916/12500 [00:02&lt;00:06, 1412.60it/s]</pre> <pre>\r 32%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e                                                                                                                                                                                       | 4058/12500 [00:02&lt;00:05, 1408.37it/s]</pre> <pre>\r 34%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d                                                                                                                                                                                    | 4200/12500 [00:03&lt;00:05, 1411.47it/s]</pre> <pre>\r 35%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d                                                                                                                                                                                 | 4342/12500 [00:03&lt;00:05, 1399.16it/s]</pre> <pre>\r 36%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c                                                                                                                                                                              | 4483/12500 [00:03&lt;00:05, 1402.27it/s]</pre> <pre>\r 37%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c                                                                                                                                                                           | 4624/12500 [00:03&lt;00:05, 1403.36it/s]</pre> <pre>\r 38%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b                                                                                                                                                                        | 4765/12500 [00:03&lt;00:05, 1403.40it/s]</pre> <pre>\r 39%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a                                                                                                                                                                     | 4906/12500 [00:03&lt;00:05, 1395.73it/s]</pre> <pre>\r 40%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a                                                                                                                                                                  | 5046/12500 [00:03&lt;00:05, 1395.60it/s]</pre> <pre>\r 42%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589                                                                                                                                                               | 5189/12500 [00:03&lt;00:05, 1403.23it/s]</pre> <pre>\r 43%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588                                                                                                                                                            | 5331/12500 [00:03&lt;00:05, 1406.74it/s]</pre> <pre>\r 44%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f                                                                                                                                                        | 5476/12500 [00:03&lt;00:04, 1418.81it/s]</pre> <pre>\r 45%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e                                                                                                                                                     | 5622/12500 [00:04&lt;00:04, 1428.32it/s]</pre> <pre>\r 46%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d                                                                                                                                                  | 5765/12500 [00:04&lt;00:04, 1427.56it/s]</pre> <pre>\r 47%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c                                                                                                                                               | 5908/12500 [00:04&lt;00:04, 1425.27it/s]</pre> <pre>\r 48%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b                                                                                                                                            | 6051/12500 [00:04&lt;00:04, 1419.51it/s]</pre> <pre>\r 50%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a                                                                                                                                         | 6193/12500 [00:04&lt;00:04, 1405.84it/s]</pre> <pre>\r 51%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a                                                                                                                                      | 6334/12500 [00:04&lt;00:04, 1403.59it/s]</pre> <pre>\r 52%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589                                                                                                                                   | 6478/12500 [00:04&lt;00:04, 1411.29it/s]</pre> <pre>\r 53%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588                                                                                                                                | 6620/12500 [00:04&lt;00:04, 1400.18it/s]</pre> <pre>\r 54%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f                                                                                                                            | 6762/12500 [00:04&lt;00:04, 1404.76it/s]</pre> <pre>\r 55%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f                                                                                                                         | 6904/12500 [00:04&lt;00:03, 1408.23it/s]</pre> <pre>\r 56%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e                                                                                                                      | 7045/12500 [00:05&lt;00:03, 1403.69it/s]</pre> <pre>\r 57%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e                                                                                                                   | 7186/12500 [00:05&lt;00:03, 1380.90it/s]</pre> <pre>\r 59%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d                                                                                                                | 7325/12500 [00:05&lt;00:03, 1368.55it/s]</pre> <pre>\r 60%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e                                                                                                             | 7462/12500 [00:05&lt;00:03, 1366.96it/s]</pre> <pre>\r 61%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d                                                                                                          | 7603/12500 [00:05&lt;00:03, 1376.62it/s]</pre> <pre>\r 62%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d                                                                                                       | 7741/12500 [00:05&lt;00:03, 1375.89it/s]</pre> <pre>\r 63%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d                                                                                                    | 7879/12500 [00:05&lt;00:03, 1376.27it/s]</pre> <pre>\r 64%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d                                                                                                 | 8019/12500 [00:05&lt;00:03, 1383.33it/s]</pre> <pre>\r 65%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c                                                                                              | 8161/12500 [00:05&lt;00:03, 1392.31it/s]</pre> <pre>\r 66%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b                                                                                           | 8304/12500 [00:05&lt;00:02, 1401.96it/s]</pre> <pre>\r 68%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a                                                                                        | 8446/12500 [00:06&lt;00:02, 1404.95it/s]</pre> <pre>\r 69%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589                                                                                     | 8589/12500 [00:06&lt;00:02, 1409.28it/s]</pre> <pre>\r 70%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589                                                                                  | 8730/12500 [00:06&lt;00:02, 1408.50it/s]</pre> <pre>\r 71%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588                                                                               | 8872/12500 [00:06&lt;00:02, 1408.48it/s]</pre> <pre>\r 72%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f                                                                           | 9016/12500 [00:06&lt;00:02, 1415.91it/s]</pre> <pre>\r 73%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e                                                                        | 9158/12500 [00:06&lt;00:02, 1408.82it/s]</pre> <pre>\r 74%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e                                                                     | 9299/12500 [00:06&lt;00:02, 1405.51it/s]</pre> <pre>\r 76%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d                                                                  | 9440/12500 [00:06&lt;00:02, 1402.88it/s]</pre> <pre>\r 77%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c                                                               | 9583/12500 [00:06&lt;00:02, 1408.41it/s]</pre> <pre>\r 78%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c                                                            | 9724/12500 [00:06&lt;00:01, 1405.51it/s]</pre> <pre>\r 79%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b                                                         | 9865/12500 [00:07&lt;00:01, 1402.89it/s]</pre> <pre>\r 80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589                                                      | 10006/12500 [00:07&lt;00:01, 1392.30it/s]</pre> <pre>\r 81%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588                                                   | 10148/12500 [00:07&lt;00:01, 1400.10it/s]</pre> <pre>\r 82%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588                                                | 10291/12500 [00:07&lt;00:01, 1408.55it/s]</pre> <pre>\r 83%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f                                            | 10433/12500 [00:07&lt;00:01, 1409.99it/s]</pre> <pre>\r 85%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e                                         | 10575/12500 [00:07&lt;00:01, 1406.51it/s]</pre> <pre>\r 86%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e                                      | 10716/12500 [00:07&lt;00:01, 1403.10it/s]</pre> <pre>\r 87%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d                                   | 10858/12500 [00:07&lt;00:01, 1405.25it/s]</pre> <pre>\r 88%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c                                | 11001/12500 [00:07&lt;00:01, 1411.39it/s]</pre> <pre>\r 89%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c                             | 11144/12500 [00:07&lt;00:00, 1415.28it/s]</pre> <pre>\r 90%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b                          | 11286/12500 [00:08&lt;00:00, 1414.91it/s]</pre> <pre>\r 91%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a                       | 11430/12500 [00:08&lt;00:00, 1419.34it/s]</pre> <pre>\r 93%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589                    | 11572/12500 [00:08&lt;00:00, 1411.97it/s]</pre> <pre>\r 94%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589                 | 11715/12500 [00:08&lt;00:00, 1414.61it/s]</pre> <pre>\r 95%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588              | 11857/12500 [00:08&lt;00:00, 1411.37it/s]</pre> <pre>\r 96%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f          | 11999/12500 [00:08&lt;00:00, 1408.05it/s]</pre> <pre>\r 97%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f       | 12140/12500 [00:08&lt;00:00, 1402.14it/s]</pre> <pre>\r 98%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e    | 12282/12500 [00:08&lt;00:00, 1404.86it/s]</pre> <pre>\r 99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d | 12427/12500 [00:08&lt;00:00, 1416.52it/s]</pre> <pre>\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 12500/12500 [00:08&lt;00:00, 1398.79it/s]</pre> <pre>\n</pre> In\u00a0[4]: Copied! <pre>tree = HoeffdingTreeRegressor()\nsteps, results_tree = test_train_eval(tree, friedman.take(n_samples))\n</pre> tree = HoeffdingTreeRegressor() steps, results_tree = test_train_eval(tree, friedman.take(n_samples)) <pre>\r  0%|                                                                                                                                                                                                                                                                                             | 0/12500 [00:00&lt;?, ?it/s]</pre> <pre>\r  7%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589                                                                                                                                                                                                                                                              | 865/12500 [00:00&lt;00:01, 8648.28it/s]</pre> <pre>\r 14%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b                                                                                                                                                                                                                                          | 1730/12500 [00:00&lt;00:01, 8607.06it/s]</pre> <pre>\r 22%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588                                                                                                                                                                                                                     | 2714/12500 [00:00&lt;00:01, 9095.22it/s]</pre> <pre>\r 29%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e                                                                                                                                                                                                | 3646/12500 [00:00&lt;00:00, 9180.87it/s]</pre> <pre>\r 37%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e                                                                                                                                                                            | 4564/12500 [00:00&lt;00:00, 8994.64it/s]</pre> <pre>\r 45%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e                                                                                                                                                      | 5576/12500 [00:00&lt;00:00, 9368.64it/s]</pre> <pre>\r 52%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b                                                                                                                                  | 6514/12500 [00:00&lt;00:00, 8912.22it/s]</pre> <pre>\r 59%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f                                                                                                              | 7410/12500 [00:00&lt;00:00, 8922.66it/s]</pre> <pre>\r 66%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b                                                                                           | 8306/12500 [00:00&lt;00:00, 8751.77it/s]</pre> <pre>\r 73%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a                                                                        | 9184/12500 [00:01&lt;00:00, 6431.24it/s]</pre> <pre>\r 80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c                                                       | 9950/12500 [00:01&lt;00:00, 6723.12it/s]</pre> <pre>\r 86%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d                                      | 10723/12500 [00:01&lt;00:00, 6978.26it/s]</pre> <pre>\r 94%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588                 | 11718/12500 [00:01&lt;00:00, 7767.68it/s]</pre> <pre>\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 12500/12500 [00:01&lt;00:00, 8063.74it/s]</pre> <pre>\n</pre> In\u00a0[5]: Copied! <pre>fig, ax = plt.subplots(figsize=(8, 4))\nax.plot(steps, results_mlp, label=\"MLP\")\nax.plot(steps, results_tree, label=\"Hoeffding Tree\")\nfor change_point in change_points:\n    ax.axvline(change_point, color=\"red\", alpha=0.5)\nax.set_xlim(0, n_samples)\nax.set_ylim(1, 5)\nplt.text(\n    int(change_points[0] / 2), 4, \"Concept 1\", horizontalalignment=\"center\"\n)\nplt.text(\n    int(change_points[0] + (change_points[1] - change_points[0]) / 2),\n    4,\n    \"Concept 2\",\n    horizontalalignment=\"center\",\n)\nplt.text(\n    int(change_points[1] + (n_samples - change_points[1]) / 2),\n    4,\n    \"Concept 1\",\n    horizontalalignment=\"center\",\n)\n\nax.set_xlabel(\"Steps\")\nax.set_ylabel(\"Moving MAE\")\nax.legend()\n</pre> fig, ax = plt.subplots(figsize=(8, 4)) ax.plot(steps, results_mlp, label=\"MLP\") ax.plot(steps, results_tree, label=\"Hoeffding Tree\") for change_point in change_points:     ax.axvline(change_point, color=\"red\", alpha=0.5) ax.set_xlim(0, n_samples) ax.set_ylim(1, 5) plt.text(     int(change_points[0] / 2), 4, \"Concept 1\", horizontalalignment=\"center\" ) plt.text(     int(change_points[0] + (change_points[1] - change_points[0]) / 2),     4,     \"Concept 2\",     horizontalalignment=\"center\", ) plt.text(     int(change_points[1] + (n_samples - change_points[1]) / 2),     4,     \"Concept 1\",     horizontalalignment=\"center\", )  ax.set_xlabel(\"Steps\") ax.set_ylabel(\"Moving MAE\") ax.legend() Out[5]: <pre>&lt;matplotlib.legend.Legend at 0x136ced900&gt;</pre>"},{"location":"examples/classification/example_classification/","title":"Example classification","text":"In\u00a0[1]: Copied! <pre>from river import metrics, datasets, compose, preprocessing\nfrom deep_river.classification import Classifier\nfrom torch import nn\nfrom tqdm import tqdm\n</pre> from river import metrics, datasets, compose, preprocessing from deep_river.classification import Classifier from torch import nn from tqdm import tqdm In\u00a0[2]: Copied! <pre>dataset = datasets.Phishing()\nmetric = metrics.Accuracy()\n\n\nclass MyModule(nn.Module):\n    def __init__(self, n_features):\n        super(MyModule, self).__init__()\n        self.dense0 = nn.Linear(n_features, 5)\n        self.nonlin = nn.ReLU()\n        self.dense1 = nn.Linear(5, 2)\n        self.softmax = nn.Softmax(dim=-1)\n\n    def forward(self, X, **kwargs):\n        X = self.nonlin(self.dense0(X))\n        X = self.nonlin(self.dense1(X))\n        X = self.softmax(X)\n        return X\n\n\nmodel_pipeline = compose.Pipeline(\n    preprocessing.StandardScaler,\n    Classifier(\n        module=MyModule(10), loss_fn=\"binary_cross_entropy\", optimizer_fn=\"adam\"\n    ),\n)\nmodel_pipeline\n</pre> dataset = datasets.Phishing() metric = metrics.Accuracy()   class MyModule(nn.Module):     def __init__(self, n_features):         super(MyModule, self).__init__()         self.dense0 = nn.Linear(n_features, 5)         self.nonlin = nn.ReLU()         self.dense1 = nn.Linear(5, 2)         self.softmax = nn.Softmax(dim=-1)      def forward(self, X, **kwargs):         X = self.nonlin(self.dense0(X))         X = self.nonlin(self.dense1(X))         X = self.softmax(X)         return X   model_pipeline = compose.Pipeline(     preprocessing.StandardScaler,     Classifier(         module=MyModule(10), loss_fn=\"binary_cross_entropy\", optimizer_fn=\"adam\"     ), ) model_pipeline Out[2]: <pre>StandardScaler</pre><code>StandardScaler (   with_std=True ) </code><pre>Classifier</pre><code>Classifier (   module=MyModule(   (dense0): Linear(in_features=10, out_features=5, bias=True)   (nonlin): ReLU()   (dense1): Linear(in_features=5, out_features=2, bias=True)   (softmax): Softmax(dim=-1) )   loss_fn=\"binary_cross_entropy\"   optimizer_fn=\"adam\"   lr=0.001   output_is_logit=True   is_class_incremental=False   is_feature_incremental=False   device=\"cpu\"   seed=42 ) </code> In\u00a0[3]: Copied! <pre>for x, y in dataset.take(5000):\n    y_pred = model_pipeline.predict_one(x)  # make a prediction\n    metric.update(y, y_pred)  # update the metric\n    model_pipeline.learn_one(x, y)  # make the model learn\nprint(f\"Accuracy: {metric.get()}\")\n</pre> for x, y in dataset.take(5000):     y_pred = model_pipeline.predict_one(x)  # make a prediction     metric.update(y, y_pred)  # update the metric     model_pipeline.learn_one(x, y)  # make the model learn print(f\"Accuracy: {metric.get()}\") <pre>Accuracy: 0.564\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"examples/classification/example_classification/#simple-classification-model","title":"Simple Classification Model\u00b6","text":""},{"location":"examples/classification/example_mini_batches/","title":"Example mini batches","text":"In\u00a0[1]: Copied! <pre>import pandas as pd\nfrom river import datasets\nfrom deep_river import classification\nfrom torch import nn\nfrom river import compose\nfrom river import preprocessing\nfrom itertools import islice\nfrom sklearn import metrics\n</pre> import pandas as pd from river import datasets from deep_river import classification from torch import nn from river import compose from river import preprocessing from itertools import islice from sklearn import metrics In\u00a0[2]: Copied! <pre>dataset = datasets.Phishing()\n</pre> dataset = datasets.Phishing() In\u00a0[3]: Copied! <pre>class MyModule(nn.Module):\n    def __init__(self, n_features):\n        super(MyModule, self).__init__()\n        self.dense0 = nn.Linear(n_features, 5)\n        self.nonlin = nn.ReLU()\n        self.dense1 = nn.Linear(5, 2)\n        self.softmax = nn.Softmax(dim=-1)\n\n    def forward(self, X, **kwargs):\n        X = self.nonlin(self.dense0(X))\n        X = self.nonlin(self.dense1(X))\n        X = self.softmax(X)\n        return X\n\n\ndef batcher(iterable, batch_size):\n    iterator = iter(iterable)\n    while batch := list(islice(iterator, batch_size)):\n        yield batch\n</pre> class MyModule(nn.Module):     def __init__(self, n_features):         super(MyModule, self).__init__()         self.dense0 = nn.Linear(n_features, 5)         self.nonlin = nn.ReLU()         self.dense1 = nn.Linear(5, 2)         self.softmax = nn.Softmax(dim=-1)      def forward(self, X, **kwargs):         X = self.nonlin(self.dense0(X))         X = self.nonlin(self.dense1(X))         X = self.softmax(X)         return X   def batcher(iterable, batch_size):     iterator = iter(iterable)     while batch := list(islice(iterator, batch_size)):         yield batch In\u00a0[4]: Copied! <pre>model = compose.Pipeline(\n    preprocessing.StandardScaler(),\n    classification.Classifier(\n        module=MyModule(10), loss_fn=\"binary_cross_entropy\", optimizer_fn=\"sgd\"\n    ),\n)\nmodel\n</pre> model = compose.Pipeline(     preprocessing.StandardScaler(),     classification.Classifier(         module=MyModule(10), loss_fn=\"binary_cross_entropy\", optimizer_fn=\"sgd\"     ), ) model Out[4]: <pre>StandardScaler</pre><code>StandardScaler (   with_std=True ) </code><pre>Classifier</pre><code>Classifier (   module=MyModule(   (dense0): Linear(in_features=10, out_features=5, bias=True)   (nonlin): ReLU()   (dense1): Linear(in_features=5, out_features=2, bias=True)   (softmax): Softmax(dim=-1) )   loss_fn=\"binary_cross_entropy\"   optimizer_fn=\"sgd\"   lr=0.001   output_is_logit=True   is_class_incremental=False   is_feature_incremental=False   device=\"cpu\"   seed=42 ) </code> In\u00a0[5]: Copied! <pre>y_trues = []\ny_preds = []\nfor batch in batcher(dataset, 5):\n    x, y = zip(*batch)\n    x = pd.DataFrame(x)\n    y_trues.extend(y)\n    y = pd.Series(y)\n    y_preds.extend(model.predict_many(x))\n    model.learn_many(x, y)  # make the model learn\n</pre> y_trues = [] y_preds = [] for batch in batcher(dataset, 5):     x, y = zip(*batch)     x = pd.DataFrame(x)     y_trues.extend(y)     y = pd.Series(y)     y_preds.extend(model.predict_many(x))     model.learn_many(x, y)  # make the model learn In\u00a0[6]: Copied! <pre>metrics.accuracy_score(\n    y_pred=[str(i) for i in y_preds], y_true=[str(i) for i in y_trues]\n)\n</pre> metrics.accuracy_score(     y_pred=[str(i) for i in y_preds], y_true=[str(i) for i in y_trues] ) Out[6]: <pre>0.436</pre>"},{"location":"examples/classification/example_mini_batches/#mini-batches","title":"Mini Batches\u00b6","text":"<p>Iterate over a data stream in mini batches</p>"},{"location":"examples/classification/example_rnn_classification/","title":"Example rnn classification","text":"In\u00a0[1]: Copied! <pre>from deep_river.classification import RollingClassifier\nfrom river import metrics, preprocessing, datasets\nimport torch\n</pre> from deep_river.classification import RollingClassifier from river import metrics, preprocessing, datasets import torch In\u00a0[2]: Copied! <pre>class RnnModule(torch.nn.Module):\n\n    def __init__(self, n_features, hidden_size=16, num_layers=1):\n        super().__init__()\n        self.num_layers = num_layers\n        self.n_features = n_features\n        self.hidden_size = hidden_size\n        self.rnn = torch.nn.RNN(\n            input_size=n_features, hidden_size=hidden_size, num_layers=num_layers,\n        )\n        self.linear = torch.nn.Linear(hidden_size, 2)\n\n    def forward(self, X, **kwargs):\n        out, hn = self.rnn(X) \n        hn = hn[-1]  # Take the last hidden state\n        out = self.linear(hn)\n        return torch.nn.functional.softmax(out, dim=-1)  # Return class probabilities\n</pre> class RnnModule(torch.nn.Module):      def __init__(self, n_features, hidden_size=16, num_layers=1):         super().__init__()         self.num_layers = num_layers         self.n_features = n_features         self.hidden_size = hidden_size         self.rnn = torch.nn.RNN(             input_size=n_features, hidden_size=hidden_size, num_layers=num_layers,         )         self.linear = torch.nn.Linear(hidden_size, 2)      def forward(self, X, **kwargs):         out, hn = self.rnn(X)          hn = hn[-1]  # Take the last hidden state         out = self.linear(hn)         return torch.nn.functional.softmax(out, dim=-1)  # Return class probabilities In\u00a0[3]: Copied! <pre>dataset = datasets.Keystroke()\nmetric = metrics.Accuracy()\noptimizer_fn = torch.optim.SGD\n\nmodel_pipeline = preprocessing.StandardScaler()\nmodel_pipeline |= RollingClassifier(\n    module=RnnModule(n_features=31, hidden_size=16, num_layers=2),\n    loss_fn=\"binary_cross_entropy\",\n    optimizer_fn=torch.optim.SGD,\n    window_size=20,\n    lr=1e-2,\n    append_predict=True,\n    is_class_incremental=False,\n)\nmodel_pipeline\n</pre> dataset = datasets.Keystroke() metric = metrics.Accuracy() optimizer_fn = torch.optim.SGD  model_pipeline = preprocessing.StandardScaler() model_pipeline |= RollingClassifier(     module=RnnModule(n_features=31, hidden_size=16, num_layers=2),     loss_fn=\"binary_cross_entropy\",     optimizer_fn=torch.optim.SGD,     window_size=20,     lr=1e-2,     append_predict=True,     is_class_incremental=False, ) model_pipeline Out[3]: <pre>StandardScaler</pre><code>StandardScaler (   with_std=True ) </code><pre>RollingClassifierInitialized</pre><code>RollingClassifierInitialized (   module=RnnModule(   (rnn): RNN(31, 16, num_layers=2)   (linear): Linear(in_features=16, out_features=2, bias=True) )   loss_fn=\"binary_cross_entropy\"   optimizer_fn=&lt;class 'torch.optim.sgd.SGD'&gt;   lr=0.01   output_is_logit=True   is_class_incremental=False   is_feature_incremental=False   device=\"cpu\"   seed=42   window_size=20   append_predict=True ) </code> In\u00a0[4]: Copied! <pre>for x, y in dataset:\n    y_pred = model_pipeline.predict_one(x)  # make a prediction\n    metric.update(y, y_pred)  # update the metric\n    model_pipeline.learn_one(x, y)  # make the model learn\n\nprint(f\"Accuracy: {metric.get():.2f}\")\n</pre> for x, y in dataset:     y_pred = model_pipeline.predict_one(x)  # make a prediction     metric.update(y, y_pred)  # update the metric     model_pipeline.learn_one(x, y)  # make the model learn  print(f\"Accuracy: {metric.get():.2f}\") <pre>Accuracy: 0.04\n</pre> In\u00a0[5]: Copied! <pre>class LSTMModule(torch.nn.Module):\n    def __init__(self, n_features, hidden_size=4):\n        super().__init__()\n        self.n_features = n_features\n        self.hidden_size = hidden_size\n        self.lstm = torch.nn.LSTM(\n            input_size=n_features, hidden_size=hidden_size, num_layers=1\n        )\n        self.linear = torch.nn.Linear(hidden_size, 2)\n\n    def forward(self, X, **kwargs):\n        # lstm with input, hidden, and internal state\n        output, (hn, cn) = self.lstm(X)\n        x = hn.view(-1, self.hidden_size)\n        x = self.linear(x)\n        return torch.nn.functional.softmax(x, dim=-1) \n</pre> class LSTMModule(torch.nn.Module):     def __init__(self, n_features, hidden_size=4):         super().__init__()         self.n_features = n_features         self.hidden_size = hidden_size         self.lstm = torch.nn.LSTM(             input_size=n_features, hidden_size=hidden_size, num_layers=1         )         self.linear = torch.nn.Linear(hidden_size, 2)      def forward(self, X, **kwargs):         # lstm with input, hidden, and internal state         output, (hn, cn) = self.lstm(X)         x = hn.view(-1, self.hidden_size)         x = self.linear(x)         return torch.nn.functional.softmax(x, dim=-1)  In\u00a0[6]: Copied! <pre>dataset = datasets.Keystroke()\nmetric = metrics.Accuracy()\noptimizer_fn = torch.optim.SGD\n\nmodel_pipeline = preprocessing.StandardScaler()\nmodel_pipeline |= RollingClassifier(\n    module=LSTMModule(n_features=31, hidden_size=4),\n    loss_fn=\"binary_cross_entropy\",\n    optimizer_fn=torch.optim.SGD,\n    window_size=20,\n    lr=1e-2,\n    append_predict=True,\n)\nmodel_pipeline\n</pre> dataset = datasets.Keystroke() metric = metrics.Accuracy() optimizer_fn = torch.optim.SGD  model_pipeline = preprocessing.StandardScaler() model_pipeline |= RollingClassifier(     module=LSTMModule(n_features=31, hidden_size=4),     loss_fn=\"binary_cross_entropy\",     optimizer_fn=torch.optim.SGD,     window_size=20,     lr=1e-2,     append_predict=True, ) model_pipeline Out[6]: <pre>StandardScaler</pre><code>StandardScaler (   with_std=True ) </code><pre>RollingClassifierInitialized</pre><code>RollingClassifierInitialized (   module=LSTMModule(   (lstm): LSTM(31, 4)   (linear): Linear(in_features=4, out_features=2, bias=True) )   loss_fn=\"binary_cross_entropy\"   optimizer_fn=&lt;class 'torch.optim.sgd.SGD'&gt;   lr=0.01   output_is_logit=True   is_class_incremental=False   is_feature_incremental=False   device=\"cpu\"   seed=42   window_size=20   append_predict=True ) </code> In\u00a0[7]: Copied! <pre>for x, y in dataset:\n    y_pred = model_pipeline.predict_one(x)  # make a prediction\n    metric.update(y, y_pred)  # update the metric\n    model_pipeline.learn_one(x, y)  # make the model learn\nprint(f\"Accuracy: {metric.get():.2f}\")\n</pre> for x, y in dataset:     y_pred = model_pipeline.predict_one(x)  # make a prediction     metric.update(y, y_pred)  # update the metric     model_pipeline.learn_one(x, y)  # make the model learn print(f\"Accuracy: {metric.get():.2f}\") <pre>Accuracy: 0.03\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"examples/classification/example_rnn_classification/#rnn-classification-models","title":"RNN Classification Models\u00b6","text":"<p>This example shows the application of RNN models in river-torch with and without usage of an incremental class adaption strategy.</p>"},{"location":"examples/classification/example_rnn_classification/#rnn-model","title":"RNN Model\u00b6","text":""},{"location":"examples/classification/example_rnn_classification/#classification-without-incremental-class-adapation-strategy","title":"Classification without incremental class adapation strategy\u00b6","text":""},{"location":"examples/classification/example_rnn_classification/#lstm-model","title":"LSTM Model\u00b6","text":""},{"location":"examples/classification/example_rnn_classification/#classifcation-without-incremental-class-adaption-strategy","title":"Classifcation without incremental class adaption strategy\u00b6","text":""},{"location":"examples/model_persistence/","title":"Model Persistence Examples","text":"<p>This directory contains examples demonstrating the model persistence functionality in deep-river.</p>"},{"location":"examples/model_persistence/#files","title":"Files","text":"<ul> <li><code>model_persistence_demo.ipynb</code> - Comprehensive demonstration of saving and loading models</li> </ul>"},{"location":"examples/model_persistence/#what-youll-learn","title":"What you'll learn","text":"<p>The notebook covers:</p> <ol> <li>Basic Model Persistence - Save and load classification and regression models</li> <li>Model Information - Inspect saved models before loading</li> <li>Continued Training - Resume training from saved checkpoints</li> <li>Error Handling - Robust error handling and best practices</li> <li>Model Comparison - Verify model integrity after loading</li> </ol>"},{"location":"examples/model_persistence/#key-features-demonstrated","title":"Key Features Demonstrated","text":"<ul> <li>Saving PyTorch model weights and architecture</li> <li>Preserving River model state for incremental learning</li> <li>Storing model configuration and hyperparameters</li> <li>Training metadata and statistics preservation</li> <li>Cross-session model persistence</li> <li>Production deployment readiness</li> </ul>"},{"location":"examples/model_persistence/#usage","title":"Usage","text":"<p>Open the notebook in Jupyter Lab or VS Code and run the cells sequentially to see the complete persistence workflow in action.</p> <p>The examples use standard datasets from the River library and demonstrate realistic training scenarios.</p>"},{"location":"examples/model_persistence/model_persistence_demo/","title":"Model persistence demo","text":"In\u00a0[1]: Copied! <pre>import torch.nn as nn\nfrom river import datasets, metrics\nimport tempfile\nimport os\nfrom deep_river.classification import Classifier\nfrom deep_river import base\n</pre> import torch.nn as nn from river import datasets, metrics import tempfile import os from deep_river.classification import Classifier from deep_river import base In\u00a0[2]: Copied! <pre>dataset = datasets.Phishing()\nmetric = metrics.Accuracy()\n\nclass SimpleNet(nn.Module):\n    def __init__(self, n_features):\n        super().__init__()\n        self.dense0 = nn.Linear(n_features, 5)\n        self.nonlin = nn.ReLU()\n        self.dense1 = nn.Linear(5, 2)\n        self.softmax = nn.Softmax(dim=-1)\n\n    def forward(self, X, **kwargs):\n        X = self.nonlin(self.dense0(X))\n        X = self.nonlin(self.dense1(X))\n        X = self.softmax(X)\n        return X\n\nmodel = Classifier(\n    module=SimpleNet(n_features=9),\n    loss_fn=\"binary_cross_entropy\",\n    optimizer_fn=\"adam\"\n)\nmodel\n</pre> dataset = datasets.Phishing() metric = metrics.Accuracy()  class SimpleNet(nn.Module):     def __init__(self, n_features):         super().__init__()         self.dense0 = nn.Linear(n_features, 5)         self.nonlin = nn.ReLU()         self.dense1 = nn.Linear(5, 2)         self.softmax = nn.Softmax(dim=-1)      def forward(self, X, **kwargs):         X = self.nonlin(self.dense0(X))         X = self.nonlin(self.dense1(X))         X = self.softmax(X)         return X  model = Classifier(     module=SimpleNet(n_features=9),     loss_fn=\"binary_cross_entropy\",     optimizer_fn=\"adam\" ) model Out[2]: <pre>Classifier</pre><code>Classifier (   module=SimpleNet(   (dense0): Linear(in_features=9, out_features=5, bias=True)   (nonlin): ReLU()   (dense1): Linear(in_features=5, out_features=2, bias=True)   (softmax): Softmax(dim=-1) )   loss_fn=\"binary_cross_entropy\"   optimizer_fn=\"adam\"   lr=0.001   output_is_logit=True   is_class_incremental=False   is_feature_incremental=False   device=\"cpu\"   seed=42 ) </code> <p>Train the model on 1000 samples from the phishing dataset:</p> In\u00a0[3]: Copied! <pre># Train the model\nfor x, y in dataset.take(1000):\n    y_pred = model.predict_one(x)\n    if y_pred is not None:\n        metric.update(y, y_pred)\n    model.learn_one(x, y)\n    \nprint(f\"Accuracy: {metric.get():.3f}\")\n</pre> # Train the model for x, y in dataset.take(1000):     y_pred = model.predict_one(x)     if y_pred is not None:         metric.update(y, y_pred)     model.learn_one(x, y)      print(f\"Accuracy: {metric.get():.3f}\") <pre>Accuracy: 0.725\n</pre> In\u00a0[4]: Copied! <pre># Save the model\ntemp_dir = tempfile.mkdtemp()\nmodel_path = os.path.join(temp_dir, \"classifier.pkl\")\n\nmodel.save(model_path)\nprint(f\"Model saved to: {model_path}\")\n</pre> # Save the model temp_dir = tempfile.mkdtemp() model_path = os.path.join(temp_dir, \"classifier.pkl\")  model.save(model_path) print(f\"Model saved to: {model_path}\") <pre>Model saved to: /var/folders/z7/fzv5wnys4hl1pny_xr6c0g5c0000gn/T/tmpp2azp9sr/classifier.pkl\n</pre> In\u00a0[5]: Copied! <pre># Load the model\nloaded_model = Classifier.load(model_path)\nprint(f\"Model loaded: {type(loaded_model).__name__}\")\n\n\nx, y = next(dataset.take(1))\ny_pred = loaded_model.predict_one(x)\n\nprint(f\"Prediction: {y_pred}, Expected: {y}\")\n</pre> # Load the model loaded_model = Classifier.load(model_path) print(f\"Model loaded: {type(loaded_model).__name__}\")   x, y = next(dataset.take(1)) y_pred = loaded_model.predict_one(x)  print(f\"Prediction: {y_pred}, Expected: {y}\") <pre>Model loaded: Classifier\nPrediction: True, Expected: True\n</pre> In\u00a0[6]: Copied! <pre># Clean up\nimport shutil\nshutil.rmtree(temp_dir)\nprint(\"Cleanup completed\")\n</pre> # Clean up import shutil shutil.rmtree(temp_dir) print(\"Cleanup completed\") <pre>Cleanup completed\n</pre>"},{"location":"examples/model_persistence/model_persistence_demo/#model-persistence","title":"Model Persistence\u00b6","text":"<p>Save and load pre-trained models</p> <p>This example demonstrates how to save trained deep-river models to disk and load them later for continued training or inference.</p>"},{"location":"examples/model_persistence/model_persistence_demo/#create-and-train-a-classification-model","title":"Create and Train a Classification Model\u00b6","text":"<p>We will create a simple neural network for phishing detection and train it on some data.</p>"},{"location":"examples/model_persistence/model_persistence_demo/#save-the-trained-model","title":"Save the Trained Model\u00b6","text":"<p>Now we will save the trained model to disk using the built-in save functionality:</p>"},{"location":"examples/model_persistence/model_persistence_demo/#load-the-model-and-test","title":"Load the Model and Test\u00b6","text":"<p>Load the saved model and verify it works correctly:</p>"},{"location":"examples/model_persistence/model_persistence_demo/#cleanup","title":"Cleanup\u00b6","text":"<p>Clean up temporary files:</p>"},{"location":"examples/regression/bike-sharing-forecasting/","title":"Bike sharing forecasting","text":"<p>In this tutorial we're going to forecast the number of bikes in 5 bike stations from the city of Toulouse. We'll do so by building a simple model step by step. The dataset contains 182,470 observations. Let's first take a peak at the data.</p> In\u00a0[1]: Copied! <pre>from pprint import pprint\nfrom river import datasets\n\ndataset = datasets.Bikes()\n\nfor x, y in dataset:\n    pprint(x)\n    print(f\"Number of available bikes: {y}\")\n    break\n</pre> from pprint import pprint from river import datasets  dataset = datasets.Bikes()  for x, y in dataset:     pprint(x)     print(f\"Number of available bikes: {y}\")     break <pre>{'clouds': 75,\n 'description': 'light rain',\n 'humidity': 81,\n 'moment': datetime.datetime(2016, 4, 1, 0, 0, 7),\n 'pressure': 1017.0,\n 'station': 'metro-canal-du-midi',\n 'temperature': 6.54,\n 'wind': 9.3}\nNumber of available bikes: 1\n</pre> <p>Let's start by using a simple linear regression on the numeric features. We can select the numeric features and discard the rest of the features using a <code>Select</code>. Linear regression is very likely to go haywire if we don't scale the data, so we'll use a <code>StandardScaler</code> to do just that. We'll evaluate the model by measuring the mean absolute error. Finally we'll print the score every 20,000 observations.</p> In\u00a0[2]: Copied! <pre>from river import compose\nfrom river import linear_model\nfrom river import metrics\nfrom river import evaluate\nfrom river import preprocessing\nfrom river import optim\n\nmodel = compose.Select(\"clouds\", \"humidity\", \"pressure\", \"temperature\", \"wind\")\nmodel |= preprocessing.StandardScaler()\nmodel |= linear_model.LinearRegression(optimizer=optim.SGD(0.001))\n\nmetric = metrics.MAE()\n\nevaluate.progressive_val_score(\n    dataset.take(50000), model, metric, print_every=5_000\n)\n</pre> from river import compose from river import linear_model from river import metrics from river import evaluate from river import preprocessing from river import optim  model = compose.Select(\"clouds\", \"humidity\", \"pressure\", \"temperature\", \"wind\") model |= preprocessing.StandardScaler() model |= linear_model.LinearRegression(optimizer=optim.SGD(0.001))  metric = metrics.MAE()  evaluate.progressive_val_score(     dataset.take(50000), model, metric, print_every=5_000 ) <pre>[5,000] MAE: 4.258195\n</pre> <pre>[10,000] MAE: 4.495646\n</pre> <pre>[15,000] MAE: 4.752093\n</pre> <pre>[20,000] MAE: 4.912763\n</pre> <pre>[25,000] MAE: 4.93422\n</pre> <pre>[30,000] MAE: 5.164359\n</pre> <pre>[35,000] MAE: 5.320904\n</pre> <pre>[40,000] MAE: 5.333578\n</pre> <pre>[45,000] MAE: 5.35498\n</pre> <pre>[50,000] MAE: 5.378719\n</pre> Out[2]: <pre>MAE: 5.378719</pre> <p>The model doesn't seem to be doing that well, but then again we didn't provide a lot of features. Generally, a good idea for this kind of problem is to look at an average of the previous values. For example, for each station we can look at the average number of bikes per hour. To do so we first have to extract the hour from the  <code>moment</code> field. We can then use a <code>TargetAgg</code> to aggregate the values of the target.</p> In\u00a0[3]: Copied! <pre>from river import feature_extraction\nfrom river import stats\n\n\ndef get_hour(x):\n    x[\"hour\"] = x[\"moment\"].hour\n    return x\n\n\nmodel = compose.Select(\"clouds\", \"humidity\", \"pressure\", \"temperature\", \"wind\")\nmodel += get_hour | feature_extraction.TargetAgg(\n    by=[\"station\", \"hour\"], how=stats.Mean()\n)\nmodel |= preprocessing.StandardScaler()\nmodel |= linear_model.LinearRegression(optimizer=optim.SGD(1e-2))\n\nmetric = metrics.MAE()\n\nevaluate.progressive_val_score(\n    dataset.take(50000), model, metric, print_every=5_000\n)\n</pre> from river import feature_extraction from river import stats   def get_hour(x):     x[\"hour\"] = x[\"moment\"].hour     return x   model = compose.Select(\"clouds\", \"humidity\", \"pressure\", \"temperature\", \"wind\") model += get_hour | feature_extraction.TargetAgg(     by=[\"station\", \"hour\"], how=stats.Mean() ) model |= preprocessing.StandardScaler() model |= linear_model.LinearRegression(optimizer=optim.SGD(1e-2))  metric = metrics.MAE()  evaluate.progressive_val_score(     dataset.take(50000), model, metric, print_every=5_000 ) <pre>[5,000] MAE: 70.674347\n</pre> <pre>[10,000] MAE: 37.085408\n</pre> <pre>[15,000] MAE: 25.784916\n</pre> <pre>[20,000] MAE: 20.189606\n</pre> <pre>[25,000] MAE: 16.932216\n</pre> <pre>[30,000] MAE: 14.674797\n</pre> <pre>[35,000] MAE: 13.090288\n</pre> <pre>[40,000] MAE: 11.851672\n</pre> <pre>[45,000] MAE: 10.82784\n</pre> <pre>[50,000] MAE: 10.110409\n</pre> Out[3]: <pre>MAE: 10.110409</pre> <p>By adding a single feature, we've managed to significantly reduce the mean absolute error. At this point you might think that the model is getting slightly complex, and is difficult to understand and test. Pipelines have the advantage of being terse, but they aren't always to debug. Thankfully <code>river</code> has some ways to relieve the pain.</p> <p>The first thing we can do it to visualize the pipeline, to get an idea of how the data flows through it.</p> In\u00a0[4]: Copied! <pre>model\n</pre> model Out[4]: <pre>['clouds', [...]</pre><code>Select (   clouds   humidity   pressure   temperature   wind ) </code><pre>get_hour</pre><code> def get_hour(x):     x[\"hour\"] = x[\"moment\"].hour     return x  </code><pre>y_mean_by_station_and_hour</pre><code>TargetAgg (   by=['station', 'hour']   how=Mean ()   target_name=\"y\" ) </code><pre>StandardScaler</pre><code>StandardScaler (   with_std=True ) </code><pre>LinearRegression</pre><code>LinearRegression (   optimizer=SGD (     lr=Constant (       learning_rate=0.01     )   )   loss=Squared ()   l2=0.   l1=0.   intercept_init=0.   intercept_lr=Constant (     learning_rate=0.01   )   clip_gradient=1e+12   initializer=Zeros () ) </code> <p>The <code>debug_one</code> method shows what happens to an input set of features, step by step.</p> <p>And now comes the catch. Up until now we've been using the <code>progressive_val_score</code> method from the <code>evaluate</code> module. What this does is that it sequentially predicts the output of an observation and updates the model immediately afterwards. This way of proceeding is often used for evaluating online learning models. But in some cases it is the wrong approach.</p> <p>When evaluating a machine learning model, the goal is to simulate production conditions in order to get a trust-worthy assessment of the performance of the model. In our case, we typically want to forecast the number of bikes available in a station, say, 30 minutes ahead. Then, once the 30 minutes have passed, the true number of available bikes will be available and we will be able to update the model using the features available 30 minutes ago.</p> <p>What we really want is to evaluate the model by forecasting 30 minutes ahead and only updating the model once the true values are available. This can be done using the <code>moment</code> and <code>delay</code> parameters in the  <code>progressive_val_score</code> method. The idea is that each observation in the stream of the data is shown twice to the model: once for making a prediction, and once for updating the model when the true value is revealed. The <code>moment</code> parameter determines which variable should be used as a timestamp, while the <code>delay</code> parameter controls the duration to wait before revealing the true values to the model.</p> In\u00a0[5]: Copied! <pre>import datetime as dt\n\nevaluate.progressive_val_score(\n    dataset=dataset.take(50000),\n    model=model.clone(),\n    metric=metrics.MAE(),\n    moment=\"moment\",\n    delay=dt.timedelta(minutes=30),\n    print_every=5_000,\n)\n</pre> import datetime as dt  evaluate.progressive_val_score(     dataset=dataset.take(50000),     model=model.clone(),     metric=metrics.MAE(),     moment=\"moment\",     delay=dt.timedelta(minutes=30),     print_every=5_000, ) <pre>[5,000] MAE: 68.449039\n</pre> <pre>[10,000] MAE: 36.322091\n</pre> <pre>[15,000] MAE: 25.515207\n</pre> <pre>[20,000] MAE: 20.198137\n</pre> <pre>[25,000] MAE: 17.023774\n</pre> <pre>[30,000] MAE: 14.857317\n</pre> <pre>[35,000] MAE: 13.370759\n</pre> <pre>[40,000] MAE: 12.199763\n</pre> <pre>[45,000] MAE: 11.218521\n</pre> <pre>[50,000] MAE: 10.522559\n</pre> Out[5]: <pre>MAE: 10.522559</pre> <p>The performance is a bit worse, which is to be expected. Indeed, the task is more difficult: the model is only shown the ground truth 30 minutes after making a prediction.</p> In\u00a0[6]: Copied! <pre>from deep_river.regression import Regressor\nfrom river import feature_extraction\nfrom river import stats\nimport torch\n\n\nclass LinearRegression(torch.nn.Module):\n    def __init__(self, n_features, outputSize=1):\n        super(LinearRegression, self).__init__()\n        self.linear = torch.nn.Linear(n_features, outputSize)\n\n    def forward(self, x):\n        out = self.linear(x)\n        return out\n\n\nmodel = compose.Select(\"clouds\", \"humidity\", \"pressure\", \"temperature\", \"wind\")\nmodel += get_hour | feature_extraction.TargetAgg(\n    by=[\"station\", \"hour\"], how=stats.Mean()\n)\nmodel |= preprocessing.StandardScaler()\nmodel |= Regressor(\n    module=LinearRegression(10),\n    loss_fn=\"mse\",\n    optimizer_fn=\"sgd\",\n    lr=1e-2,\n)\n</pre> from deep_river.regression import Regressor from river import feature_extraction from river import stats import torch   class LinearRegression(torch.nn.Module):     def __init__(self, n_features, outputSize=1):         super(LinearRegression, self).__init__()         self.linear = torch.nn.Linear(n_features, outputSize)      def forward(self, x):         out = self.linear(x)         return out   model = compose.Select(\"clouds\", \"humidity\", \"pressure\", \"temperature\", \"wind\") model += get_hour | feature_extraction.TargetAgg(     by=[\"station\", \"hour\"], how=stats.Mean() ) model |= preprocessing.StandardScaler() model |= Regressor(     module=LinearRegression(10),     loss_fn=\"mse\",     optimizer_fn=\"sgd\",     lr=1e-2, ) In\u00a0[7]: Copied! <pre>import datetime as dt\n\nevaluate.progressive_val_score(\n    dataset=dataset.take(50000),\n    model=model.clone(),\n    metric=metrics.MAE(),\n    moment=\"moment\",\n    delay=dt.timedelta(minutes=30),\n    print_every=5_000,\n)\n</pre> import datetime as dt  evaluate.progressive_val_score(     dataset=dataset.take(50000),     model=model.clone(),     metric=metrics.MAE(),     moment=\"moment\",     delay=dt.timedelta(minutes=30),     print_every=5_000, ) <pre>[5,000] MAE: 69.777636\n</pre> <pre>[10,000] MAE: 36.989336\n</pre> <pre>[15,000] MAE: 25.960218\n</pre> <pre>[20,000] MAE: 20.53185\n</pre> <pre>[25,000] MAE: 17.290755\n</pre> <pre>[30,000] MAE: 15.079789\n</pre> <pre>[35,000] MAE: 13.561451\n</pre> <pre>[40,000] MAE: 12.366619\n</pre> <pre>[45,000] MAE: 11.366838\n</pre> <pre>[50,000] MAE: 10.656045\n</pre> Out[7]: <pre>MAE: 10.656045</pre> In\u00a0[8]: Copied! <pre>from deep_river.regression import RollingRegressor\nfrom river import feature_extraction\nfrom river import stats\nimport torch\n\n\nclass RnnModule(torch.nn.Module):\n\n    def __init__(self, n_features, hidden_size):\n        super().__init__()\n        self.n_features = n_features\n        self.rnn = torch.nn.RNN(\n            input_size=n_features, hidden_size=hidden_size, num_layers=1\n        )\n        self.fc = torch.nn.Linear(in_features=hidden_size, out_features=1)\n\n    def forward(self, X, **kwargs):\n        output, hn = self.rnn(X)  # lstm with input, hidden, and internal state\n        return self.fc(output[-1, :])\n\n\nmodel = compose.Select(\"clouds\", \"humidity\", \"pressure\", \"temperature\", \"wind\")\nmodel += get_hour | feature_extraction.TargetAgg(\n    by=[\"station\", \"hour\"], how=stats.Mean()\n)\nmodel |= preprocessing.StandardScaler()\nmodel |= RollingRegressor(\n    module=RnnModule(10, hidden_size=16),\n    loss_fn=\"mse\",\n    optimizer_fn=\"sgd\",\n    lr=1e-2,\n    hidden_size=20,\n    window_size=32,\n)\n</pre> from deep_river.regression import RollingRegressor from river import feature_extraction from river import stats import torch   class RnnModule(torch.nn.Module):      def __init__(self, n_features, hidden_size):         super().__init__()         self.n_features = n_features         self.rnn = torch.nn.RNN(             input_size=n_features, hidden_size=hidden_size, num_layers=1         )         self.fc = torch.nn.Linear(in_features=hidden_size, out_features=1)      def forward(self, X, **kwargs):         output, hn = self.rnn(X)  # lstm with input, hidden, and internal state         return self.fc(output[-1, :])   model = compose.Select(\"clouds\", \"humidity\", \"pressure\", \"temperature\", \"wind\") model += get_hour | feature_extraction.TargetAgg(     by=[\"station\", \"hour\"], how=stats.Mean() ) model |= preprocessing.StandardScaler() model |= RollingRegressor(     module=RnnModule(10, hidden_size=16),     loss_fn=\"mse\",     optimizer_fn=\"sgd\",     lr=1e-2,     hidden_size=20,     window_size=32, ) In\u00a0[9]: Copied! <pre>import datetime as dt\n\nevaluate.progressive_val_score(\n    dataset=dataset.take(50000),\n    model=model.clone(),\n    metric=metrics.MAE(),\n    moment=\"moment\",\n    delay=dt.timedelta(minutes=30),\n    print_every=5_000,\n)\n</pre> import datetime as dt  evaluate.progressive_val_score(     dataset=dataset.take(50000),     model=model.clone(),     metric=metrics.MAE(),     moment=\"moment\",     delay=dt.timedelta(minutes=30),     print_every=5_000, ) <pre>[5,000] MAE: 4.308362\n</pre> <pre>[10,000] MAE: 4.350874\n</pre> <pre>[15,000] MAE: 4.23753\n</pre> <pre>[20,000] MAE: 4.249423\n</pre> <pre>[25,000] MAE: 4.292688\n</pre> <pre>[30,000] MAE: 4.319149\n</pre> <pre>[35,000] MAE: 4.380308\n</pre> <pre>[40,000] MAE: 4.357141\n</pre> <pre>[45,000] MAE: 4.262677\n</pre> <pre>[50,000] MAE: 4.308476\n</pre> Out[9]: <pre>MAE: 4.308476</pre> In\u00a0[10]: Copied! <pre>class LstmModule(torch.nn.Module):\n\n    def __init__(self, n_features, hidden_size=1):\n        super().__init__()\n        self.n_features = n_features\n        self.hidden_size = hidden_size\n        self.lstm = torch.nn.LSTM(\n            input_size=n_features,\n            hidden_size=hidden_size,\n            num_layers=1,\n            bidirectional=False,\n        )\n        self.fc = torch.nn.Linear(in_features=hidden_size, out_features=1)\n\n    def forward(self, X, **kwargs):\n        output, (hn, cn) = self.lstm(\n            X\n        )  # lstm with input, hidden, and internal state\n        return self.fc(output[-1, :])\n\n\nmodel = compose.Select(\"clouds\", \"humidity\", \"pressure\", \"temperature\", \"wind\")\nmodel += get_hour | feature_extraction.TargetAgg(\n    by=[\"station\", \"hour\"], how=stats.Mean()\n)\nmodel |= preprocessing.StandardScaler()\nmodel |= RollingRegressor(\n    module=LstmModule(10, hidden_size=16),\n    loss_fn=\"mse\",\n    optimizer_fn=\"sgd\",\n    lr=1e-2,\n    hidden_size=20,\n    window_size=32,\n)\n</pre> class LstmModule(torch.nn.Module):      def __init__(self, n_features, hidden_size=1):         super().__init__()         self.n_features = n_features         self.hidden_size = hidden_size         self.lstm = torch.nn.LSTM(             input_size=n_features,             hidden_size=hidden_size,             num_layers=1,             bidirectional=False,         )         self.fc = torch.nn.Linear(in_features=hidden_size, out_features=1)      def forward(self, X, **kwargs):         output, (hn, cn) = self.lstm(             X         )  # lstm with input, hidden, and internal state         return self.fc(output[-1, :])   model = compose.Select(\"clouds\", \"humidity\", \"pressure\", \"temperature\", \"wind\") model += get_hour | feature_extraction.TargetAgg(     by=[\"station\", \"hour\"], how=stats.Mean() ) model |= preprocessing.StandardScaler() model |= RollingRegressor(     module=LstmModule(10, hidden_size=16),     loss_fn=\"mse\",     optimizer_fn=\"sgd\",     lr=1e-2,     hidden_size=20,     window_size=32, ) In\u00a0[11]: Copied! <pre>import datetime as dt\n\nevaluate.progressive_val_score(\n    dataset=dataset.take(50000),\n    model=model.clone(),\n    metric=metrics.MAE(),\n    moment=\"moment\",\n    delay=dt.timedelta(minutes=30),\n    print_every=5_000,\n)\n</pre> import datetime as dt  evaluate.progressive_val_score(     dataset=dataset.take(50000),     model=model.clone(),     metric=metrics.MAE(),     moment=\"moment\",     delay=dt.timedelta(minutes=30),     print_every=5_000, ) <pre>[5,000] MAE: 4.245994\n</pre> <pre>[10,000] MAE: 4.000317\n</pre> <pre>[15,000] MAE: 3.823724\n</pre> <pre>[20,000] MAE: 3.819412\n</pre> <pre>[25,000] MAE: 3.86971\n</pre> <pre>[30,000] MAE: 3.820139\n</pre> <pre>[35,000] MAE: 3.843358\n</pre> <pre>[40,000] MAE: 3.789714\n</pre> <pre>[45,000] MAE: 3.731681\n</pre> <pre>[50,000] MAE: 3.758691\n</pre> Out[11]: <pre>MAE: 3.758691</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"examples/regression/bike-sharing-forecasting/#bike-sharing-forecasting","title":"Bike-sharing forecasting\u00b6","text":""},{"location":"examples/regression/bike-sharing-forecasting/#goin-deep","title":"Goin' Deep\u00b6","text":""},{"location":"examples/regression/bike-sharing-forecasting/#rebuilding-linear-regression-in-pytorch","title":"Rebuilding Linear Regression in PyTorch\u00b6","text":""},{"location":"examples/regression/bike-sharing-forecasting/#building-rnn-models","title":"Building RNN Models\u00b6","text":""},{"location":"examples/regression/bike-sharing-forecasting/#building-lstm-models","title":"Building LSTM Models\u00b6","text":""},{"location":"examples/regression/example_mini_batches/","title":"Example mini batches","text":"In\u00a0[1]: Copied! <pre>import pandas as pd\nfrom river import datasets\nfrom deep_river import regression\nfrom torch import nn\nfrom river import compose\nfrom river import preprocessing\nfrom itertools import islice\nfrom pprint import pprint\nfrom sklearn import metrics\n</pre> import pandas as pd from river import datasets from deep_river import regression from torch import nn from river import compose from river import preprocessing from itertools import islice from pprint import pprint from sklearn import metrics In\u00a0[2]: Copied! <pre>class MyModule(nn.Module):\n    def __init__(self, n_features):\n        super(MyModule, self).__init__()\n        self.dense0 = nn.Linear(n_features, 5)\n        self.nonlin = nn.ReLU()\n        self.dense1 = nn.Linear(5, 1)\n        self.softmax = nn.Softmax(dim=-1)\n\n    def forward(self, X, **kwargs):\n        X = self.nonlin(self.dense0(X))\n        X = self.nonlin(self.dense1(X))\n        X = self.softmax(X)\n        return X\n\n\ndef batcher(iterable, batch_size):\n    iterator = iter(iterable)\n    while batch := list(islice(iterator, batch_size)):\n        yield batch\n</pre> class MyModule(nn.Module):     def __init__(self, n_features):         super(MyModule, self).__init__()         self.dense0 = nn.Linear(n_features, 5)         self.nonlin = nn.ReLU()         self.dense1 = nn.Linear(5, 1)         self.softmax = nn.Softmax(dim=-1)      def forward(self, X, **kwargs):         X = self.nonlin(self.dense0(X))         X = self.nonlin(self.dense1(X))         X = self.softmax(X)         return X   def batcher(iterable, batch_size):     iterator = iter(iterable)     while batch := list(islice(iterator, batch_size)):         yield batch In\u00a0[3]: Copied! <pre>dataset = datasets.Bikes()\n\nfor x, y in dataset:\n    pprint(x)\n    print(f\"Number of available bikes: {y}\")\n    break\n</pre> dataset = datasets.Bikes()  for x, y in dataset:     pprint(x)     print(f\"Number of available bikes: {y}\")     break <pre>{'clouds': 75,\n 'description': 'light rain',\n 'humidity': 81,\n 'moment': datetime.datetime(2016, 4, 1, 0, 0, 7),\n 'pressure': 1017.0,\n 'station': 'metro-canal-du-midi',\n 'temperature': 6.54,\n 'wind': 9.3}\nNumber of available bikes: 1\n</pre> In\u00a0[4]: Copied! <pre>dataset = datasets.Bikes()\n\nmodel_pipeline = compose.Select(\n    \"clouds\", \"humidity\", \"pressure\", \"temperature\", \"wind\"\n)\nmodel_pipeline |= regression.Regressor(\n    module=MyModule(5), loss_fn=\"mse\", optimizer_fn=\"sgd\"\n)\nmodel_pipeline\n</pre> dataset = datasets.Bikes()  model_pipeline = compose.Select(     \"clouds\", \"humidity\", \"pressure\", \"temperature\", \"wind\" ) model_pipeline |= regression.Regressor(     module=MyModule(5), loss_fn=\"mse\", optimizer_fn=\"sgd\" ) model_pipeline Out[4]: <pre>['clouds', [...]</pre><code>Select (   clouds   humidity   pressure   temperature   wind ) </code><pre>Regressor</pre><code>Regressor (   module=MyModule(   (dense0): Linear(in_features=5, out_features=5, bias=True)   (nonlin): ReLU()   (dense1): Linear(in_features=5, out_features=1, bias=True)   (softmax): Softmax(dim=-1) )   loss_fn=\"mse\"   optimizer_fn=\"sgd\"   lr=0.001   is_feature_incremental=False   device=\"cpu\"   seed=42 ) </code> In\u00a0[5]: Copied! <pre>y_trues = []\ny_preds = []\nfor batch in batcher(dataset.take(5000), 5):\n    x, y = zip(*batch)\n    x = pd.DataFrame(x)\n    y_trues.extend(y)\n    y_preds.extend(model_pipeline.predict_many(X=x).values)\n    model_pipeline.learn_many(X=x, y=pd.Series(y))\n</pre> y_trues = [] y_preds = [] for batch in batcher(dataset.take(5000), 5):     x, y = zip(*batch)     x = pd.DataFrame(x)     y_trues.extend(y)     y_preds.extend(model_pipeline.predict_many(X=x).values)     model_pipeline.learn_many(X=x, y=pd.Series(y)) In\u00a0[6]: Copied! <pre>metrics.mean_squared_error(y_true=y_trues, y_pred=y_preds)\n</pre> metrics.mean_squared_error(y_true=y_trues, y_pred=y_preds) Out[6]: <pre>102.4412</pre>"},{"location":"examples/regression/example_regression/","title":"Example regression","text":"In\u00a0[1]: Copied! <pre>from river import (\n    metrics,\n    compose,\n    preprocessing,\n    datasets,\n    stats,\n    feature_extraction,\n)\nfrom deep_river.regression import Regressor\nfrom torch import nn\nfrom pprint import pprint\nfrom tqdm import tqdm\n</pre> from river import (     metrics,     compose,     preprocessing,     datasets,     stats,     feature_extraction, ) from deep_river.regression import Regressor from torch import nn from pprint import pprint from tqdm import tqdm In\u00a0[2]: Copied! <pre>dataset = datasets.Bikes()\n\nfor x, y in dataset:\n    pprint(x)\n    print(f\"Number of available bikes: {y}\")\n    break\n</pre> dataset = datasets.Bikes()  for x, y in dataset:     pprint(x)     print(f\"Number of available bikes: {y}\")     break <pre>{'clouds': 75,\n 'description': 'light rain',\n 'humidity': 81,\n 'moment': datetime.datetime(2016, 4, 1, 0, 0, 7),\n 'pressure': 1017.0,\n 'station': 'metro-canal-du-midi',\n 'temperature': 6.54,\n 'wind': 9.3}\nNumber of available bikes: 1\n</pre> In\u00a0[3]: Copied! <pre>class MyModule(nn.Module):\n    def __init__(self, n_features):\n        super(MyModule, self).__init__()\n        self.dense0 = nn.Linear(n_features, 5)\n        self.nonlin = nn.ReLU()\n        self.dense1 = nn.Linear(5, 1)\n        self.softmax = nn.Softmax(dim=-1)\n\n    def forward(self, X, **kwargs):\n        X = self.nonlin(self.dense0(X))\n        X = self.nonlin(self.dense1(X))\n        X = self.softmax(X)\n        return X\n\n\ndef get_hour(x):\n    x[\"hour\"] = x[\"moment\"].hour\n    return x\n</pre> class MyModule(nn.Module):     def __init__(self, n_features):         super(MyModule, self).__init__()         self.dense0 = nn.Linear(n_features, 5)         self.nonlin = nn.ReLU()         self.dense1 = nn.Linear(5, 1)         self.softmax = nn.Softmax(dim=-1)      def forward(self, X, **kwargs):         X = self.nonlin(self.dense0(X))         X = self.nonlin(self.dense1(X))         X = self.softmax(X)         return X   def get_hour(x):     x[\"hour\"] = x[\"moment\"].hour     return x In\u00a0[4]: Copied! <pre>metric = metrics.MAE()\n\nmodel_pipeline = compose.Select(\n    \"clouds\", \"humidity\", \"pressure\", \"temperature\", \"wind\"\n)\nmodel_pipeline += get_hour | feature_extraction.TargetAgg(\n    by=[\"station\", \"hour\"], how=stats.Mean()\n)\nmodel_pipeline |= preprocessing.StandardScaler()\nmodel_pipeline |= Regressor(module=MyModule(10), loss_fn=\"mse\", optimizer_fn=\"sgd\")\nmodel_pipeline\n</pre> metric = metrics.MAE()  model_pipeline = compose.Select(     \"clouds\", \"humidity\", \"pressure\", \"temperature\", \"wind\" ) model_pipeline += get_hour | feature_extraction.TargetAgg(     by=[\"station\", \"hour\"], how=stats.Mean() ) model_pipeline |= preprocessing.StandardScaler() model_pipeline |= Regressor(module=MyModule(10), loss_fn=\"mse\", optimizer_fn=\"sgd\") model_pipeline Out[4]: <pre>['clouds', [...]</pre><code>Select (   clouds   humidity   pressure   temperature   wind ) </code><pre>get_hour</pre><code> def get_hour(x):     x[\"hour\"] = x[\"moment\"].hour     return x  </code><pre>y_mean_by_station_and_hour</pre><code>TargetAgg (   by=['station', 'hour']   how=Mean ()   target_name=\"y\" ) </code><pre>StandardScaler</pre><code>StandardScaler (   with_std=True ) </code><pre>Regressor</pre><code>Regressor (   module=MyModule(   (dense0): Linear(in_features=10, out_features=5, bias=True)   (nonlin): ReLU()   (dense1): Linear(in_features=5, out_features=1, bias=True)   (softmax): Softmax(dim=-1) )   loss_fn=\"mse\"   optimizer_fn=\"sgd\"   lr=0.001   is_feature_incremental=False   device=\"cpu\"   seed=42 ) </code> In\u00a0[5]: Copied! <pre>for x, y in tqdm(dataset.take(5000)):\n    y_pred = model_pipeline.predict_one(x)\n    metric.update(y_true=y, y_pred=y_pred)\n    model_pipeline.learn_one(x=x, y=y)\nprint(f\"MAE: {metric.get():.2f}\")\n</pre> for x, y in tqdm(dataset.take(5000)):     y_pred = model_pipeline.predict_one(x)     metric.update(y_true=y, y_pred=y_pred)     model_pipeline.learn_one(x=x, y=y) print(f\"MAE: {metric.get():.2f}\") <pre>\r0it [00:00, ?it/s]</pre> <pre>\r53it [00:00, 525.82it/s]</pre> <pre>\r157it [00:00, 821.97it/s]</pre> <pre>\r258it [00:00, 906.00it/s]</pre> <pre>\r365it [00:00, 969.28it/s]</pre> <pre>\r474it [00:00, 1010.64it/s]</pre> <pre>\r585it [00:00, 1041.72it/s]</pre> <pre>\r695it [00:00, 1059.12it/s]</pre> <pre>\r806it [00:00, 1072.91it/s]</pre> <pre>\r914it [00:00, 1074.63it/s]</pre> <pre>\r1028it [00:01, 1091.78it/s]</pre> <pre>\r1147it [00:01, 1118.35it/s]</pre> <pre>\r1259it [00:01, 1106.12it/s]</pre> <pre>\r1370it [00:01, 1066.01it/s]</pre> <pre>\r1479it [00:01, 1072.76it/s]</pre> <pre>\r1587it [00:01, 1073.15it/s]</pre> <pre>\r1698it [00:01, 1083.37it/s]</pre> <pre>\r1808it [00:01, 1087.79it/s]</pre> <pre>\r1918it [00:01, 1090.43it/s]</pre> <pre>\r2028it [00:01, 1088.89it/s]</pre> <pre>\r2138it [00:02, 1092.02it/s]</pre> <pre>\r2251it [00:02, 1103.27it/s]</pre> <pre>\r2365it [00:02, 1113.66it/s]</pre> <pre>\r2477it [00:02, 1108.26it/s]</pre> <pre>\r2591it [00:02, 1115.86it/s]</pre> <pre>\r2703it [00:02, 1102.41it/s]</pre> <pre>\r2814it [00:02, 1095.06it/s]</pre> <pre>\r2934it [00:02, 1123.38it/s]</pre> <pre>\r3047it [00:02, 1123.59it/s]</pre> <pre>\r3160it [00:02, 1114.18it/s]</pre> <pre>\r3272it [00:03, 1067.75it/s]</pre> <pre>\r3380it [00:03, 1063.18it/s]</pre> <pre>\r3487it [00:03, 1064.14it/s]</pre> <pre>\r3594it [00:03, 1052.28it/s]</pre> <pre>\r3700it [00:03, 1048.48it/s]</pre> <pre>\r3808it [00:03, 1054.64it/s]</pre> <pre>\r3916it [00:03, 1059.51it/s]</pre> <pre>\r4023it [00:03, 1045.39it/s]</pre> <pre>\r4129it [00:03, 1048.36it/s]</pre> <pre>\r4239it [00:03, 1061.50it/s]</pre> <pre>\r4350it [00:04, 1074.71it/s]</pre> <pre>\r4458it [00:04, 1072.10it/s]</pre> <pre>\r4566it [00:04, 1073.53it/s]</pre> <pre>\r4675it [00:04, 1077.60it/s]</pre> <pre>\r4784it [00:04, 1079.27it/s]</pre> <pre>\r4897it [00:04, 1092.86it/s]</pre> <pre>\r5000it [00:04, 1070.85it/s]</pre> <pre>MAE: 6.83\n</pre> <pre>\n</pre>"},{"location":"examples/regression/example_regression/#simple-regression-model","title":"Simple Regression Model\u00b6","text":""},{"location":"examples/regression/example_rnn_regression/","title":"Example rnn regression","text":"In\u00a0[1]: Copied! <pre>from deep_river.regression import RollingRegressor\nfrom river import (\n    metrics,\n    compose,\n    preprocessing,\n    datasets,\n    stats,\n    feature_extraction,\n)\nfrom torch import nn\n</pre> from deep_river.regression import RollingRegressor from river import (     metrics,     compose,     preprocessing,     datasets,     stats,     feature_extraction, ) from torch import nn In\u00a0[2]: Copied! <pre>def get_hour(x):\n    x[\"hour\"] = x[\"moment\"].hour\n    return x\n</pre> def get_hour(x):     x[\"hour\"] = x[\"moment\"].hour     return x In\u00a0[3]: Copied! <pre>class RnnModule(nn.Module):\n\n    def __init__(self, n_features, hidden_size):\n        super().__init__()\n        self.n_features = n_features\n        self.rnn = nn.RNN(\n            input_size=n_features, hidden_size=hidden_size, num_layers=1\n        )\n        self.fc = nn.Linear(in_features=hidden_size, out_features=1)\n\n    def forward(self, X, **kwargs):\n        output, hn = self.rnn(X)  # lstm with input, hidden, and internal state\n        return self.fc(output[-1, :])\n</pre> class RnnModule(nn.Module):      def __init__(self, n_features, hidden_size):         super().__init__()         self.n_features = n_features         self.rnn = nn.RNN(             input_size=n_features, hidden_size=hidden_size, num_layers=1         )         self.fc = nn.Linear(in_features=hidden_size, out_features=1)      def forward(self, X, **kwargs):         output, hn = self.rnn(X)  # lstm with input, hidden, and internal state         return self.fc(output[-1, :]) In\u00a0[4]: Copied! <pre>dataset = datasets.Bikes()\nmetric = metrics.MAE()\n\nmodel_pipeline = compose.Select(\n    \"clouds\", \"humidity\", \"pressure\", \"temperature\", \"wind\"\n)\nmodel_pipeline += get_hour | feature_extraction.TargetAgg(\n    by=[\"station\", \"hour\"], how=stats.Mean()\n)\nmodel_pipeline |= preprocessing.StandardScaler()\nmodel_pipeline |= RollingRegressor(\n    module=RnnModule(10, 16),\n    loss_fn=\"mse\",\n    optimizer_fn=\"sgd\",\n    window_size=20,\n    lr=1e-2,\n    hidden_size=32,  # parameters of MyModule can be overwritten\n    append_predict=True,\n)\nmodel_pipeline\n</pre> dataset = datasets.Bikes() metric = metrics.MAE()  model_pipeline = compose.Select(     \"clouds\", \"humidity\", \"pressure\", \"temperature\", \"wind\" ) model_pipeline += get_hour | feature_extraction.TargetAgg(     by=[\"station\", \"hour\"], how=stats.Mean() ) model_pipeline |= preprocessing.StandardScaler() model_pipeline |= RollingRegressor(     module=RnnModule(10, 16),     loss_fn=\"mse\",     optimizer_fn=\"sgd\",     window_size=20,     lr=1e-2,     hidden_size=32,  # parameters of MyModule can be overwritten     append_predict=True, ) model_pipeline Out[4]: <pre>['clouds', [...]</pre><code>Select (   clouds   humidity   pressure   temperature   wind ) </code><pre>get_hour</pre><code> def get_hour(x):     x[\"hour\"] = x[\"moment\"].hour     return x  </code><pre>y_mean_by_station_and_hour</pre><code>TargetAgg (   by=['station', 'hour']   how=Mean ()   target_name=\"y\" ) </code><pre>StandardScaler</pre><code>StandardScaler (   with_std=True ) </code><pre>RollingRegressor</pre><code>RollingRegressor (   module=RnnModule(   (rnn): RNN(10, 16)   (fc): Linear(in_features=16, out_features=1, bias=True) )   loss_fn=\"mse\"   optimizer_fn=\"sgd\"   lr=0.01   is_feature_incremental=False   device=\"cpu\"   seed=42   window_size=20   append_predict=True ) </code> In\u00a0[5]: Copied! <pre>for x, y in dataset.take(5000):\n    y_pred = model_pipeline.predict_one(x)\n    metric.update(y_true=y, y_pred=y_pred)\n    model_pipeline.learn_one(x=x, y=y)\nprint(f\"MAE: {metric.get():.2f}\")\n</pre> for x, y in dataset.take(5000):     y_pred = model_pipeline.predict_one(x)     metric.update(y_true=y, y_pred=y_pred)     model_pipeline.learn_one(x=x, y=y) print(f\"MAE: {metric.get():.2f}\") <pre>MAE: 3.37\n</pre> In\u00a0[6]: Copied! <pre>class LstmModule(nn.Module):\n\n    def __init__(self, n_features, hidden_size=1):\n        super().__init__()\n        self.n_features = n_features\n        self.hidden_size = hidden_size\n        self.lstm = nn.LSTM(\n            input_size=n_features,\n            hidden_size=hidden_size,\n            num_layers=1,\n            bidirectional=False,\n        )\n        self.fc = nn.Linear(in_features=hidden_size, out_features=1)\n\n    def forward(self, X, **kwargs):\n        output, (hn, cn) = self.lstm(\n            X\n        )  # lstm with input, hidden, and internal state\n        return self.fc(output[-1, :])\n</pre> class LstmModule(nn.Module):      def __init__(self, n_features, hidden_size=1):         super().__init__()         self.n_features = n_features         self.hidden_size = hidden_size         self.lstm = nn.LSTM(             input_size=n_features,             hidden_size=hidden_size,             num_layers=1,             bidirectional=False,         )         self.fc = nn.Linear(in_features=hidden_size, out_features=1)      def forward(self, X, **kwargs):         output, (hn, cn) = self.lstm(             X         )  # lstm with input, hidden, and internal state         return self.fc(output[-1, :]) In\u00a0[7]: Copied! <pre>dataset = datasets.Bikes()\nmetric = metrics.MAE()\n\nmodel_pipeline = compose.Select(\n    \"clouds\", \"humidity\", \"pressure\", \"temperature\", \"wind\"\n)\nmodel_pipeline += get_hour | feature_extraction.TargetAgg(\n    by=[\"station\", \"hour\"], how=stats.Mean()\n)\nmodel_pipeline |= preprocessing.StandardScaler()\nmodel_pipeline |= RollingRegressor(\n    module=LstmModule(10, 16),\n    loss_fn=\"mse\",\n    optimizer_fn=\"sgd\",\n    window_size=20,\n    lr=1e-2,\n    hidden_size=32,  # parameters of MyModule can be overwritten\n    append_predict=True,\n)\nmodel_pipeline\n</pre> dataset = datasets.Bikes() metric = metrics.MAE()  model_pipeline = compose.Select(     \"clouds\", \"humidity\", \"pressure\", \"temperature\", \"wind\" ) model_pipeline += get_hour | feature_extraction.TargetAgg(     by=[\"station\", \"hour\"], how=stats.Mean() ) model_pipeline |= preprocessing.StandardScaler() model_pipeline |= RollingRegressor(     module=LstmModule(10, 16),     loss_fn=\"mse\",     optimizer_fn=\"sgd\",     window_size=20,     lr=1e-2,     hidden_size=32,  # parameters of MyModule can be overwritten     append_predict=True, ) model_pipeline Out[7]: <pre>['clouds', [...]</pre><code>Select (   clouds   humidity   pressure   temperature   wind ) </code><pre>get_hour</pre><code> def get_hour(x):     x[\"hour\"] = x[\"moment\"].hour     return x  </code><pre>y_mean_by_station_and_hour</pre><code>TargetAgg (   by=['station', 'hour']   how=Mean ()   target_name=\"y\" ) </code><pre>StandardScaler</pre><code>StandardScaler (   with_std=True ) </code><pre>RollingRegressor</pre><code>RollingRegressor (   module=LstmModule(   (lstm): LSTM(10, 16)   (fc): Linear(in_features=16, out_features=1, bias=True) )   loss_fn=\"mse\"   optimizer_fn=\"sgd\"   lr=0.01   is_feature_incremental=False   device=\"cpu\"   seed=42   window_size=20   append_predict=True ) </code> In\u00a0[8]: Copied! <pre>for x, y in dataset.take(5000):\n    y_pred = model_pipeline.predict_one(x)\n    metric.update(y_true=y, y_pred=y_pred)\n    model_pipeline.learn_one(x=x, y=y)\nprint(f\"MAE: {metric.get():.2f}\")\n</pre> for x, y in dataset.take(5000):     y_pred = model_pipeline.predict_one(x)     metric.update(y_true=y, y_pred=y_pred)     model_pipeline.learn_one(x=x, y=y) print(f\"MAE: {metric.get():.2f}\") <pre>MAE: 2.80\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"examples/regression/example_rnn_regression/#lstm-regression-model","title":"LSTM Regression Model\u00b6","text":""},{"location":"examples/regression/example_rnn_regression/#simple-rnn-regression-model","title":"Simple RNN Regression Model\u00b6","text":""},{"location":"examples/regression/example_rnn_regression/#lstm-regression-model","title":"LSTM Regression Model\u00b6","text":""},{"location":"reference/SUMMARY/","title":"SUMMARY","text":"<ul> <li>anomaly<ul> <li>ae</li> <li>probability_weighted_ae</li> <li>rolling_ae</li> <li>scaler</li> </ul> </li> <li>base</li> <li>classification<ul> <li>classifier</li> <li>rolling_classifier</li> <li>zoo</li> </ul> </li> <li>regression<ul> <li>multioutput</li> <li>regressor</li> <li>rolling_regressor</li> <li>zoo</li> </ul> </li> <li>params</li> <li>tensor_conversion</li> </ul>"},{"location":"reference/base/","title":"base","text":""},{"location":"reference/base/#deep_river.base","title":"base","text":"<p>Classes:</p> Name Description <code>DeepEstimator</code> <p>Incremental wrapper around a PyTorch module with dynamic feature adaptation.</p> <code>RollingDeepEstimator</code> <p>Extension of :class:<code>DeepEstimator</code> with a fixed-size rolling window.</p>"},{"location":"reference/base/#deep_river.base.DeepEstimator","title":"DeepEstimator","text":"<pre><code>DeepEstimator(\n    module: Module,\n    loss_fn: Union[str, Callable] = \"mse\",\n    optimizer_fn: Union[str, Callable] = \"sgd\",\n    lr: float = 0.001,\n    device: str = \"cpu\",\n    seed: int = 42,\n    is_feature_incremental: bool = False,\n    gradient_clip_value: float | None = None,\n    **kwargs\n)\n</code></pre> <p>               Bases: <code>Estimator</code></p> <p>Incremental wrapper around a PyTorch module with dynamic feature adaptation.</p> <p>This class augments a regular <code>torch.nn.Module</code> with utilities that make it compatible with the <code>river</code> incremental learning API. Beyond standard online optimisation it optionally supports feature-incremental learning: whenever previously unseen input feature names appear, the first trainable layer (the input layer) can be expanded on\u2011the\u2011fly so that the model seamlessly accepts the enlarged feature space without re\u2011initialisation.</p> <p>The class also provides a persistence protocol (<code>save</code>/<code>load</code>/<code>clone</code>) that captures both the module weights and the runtime state (observed feature names, rolling buffers, etc.), allowing exact round\u2011trips across Python sessions. Optimisers are transparently rebuilt after structural changes so any newly created parameters participate in subsequent optimisation steps.</p> Typical workflow <ol> <li>Instantiate with a vanilla PyTorch module (e.g. an <code>nn.Sequential</code> or a    custom subclass).</li> <li>Feed samples via higher level task specific subclasses (e.g. classifier)    that call <code>_learn</code> internally.</li> <li>(Optional) Enable <code>is_feature_incremental=True</code> for dynamic input growth.</li> <li>Persist with <code>save</code> and later restore with <code>load</code>.</li> </ol> Example <p>import torch from torch import nn from deep_river.base import DeepEstimator class TinyNet(nn.Module): ...     def init(self, n_features=3): ...         super().init() ...         self.fc = nn.Linear(n_features, 2) ...     def forward(self, x): ...         return self.fc(x) est = DeepEstimator( ...     module=TinyNet(3), ...     loss_fn='mse', ...     optimizer_fn='sgd', ...     is_feature_incremental=True, ... ) est._update_observed_features({'a': 1.0, 'b': 2.0, 'c': 3.0})  # internal bookkeeping True</p> Notes <ul> <li>The class itself is task\u2011agnostic. Task specific behaviour (e.g. converting   labels to one\u2011hot encodings) lives in subclasses such as <code>Classifier</code> or   <code>Regressor</code>.</li> <li>Only the first and last trainable leaf modules are treated as input and   output layers. Non\u2011parametric layers (e.g. <code>ReLU</code>) are skipped.</li> </ul> <p>Parameters:</p> Name Type Description Default <code>module</code> <code>Module</code> <p>The PyTorch model whose parameters are to be updated incrementally.</p> required <code>loss_fn</code> <code>str | Callable</code> <p>Loss identifier or callable passed to :func:<code>get_loss_fn</code>.</p> <code>'mse'</code> <code>optimizer_fn</code> <code>str | Callable</code> <p>Optimiser identifier or optimiser class / factory.</p> <code>'sgd'</code> <code>lr</code> <code>float</code> <p>Learning rate.</p> <code>1e-3</code> <code>device</code> <code>str</code> <p>Device on which the module is run.</p> <code>'cpu'</code> <code>seed</code> <code>int</code> <p>Random seed (sets <code>torch.manual_seed</code>).</p> <code>42</code> <code>is_feature_incremental</code> <code>bool</code> <p>If True, expands the input layer when new feature names are encountered.</p> <code>False</code> <code>gradient_clip_value</code> <code>float | None</code> <p>If provided, gradient norm is clipped to this value each optimisation step.</p> <code>None</code> <code>**kwargs</code> <code>dict</code> <p>Additional custom arguments retained for reconstruction on <code>clone</code> / <code>load</code>.</p> <code>{}</code> <p>Attributes:</p> Name Type Description <code>module</code> <code>Module</code> <p>The wrapped PyTorch module.</p> <code>loss_func</code> <code>Callable</code> <p>Resolved loss function callable.</p> <code>optimizer</code> <code>Optimizer</code> <p>Optimiser instance (rebuilt after structural changes).</p> <code>input_layer</code> <code>Module | None</code> <p>First trainable leaf module (may be <code>None</code> if module has no parameters).</p> <code>output_layer</code> <code>Module | None</code> <p>Last trainable leaf module.</p> <code>observed_features</code> <code>SortedSet[str]</code> <p>Ordered set of feature names seen so far.</p> <code>module_input_len</code> <code>int | None</code> <p>Cached original input size of the input layer (if identifiable).</p> <p>Methods:</p> Name Description <code>clone</code> <p>Return a fresh estimator instance with (optionally) copied state.</p> <code>draw</code> <p>Render a (partial) computational graph of the wrapped model.</p> <code>load</code> <p>Load a previously saved estimator.</p> <code>save</code> <p>Persist the estimator (architecture, weights, optimiser &amp; runtime state).</p> Source code in <code>deep_river/base.py</code> <pre><code>def __init__(\n    self,\n    module: torch.nn.Module,\n    loss_fn: Union[str, Callable] = \"mse\",\n    optimizer_fn: Union[str, Callable] = \"sgd\",\n    lr: float = 1e-3,\n    device: str = \"cpu\",\n    seed: int = 42,\n    is_feature_incremental: bool = False,\n    gradient_clip_value: float | None = None,\n    **kwargs,\n):\n    super().__init__()\n    self.module = module\n    self.lr = lr\n    self.loss_func = get_loss_fn(loss_fn)\n    self.loss_fn = loss_fn\n    self.optimizer = get_optim_fn(optimizer_fn)(\n        self.module.parameters(), lr=self.lr\n    )\n    self.optimizer_fn = optimizer_fn\n    self.device = device\n    self.seed = seed\n    self.is_feature_incremental = is_feature_incremental\n    self.gradient_clip_value = gradient_clip_value\n\n    self.kwargs = kwargs\n\n    # Explicit Optional annotations to satisfy mypy when assigning None\n    self.input_layer: Optional[torch.nn.Module] = None\n    self.output_layer: Optional[torch.nn.Module] = None\n\n    candidates = self._extract_candidate_layers(self.module)\n    # Pick the first parameterised layer as input_layer\n    for cand in candidates:\n        if any(p.requires_grad for p in cand.parameters()):\n            self.input_layer = cand\n            break\n    else:\n        self.input_layer = candidates[0] if candidates else None\n    # Pick the last parameterised layer as output_layer\n    for cand in reversed(candidates):\n        if any(p.requires_grad for p in cand.parameters()):\n            self.output_layer = cand\n            break\n    if self.output_layer is None and candidates:\n        self.output_layer = candidates[-1]\n\n    # Store initial expected input length\n    self.module_input_len = self._get_input_size() if self.input_layer else None\n    self.observed_features: SortedSet = SortedSet()\n    self.module.to(self.device)\n    torch.manual_seed(seed)\n</code></pre>"},{"location":"reference/base/#deep_river.base.DeepEstimator.clone","title":"clone","text":"<pre><code>clone(\n    new_params=None,\n    include_attributes: bool = False,\n    copy_weights: bool = False,\n)\n</code></pre> <p>Return a fresh estimator instance with (optionally) copied state.</p> <p>Parameters:</p> Name Type Description Default <code>new_params</code> <code>dict | None</code> <p>Parameter overrides for the cloned instance.</p> <code>None</code> <code>include_attributes</code> <code>bool</code> <p>If True, runtime state (observed features, buffers) is also copied.</p> <code>False</code> <code>copy_weights</code> <code>bool</code> <p>If True, model weights are copied (otherwise the module is re\u2011initialised).</p> <code>False</code> Source code in <code>deep_river/base.py</code> <pre><code>def clone(\n    self,\n    new_params=None,\n    include_attributes: bool = False,\n    copy_weights: bool = False,\n):\n    \"\"\"Return a fresh estimator instance with (optionally) copied state.\n\n    Parameters\n    ----------\n    new_params : dict | None\n        Parameter overrides for the cloned instance.\n    include_attributes : bool, default=False\n        If True, runtime state (observed features, buffers) is also copied.\n    copy_weights : bool, default=False\n        If True, model weights are copied (otherwise the module is re\u2011initialised).\n    \"\"\"\n    new_params = new_params or {}\n    copy_weights = new_params.pop(\"copy_weights\", copy_weights)\n\n    params = {**self._get_all_init_params(), **new_params}\n\n    if \"module\" not in new_params:\n        params[\"module\"] = self._rebuild_module()\n\n    new_est = self.__class__(**self._filter_kwargs(self.__class__.__init__, params))\n\n    if copy_weights and hasattr(self.module, \"state_dict\"):\n        new_est.module.load_state_dict(self.module.state_dict())\n\n    if include_attributes:\n        new_est._restore_runtime_state(self._get_runtime_state())\n\n    return new_est\n</code></pre>"},{"location":"reference/base/#deep_river.base.DeepEstimator.draw","title":"draw","text":"<pre><code>draw()\n</code></pre> <p>Render a (partial) computational graph of the wrapped model.</p> <p>Imports <code>graphviz</code> and <code>torchviz</code> lazily. Raises an informative ImportError if the optional dependencies are not installed.</p> Source code in <code>deep_river/base.py</code> <pre><code>def draw(self):  # type: ignore[override]\n    \"\"\"Render a (partial) computational graph of the wrapped model.\n\n    Imports ``graphviz`` and ``torchviz`` lazily. Raises an informative\n    ImportError if the optional dependencies are not installed.\n    \"\"\"\n    try:  # pragma: no cover\n        from torchviz import make_dot  # type: ignore\n    except Exception as err:  # noqa: BLE001\n        raise ImportError(\n            \"graphviz and torchviz must be installed to draw the model.\"\n        ) from err\n\n    first_parameter = next(self.module.parameters())\n    input_shape = first_parameter.size()\n    y_pred = self.module(torch.rand(input_shape))\n    return make_dot(y_pred.mean(), params=dict(self.module.named_parameters()))\n</code></pre>"},{"location":"reference/base/#deep_river.base.DeepEstimator.load","title":"load  <code>classmethod</code>","text":"<pre><code>load(filepath: Union[str, Path])\n</code></pre> <p>Load a previously saved estimator.</p> <p>The method reconstructs the estimator class, its wrapped module, optimiser state and runtime information (feature names, buffers, etc.).</p> Source code in <code>deep_river/base.py</code> <pre><code>@classmethod\ndef load(cls, filepath: Union[str, Path]):\n    \"\"\"Load a previously saved estimator.\n\n    The method reconstructs the estimator class, its wrapped module, optimiser\n    state and runtime information (feature names, buffers, etc.).\n    \"\"\"\n    with open(filepath, \"rb\") as f:\n        state = pickle.load(f)\n\n    estimator_cls = cls._import_from_path(state[\"estimator_class\"])\n    init_params = state[\"init_params\"]\n\n    # Rebuild module if needed\n    if \"module\" in init_params and isinstance(init_params[\"module\"], dict):\n        module_info = init_params.pop(\"module\")\n        module_cls = cls._import_from_path(module_info[\"class\"])\n        module = module_cls(\n            **cls._filter_kwargs(module_cls.__init__, module_info[\"kwargs\"])\n        )\n        if state.get(\"model_state_dict\"):\n            module.load_state_dict(state[\"model_state_dict\"])\n        init_params[\"module\"] = module\n\n    estimator = estimator_cls(\n        **cls._filter_kwargs(estimator_cls.__init__, init_params)\n    )\n\n    if state.get(\"optimizer_state_dict\") and hasattr(estimator, \"optimizer\"):\n        try:\n            estimator.optimizer.load_state_dict(\n                state[\"optimizer_state_dict\"]  # type: ignore[arg-type]\n            )\n        except Exception:  # noqa: E722\n            pass\n\n    estimator._restore_runtime_state(state.get(\"runtime_state\", {}))\n    return estimator\n</code></pre>"},{"location":"reference/base/#deep_river.base.DeepEstimator.save","title":"save","text":"<pre><code>save(filepath: Union[str, Path]) -&gt; None\n</code></pre> <p>Persist the estimator (architecture, weights, optimiser &amp; runtime state).</p> <p>Parameters:</p> Name Type Description Default <code>filepath</code> <code>str | Path</code> <p>Destination file. Parent directories are created automatically.</p> required Source code in <code>deep_river/base.py</code> <pre><code>def save(self, filepath: Union[str, Path]) -&gt; None:\n    \"\"\"Persist the estimator (architecture, weights, optimiser &amp; runtime state).\n\n    Parameters\n    ----------\n    filepath : str | Path\n        Destination file. Parent directories are created automatically.\n    \"\"\"\n    filepath = Path(filepath)\n    filepath.parent.mkdir(parents=True, exist_ok=True)\n\n    state = {\n        \"estimator_class\": f\"{type(self).__module__}.{type(self).__name__}\",\n        \"init_params\": self._get_all_init_params(),\n        \"model_state_dict\": getattr(self.module, \"state_dict\", lambda: {})(),\n        \"optimizer_state_dict\": getattr(self.optimizer, \"state_dict\", lambda: {})(),\n        \"runtime_state\": self._get_runtime_state(),\n    }\n\n    with open(filepath, \"wb\") as f:\n        pickle.dump(state, f)\n</code></pre>"},{"location":"reference/base/#deep_river.base.RollingDeepEstimator","title":"RollingDeepEstimator","text":"<pre><code>RollingDeepEstimator(\n    module: Module,\n    loss_fn: Union[str, Callable] = \"mse\",\n    optimizer_fn: Union[str, Callable] = \"sgd\",\n    lr: float = 0.001,\n    device: str = \"cpu\",\n    seed: int = 42,\n    window_size: int = 10,\n    append_predict: bool = False,\n    **kwargs\n)\n</code></pre> <p>               Bases: <code>DeepEstimator</code></p> <p>Extension of :class:<code>DeepEstimator</code> with a fixed-size rolling window.</p> <p>Maintains a <code>collections.deque</code> of the most recent <code>window_size</code> inputs enabling models (e.g. sequence learners) to condition on a short history. Optionally the model's own predictions can be appended to the window (via <code>append_predict</code>) to facilitate iterative forecasting.</p> <p>Methods:</p> Name Description <code>clone</code> <p>Return a fresh estimator instance with (optionally) copied state.</p> <code>draw</code> <p>Render a (partial) computational graph of the wrapped model.</p> <code>load</code> <p>Load a previously saved estimator.</p> <code>save</code> <p>Persist the estimator (architecture, weights, optimiser &amp; runtime state).</p> Source code in <code>deep_river/base.py</code> <pre><code>def __init__(\n    self,\n    module: torch.nn.Module,\n    loss_fn: Union[str, Callable] = \"mse\",\n    optimizer_fn: Union[str, Callable] = \"sgd\",\n    lr: float = 1e-3,\n    device: str = \"cpu\",\n    seed: int = 42,\n    window_size: int = 10,\n    append_predict: bool = False,\n    **kwargs,\n):\n    self.window_size = window_size\n    self.append_predict = append_predict\n    self._x_window: Deque = collections.deque(maxlen=window_size)\n    self._batch_i = 0\n    super().__init__(\n        module=module,\n        loss_fn=loss_fn,\n        optimizer_fn=optimizer_fn,\n        lr=lr,\n        device=device,\n        seed=seed,\n        **kwargs,\n    )\n</code></pre>"},{"location":"reference/base/#deep_river.base.RollingDeepEstimator.clone","title":"clone","text":"<pre><code>clone(\n    new_params=None,\n    include_attributes: bool = False,\n    copy_weights: bool = False,\n)\n</code></pre> <p>Return a fresh estimator instance with (optionally) copied state.</p> <p>Parameters:</p> Name Type Description Default <code>new_params</code> <code>dict | None</code> <p>Parameter overrides for the cloned instance.</p> <code>None</code> <code>include_attributes</code> <code>bool</code> <p>If True, runtime state (observed features, buffers) is also copied.</p> <code>False</code> <code>copy_weights</code> <code>bool</code> <p>If True, model weights are copied (otherwise the module is re\u2011initialised).</p> <code>False</code> Source code in <code>deep_river/base.py</code> <pre><code>def clone(\n    self,\n    new_params=None,\n    include_attributes: bool = False,\n    copy_weights: bool = False,\n):\n    \"\"\"Return a fresh estimator instance with (optionally) copied state.\n\n    Parameters\n    ----------\n    new_params : dict | None\n        Parameter overrides for the cloned instance.\n    include_attributes : bool, default=False\n        If True, runtime state (observed features, buffers) is also copied.\n    copy_weights : bool, default=False\n        If True, model weights are copied (otherwise the module is re\u2011initialised).\n    \"\"\"\n    new_params = new_params or {}\n    copy_weights = new_params.pop(\"copy_weights\", copy_weights)\n\n    params = {**self._get_all_init_params(), **new_params}\n\n    if \"module\" not in new_params:\n        params[\"module\"] = self._rebuild_module()\n\n    new_est = self.__class__(**self._filter_kwargs(self.__class__.__init__, params))\n\n    if copy_weights and hasattr(self.module, \"state_dict\"):\n        new_est.module.load_state_dict(self.module.state_dict())\n\n    if include_attributes:\n        new_est._restore_runtime_state(self._get_runtime_state())\n\n    return new_est\n</code></pre>"},{"location":"reference/base/#deep_river.base.RollingDeepEstimator.draw","title":"draw","text":"<pre><code>draw()\n</code></pre> <p>Render a (partial) computational graph of the wrapped model.</p> <p>Imports <code>graphviz</code> and <code>torchviz</code> lazily. Raises an informative ImportError if the optional dependencies are not installed.</p> Source code in <code>deep_river/base.py</code> <pre><code>def draw(self):  # type: ignore[override]\n    \"\"\"Render a (partial) computational graph of the wrapped model.\n\n    Imports ``graphviz`` and ``torchviz`` lazily. Raises an informative\n    ImportError if the optional dependencies are not installed.\n    \"\"\"\n    try:  # pragma: no cover\n        from torchviz import make_dot  # type: ignore\n    except Exception as err:  # noqa: BLE001\n        raise ImportError(\n            \"graphviz and torchviz must be installed to draw the model.\"\n        ) from err\n\n    first_parameter = next(self.module.parameters())\n    input_shape = first_parameter.size()\n    y_pred = self.module(torch.rand(input_shape))\n    return make_dot(y_pred.mean(), params=dict(self.module.named_parameters()))\n</code></pre>"},{"location":"reference/base/#deep_river.base.RollingDeepEstimator.load","title":"load  <code>classmethod</code>","text":"<pre><code>load(filepath: Union[str, Path])\n</code></pre> <p>Load a previously saved estimator.</p> <p>The method reconstructs the estimator class, its wrapped module, optimiser state and runtime information (feature names, buffers, etc.).</p> Source code in <code>deep_river/base.py</code> <pre><code>@classmethod\ndef load(cls, filepath: Union[str, Path]):\n    \"\"\"Load a previously saved estimator.\n\n    The method reconstructs the estimator class, its wrapped module, optimiser\n    state and runtime information (feature names, buffers, etc.).\n    \"\"\"\n    with open(filepath, \"rb\") as f:\n        state = pickle.load(f)\n\n    estimator_cls = cls._import_from_path(state[\"estimator_class\"])\n    init_params = state[\"init_params\"]\n\n    # Rebuild module if needed\n    if \"module\" in init_params and isinstance(init_params[\"module\"], dict):\n        module_info = init_params.pop(\"module\")\n        module_cls = cls._import_from_path(module_info[\"class\"])\n        module = module_cls(\n            **cls._filter_kwargs(module_cls.__init__, module_info[\"kwargs\"])\n        )\n        if state.get(\"model_state_dict\"):\n            module.load_state_dict(state[\"model_state_dict\"])\n        init_params[\"module\"] = module\n\n    estimator = estimator_cls(\n        **cls._filter_kwargs(estimator_cls.__init__, init_params)\n    )\n\n    if state.get(\"optimizer_state_dict\") and hasattr(estimator, \"optimizer\"):\n        try:\n            estimator.optimizer.load_state_dict(\n                state[\"optimizer_state_dict\"]  # type: ignore[arg-type]\n            )\n        except Exception:  # noqa: E722\n            pass\n\n    estimator._restore_runtime_state(state.get(\"runtime_state\", {}))\n    return estimator\n</code></pre>"},{"location":"reference/base/#deep_river.base.RollingDeepEstimator.save","title":"save","text":"<pre><code>save(filepath: Union[str, Path]) -&gt; None\n</code></pre> <p>Persist the estimator (architecture, weights, optimiser &amp; runtime state).</p> <p>Parameters:</p> Name Type Description Default <code>filepath</code> <code>str | Path</code> <p>Destination file. Parent directories are created automatically.</p> required Source code in <code>deep_river/base.py</code> <pre><code>def save(self, filepath: Union[str, Path]) -&gt; None:\n    \"\"\"Persist the estimator (architecture, weights, optimiser &amp; runtime state).\n\n    Parameters\n    ----------\n    filepath : str | Path\n        Destination file. Parent directories are created automatically.\n    \"\"\"\n    filepath = Path(filepath)\n    filepath.parent.mkdir(parents=True, exist_ok=True)\n\n    state = {\n        \"estimator_class\": f\"{type(self).__module__}.{type(self).__name__}\",\n        \"init_params\": self._get_all_init_params(),\n        \"model_state_dict\": getattr(self.module, \"state_dict\", lambda: {})(),\n        \"optimizer_state_dict\": getattr(self.optimizer, \"state_dict\", lambda: {})(),\n        \"runtime_state\": self._get_runtime_state(),\n    }\n\n    with open(filepath, \"wb\") as f:\n        pickle.dump(state, f)\n</code></pre>"},{"location":"reference/params/","title":"params","text":""},{"location":"reference/params/#deep_river.utils.params","title":"params","text":"<p>Functions:</p> Name Description <code>get_activation_fn</code> <p>Returns the requested activation function as a nn.Module class.</p> <code>get_init_fn</code> <p>Returns the requested init function.</p> <code>get_loss_fn</code> <p>Returns the requested loss function as a function.</p> <code>get_optim_fn</code> <p>Returns the requested optimizer as a nn.Module class.</p>"},{"location":"reference/params/#deep_river.utils.params.get_activation_fn","title":"get_activation_fn","text":"<pre><code>get_activation_fn(\n    activation_fn: Union[str, Callable],\n) -&gt; Callable\n</code></pre> <p>Returns the requested activation function as a nn.Module class.</p> <p>Parameters:</p> Name Type Description Default <code>activation_fn</code> <code>Union[str, Callable]</code> <p>The activation function to fetch. Can be a string or a nn.Module class.</p> required <p>Returns:</p> Type Description <code>Callable</code> <p>The class of the requested activation function.</p> Source code in <code>deep_river/utils/params.py</code> <pre><code>def get_activation_fn(activation_fn: Union[str, Callable]) -&gt; Callable:\n    \"\"\"Returns the requested activation function as a nn.Module class.\n\n    Parameters\n    ----------\n    activation_fn\n        The activation function to fetch. Can be a string or a nn.Module class.\n\n    Returns\n    -------\n    Callable\n        The class of the requested activation function.\n    \"\"\"\n    err = ValueError(\n        BASE_PARAM_ERROR.format(\"activation function\", activation_fn, \"nn.Module\")\n    )\n    if isinstance(activation_fn, str):\n        try:\n            activation_fn = ACTIVATION_FNS[activation_fn]\n        except KeyError:\n            raise err\n    elif not isinstance(activation_fn(), nn.Module):\n        raise err\n    return activation_fn\n</code></pre>"},{"location":"reference/params/#deep_river.utils.params.get_init_fn","title":"get_init_fn","text":"<pre><code>get_init_fn(init_fn)\n</code></pre> <p>Returns the requested init function.</p> <p>Parameters:</p> Name Type Description Default <code>init_fn</code> <p>The init function to fetch. Must be one of [\"xavier_uniform\", \"uniform\", \"kaiming_uniform\"].</p> required <p>Returns:</p> Type Description <code>Callable</code> <p>The class of the requested activation function.</p> Source code in <code>deep_river/utils/params.py</code> <pre><code>def get_init_fn(init_fn):\n    \"\"\"Returns the requested init function.\n\n    Parameters\n    ----------\n    init_fn\n        The init function to fetch. Must be one of [\"xavier_uniform\",\n        \"uniform\", \"kaiming_uniform\"].\n\n    Returns\n    -------\n    Callable\n        The class of the requested activation function.\n    \"\"\"\n    init_fn_ = INIT_FNS.get(init_fn, \"xavier_uniform\")\n    if init_fn.startswith(\"xavier\"):\n\n        def result(weight, activation_fn):\n            return init_fn_(weight, gain=nn.init.calculate_gain(activation_fn))\n\n    elif init_fn.startswith(\"kaiming\"):\n\n        def result(weight, activation_fn):\n            return init_fn_(weight, nonlinearity=activation_fn)\n\n    elif init_fn == \"uniform\":\n\n        def result(weight, activation_fn):\n            return 0\n\n    else:\n\n        def result(weight, activation_fn):\n            return init_fn_(weight)\n\n    return result\n</code></pre>"},{"location":"reference/params/#deep_river.utils.params.get_loss_fn","title":"get_loss_fn","text":"<pre><code>get_loss_fn(loss_fn: Union[str, Callable]) -&gt; Callable\n</code></pre> <p>Returns the requested loss function as a function.</p> <p>Parameters:</p> Name Type Description Default <code>loss_fn</code> <code>Union[str, Callable]</code> <p>The loss function to fetch. Can be a string or a function.</p> required <p>Returns:</p> Type Description <code>Callable</code> <p>The function of the requested loss function.</p> Source code in <code>deep_river/utils/params.py</code> <pre><code>def get_loss_fn(loss_fn: Union[str, Callable]) -&gt; Callable:\n    \"\"\"Returns the requested loss function as a function.\n\n    Parameters\n    ----------\n    loss_fn\n        The loss function to fetch. Can be a string or a function.\n\n    Returns\n    -------\n    Callable\n        The function of the requested loss function.\n    \"\"\"\n    err = ValueError(BASE_PARAM_ERROR.format(\"loss function\", loss_fn, \"function\"))\n    if isinstance(loss_fn, str):\n        try:\n            loss_fn = LOSS_FNS[loss_fn]\n        except KeyError:\n            raise err\n    elif not callable(loss_fn):\n        raise err\n    return loss_fn\n</code></pre>"},{"location":"reference/params/#deep_river.utils.params.get_optim_fn","title":"get_optim_fn","text":"<pre><code>get_optim_fn(optim_fn: Union[str, Callable]) -&gt; Callable\n</code></pre> <p>Returns the requested optimizer as a nn.Module class.</p> <p>Parameters:</p> Name Type Description Default <code>optim_fn</code> <code>Union[str, Callable]</code> <p>The optimizer to fetch. Can be a string or a nn.Module class.</p> required <p>Returns:</p> Type Description <code>Callable</code> <p>The class of the requested optimizer.</p> Source code in <code>deep_river/utils/params.py</code> <pre><code>def get_optim_fn(optim_fn: Union[str, Callable]) -&gt; Callable:\n    \"\"\"Returns the requested optimizer as a nn.Module class.\n\n    Parameters\n    ----------\n    optim_fn\n        The optimizer to fetch. Can be a string or a nn.Module class.\n\n\n    Returns\n    -------\n    Callable\n        The class of the requested optimizer.\n    \"\"\"\n    err = ValueError(BASE_PARAM_ERROR.format(\"optimizer\", optim_fn, \"nn.Module\"))\n    if isinstance(optim_fn, str):\n        try:\n            optim_fn = OPTIMIZER_FNS[optim_fn]\n        except KeyError:\n            raise err\n\n    elif not isinstance(\n        optim_fn(params=[torch.empty(1)], lr=1e-3), torch.optim.Optimizer\n    ):\n        raise err\n    return optim_fn\n</code></pre>"},{"location":"reference/tensor_conversion/","title":"tensor_conversion","text":""},{"location":"reference/tensor_conversion/#deep_river.utils.tensor_conversion","title":"tensor_conversion","text":"<p>Functions:</p> Name Description <code>deque2rolling_tensor</code> <p>Convert a dictionary to a rolling tensor.</p> <code>df2tensor</code> <p>Convert a dataframe to a tensor.</p> <code>dict2tensor</code> <p>Convert a dictionary to a tensor.</p> <code>float2tensor</code> <p>Convert a float to a tensor.</p> <code>labels2onehot</code> <p>Convert a label or a list of labels to a one-hot encoded tensor.</p>"},{"location":"reference/tensor_conversion/#deep_river.utils.tensor_conversion.deque2rolling_tensor","title":"deque2rolling_tensor","text":"<pre><code>deque2rolling_tensor(\n    window: Deque, device=\"cpu\", dtype=float32\n) -&gt; Tensor\n</code></pre> <p>Convert a dictionary to a rolling tensor.</p> <p>Parameters:</p> Name Type Description Default <code>window</code> <code>Deque</code> <p>Rolling window.</p> required <code>device</code> <p>Device.</p> <code>'cpu'</code> <code>dtype</code> <p>Dtype.</p> <code>float32</code> <p>Returns:</p> Type Description <code>    torch.Tensor</code> Source code in <code>deep_river/utils/tensor_conversion.py</code> <pre><code>def deque2rolling_tensor(\n    window: Deque,\n    device=\"cpu\",\n    dtype=torch.float32,\n) -&gt; torch.Tensor:\n    \"\"\"\n    Convert a dictionary to a rolling tensor.\n\n    Parameters\n    ----------\n    window\n        Rolling window.\n    device\n        Device.\n    dtype\n        Dtype.\n\n    Returns\n    -------\n        torch.Tensor\n    \"\"\"\n    output = torch.tensor(window, device=device, dtype=dtype)\n    return torch.unsqueeze(output, 1)\n</code></pre>"},{"location":"reference/tensor_conversion/#deep_river.utils.tensor_conversion.df2tensor","title":"df2tensor","text":"<pre><code>df2tensor(\n    X: DataFrame,\n    features: SortedSet,\n    default_value: float = 0.0,\n    device=\"cpu\",\n    dtype=float32,\n) -&gt; Tensor\n</code></pre> <p>Convert a dataframe to a tensor.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>DataFrame</code> <p>Dataframe.</p> required <code>features</code> <code>SortedSet</code> <p>Set of possible features.</p> required <code>default_value</code> <code>float</code> <p>Value to use for features not present in x.</p> <code>0.0</code> <code>device</code> <p>Device.</p> <code>'cpu'</code> <code>dtype</code> <p>Dtype.</p> <code>float32</code> <p>Returns:</p> Type Description <code>    torch.Tensor</code> Source code in <code>deep_river/utils/tensor_conversion.py</code> <pre><code>def df2tensor(\n    X: pd.DataFrame,\n    features: SortedSet,\n    default_value: float = 0.0,\n    device=\"cpu\",\n    dtype=torch.float32,\n) -&gt; torch.Tensor:\n    \"\"\"\n    Convert a dataframe to a tensor.\n    Parameters\n    ----------\n    X\n        Dataframe.\n    features:\n        Set of possible features.\n    default_value:\n        Value to use for features not present in x.\n    device\n        Device.\n    dtype\n        Dtype.\n\n    Returns\n    -------\n        torch.Tensor\n    \"\"\"\n    # Work on a shallow copy to avoid mutating caller's DataFrame\n    X_local = X.copy()\n    for feature in features:\n        if feature not in X_local.columns:\n            X_local[feature] = default_value\n    cols = list(features)\n    # Replace NaNs in selected columns by default_value\n    if len(cols) &gt; 0:\n        X_local[cols] = X_local[cols].fillna(default_value)\n    return torch.tensor(X_local[cols].values, device=device, dtype=dtype)\n</code></pre>"},{"location":"reference/tensor_conversion/#deep_river.utils.tensor_conversion.dict2tensor","title":"dict2tensor","text":"<pre><code>dict2tensor(\n    x: dict,\n    features: SortedSet,\n    default_value: float = 0.0,\n    device: str = \"cpu\",\n    dtype: dtype = float32,\n) -&gt; Tensor\n</code></pre> <p>Convert a dictionary to a tensor.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>dict</code> <p>Dictionary.</p> required <code>features</code> <code>SortedSet</code> <p>Set of possible features.</p> required <code>default_value</code> <code>float</code> <p>Value to use for features not present in x.</p> <code>0.0</code> <code>device</code> <code>str</code> <p>Device.</p> <code>'cpu'</code> <code>dtype</code> <code>dtype</code> <p>Dtype.</p> <code>float32</code> <p>Returns:</p> Type Description <code>    torch.Tensor</code> Source code in <code>deep_river/utils/tensor_conversion.py</code> <pre><code>def dict2tensor(\n    x: dict,\n    features: SortedSet,\n    default_value: float = 0.0,\n    device: str = \"cpu\",\n    dtype: torch.dtype = torch.float32,\n) -&gt; torch.Tensor:\n    \"\"\"\n    Convert a dictionary to a tensor.\n\n    Parameters\n    ----------\n    x\n        Dictionary.\n    features:\n        Set of possible features.\n    default_value:\n        Value to use for features not present in x.\n    device\n        Device.\n    dtype\n        Dtype.\n\n    Returns\n    -------\n        torch.Tensor\n    \"\"\"\n    row = []\n    for feature in features:\n        val = x.get(feature, default_value)\n        # Replace None/NaN with default_value to prevent NaNs propagating\n        if val is None:\n            val = default_value\n        elif isinstance(val, float) and math.isnan(val):\n            val = default_value\n        row.append(val)\n    return torch.tensor([row], device=device, dtype=dtype)\n</code></pre>"},{"location":"reference/tensor_conversion/#deep_river.utils.tensor_conversion.float2tensor","title":"float2tensor","text":"<pre><code>float2tensor(\n    y: Union[float, int, RegTarget, dict],\n    device=\"cpu\",\n    dtype=float32,\n) -&gt; Tensor\n</code></pre> <p>Convert a float to a tensor.</p> <p>Parameters:</p> Name Type Description Default <code>y</code> <code>Union[float, int, RegTarget, dict]</code> <p>Float.</p> required <code>device</code> <p>Device.</p> <code>'cpu'</code> <code>dtype</code> <p>Dtype.</p> <code>float32</code> <p>Returns:</p> Type Description <code>    torch.Tensor</code> Source code in <code>deep_river/utils/tensor_conversion.py</code> <pre><code>def float2tensor(\n    y: Union[float, int, RegTarget, dict], device=\"cpu\", dtype=torch.float32\n) -&gt; torch.Tensor:\n    \"\"\"\n    Convert a float to a tensor.\n\n    Parameters\n    ----------\n    y\n        Float.\n    device\n        Device.\n    dtype\n        Dtype.\n\n    Returns\n    -------\n        torch.Tensor\n    \"\"\"\n    if isinstance(y, dict):\n        return torch.tensor([list(y.values())], device=device, dtype=dtype)\n    else:\n        return torch.tensor([[y]], device=device, dtype=dtype)\n</code></pre>"},{"location":"reference/tensor_conversion/#deep_river.utils.tensor_conversion.labels2onehot","title":"labels2onehot","text":"<pre><code>labels2onehot(\n    y: Union[ClfTarget, Series],\n    classes: SortedSet,\n    n_classes: Optional[int] = None,\n    device=\"cpu\",\n    dtype=float32,\n) -&gt; Tensor\n</code></pre> <p>Convert a label or a list of labels to a one-hot encoded tensor.</p> <p>Parameters:</p> Name Type Description Default <code>y</code> <code>Union[ClfTarget, Series]</code> <p>Label or list of labels.</p> required <code>classes</code> <code>SortedSet</code> <p>Classes.</p> required <code>n_classes</code> <code>Optional[int]</code> <p>Number of classes.</p> <code>None</code> <code>device</code> <p>Device.</p> <code>'cpu'</code> <code>dtype</code> <p>Dtype.</p> <code>float32</code> <p>Returns:</p> Type Description <code>    torch.Tensor</code> Source code in <code>deep_river/utils/tensor_conversion.py</code> <pre><code>def labels2onehot(\n    y: Union[base.typing.ClfTarget, pd.Series],\n    classes: SortedSet,\n    n_classes: Optional[int] = None,\n    device=\"cpu\",\n    dtype=torch.float32,\n) -&gt; torch.Tensor:\n    \"\"\"\n    Convert a label or a list of labels to a one-hot encoded tensor.\n\n    Parameters\n    ----------\n    y\n        Label or list of labels.\n    classes\n        Classes.\n    n_classes\n        Number of classes.\n    device\n        Device.\n    dtype\n        Dtype.\n\n    Returns\n    -------\n        torch.Tensor\n    \"\"\"\n\n    def get_class_index(label):\n        \"\"\"Retrieve class index with type checking and conversion.\"\"\"\n        if isinstance(label, float):  # Handle float case\n            if label.is_integer():  # Convert to int if it's an integer-like float\n                label = int(label)\n            else:\n                raise ValueError(\n                    f\"Label {label} is a float and cannot be mapped to a class index.\"\n                )\n        return classes.index(label)\n\n    if n_classes is None:\n        n_classes = len(classes)\n    if isinstance(y, pd.Series):\n        onehot = torch.zeros(len(y), n_classes, device=device, dtype=dtype)\n        pos_idcs = [get_class_index(y_i) for y_i in y]\n        for i, pos_idx in enumerate(pos_idcs):\n            if isinstance(pos_idx, int) and pos_idx &lt; n_classes:\n                onehot[i, pos_idx] = 1\n    else:\n        onehot = torch.zeros(1, n_classes, device=device, dtype=dtype)\n        pos_idx = classes.index(y)\n        if isinstance(pos_idx, int) and pos_idx &lt; n_classes:\n            onehot[0, pos_idx] = 1\n\n    return onehot\n</code></pre>"},{"location":"reference/anomaly/ae/","title":"ae","text":""},{"location":"reference/anomaly/ae/#deep_river.anomaly.ae","title":"ae","text":"<p>Classes:</p> Name Description <code>Autoencoder</code> <p>Represents an initialized autoencoder for anomaly detection and feature learning.</p>"},{"location":"reference/anomaly/ae/#deep_river.anomaly.ae.Autoencoder","title":"Autoencoder","text":"<pre><code>Autoencoder(\n    module: Module,\n    loss_fn: Union[str, Callable] = \"mse\",\n    optimizer_fn: Union[str, Callable] = \"sgd\",\n    lr: float = 0.001,\n    is_feature_incremental: bool = False,\n    device: str = \"cpu\",\n    seed: int = 42,\n    **kwargs\n)\n</code></pre> <p>               Bases: <code>DeepEstimator</code>, <code>AnomalyDetector</code></p> <p>Represents an initialized autoencoder for anomaly detection and feature learning.</p> <p>This class is built upon the DeepEstimatorInitialized and AnomalyDetector base classes. It provides methods for performing unsupervised learning through an autoencoder mechanism. The primary objective of the class is to train the autoencoder on input data and compute anomaly scores based on the reconstruction error. It supports learning on individual examples or entire batches of data.</p> <p>Attributes:</p> Name Type Description <code>is_feature_incremental</code> <code>bool</code> <p>Indicates whether the model is designed to increment features dynamically.</p> <code>module</code> <code>Module</code> <p>The PyTorch model representing the autoencoder architecture.</p> <code>loss_fn</code> <code>Union[str, Callable]</code> <p>Specifies the loss function to compute the reconstruction error.</p> <code>optimizer_fn</code> <code>Union[str, Callable]</code> <p>Specifies the optimizer to be used for training the autoencoder.</p> <code>lr</code> <code>float</code> <p>The learning rate for optimization.</p> <code>device</code> <code>str</code> <p>The device on which the model is loaded and trained (e.g., \"cpu\", \"cuda\").</p> <code>seed</code> <code>int</code> <p>Random seed for ensuring reproducibility.</p> <p>Methods:</p> Name Description <code>clone</code> <p>Return a fresh estimator instance with (optionally) copied state.</p> <code>draw</code> <p>Render a (partial) computational graph of the wrapped model.</p> <code>learn_many</code> <p>Performs one step of training with a batch of examples.</p> <code>learn_one</code> <p>Performs one step of training with a single example.</p> <code>load</code> <p>Load a previously saved estimator.</p> <code>save</code> <p>Persist the estimator (architecture, weights, optimiser &amp; runtime state).</p> <code>score_many</code> <p>Returns an anomaly score for the provided batch of examples in</p> <code>score_one</code> <p>Returns an anomaly score for the provided example in the form of</p> Source code in <code>deep_river/anomaly/ae.py</code> <pre><code>def __init__(\n    self,\n    module: torch.nn.Module,\n    loss_fn: Union[str, Callable] = \"mse\",\n    optimizer_fn: Union[str, Callable] = \"sgd\",\n    lr: float = 1e-3,\n    is_feature_incremental: bool = False,\n    device: str = \"cpu\",\n    seed: int = 42,\n    **kwargs,\n):\n    super().__init__(\n        module=module,\n        loss_fn=loss_fn,\n        optimizer_fn=optimizer_fn,\n        lr=lr,\n        is_feature_incremental=is_feature_incremental,\n        device=device,\n        seed=seed,\n        **kwargs,\n    )\n    self.is_feature_incremental = is_feature_incremental\n</code></pre>"},{"location":"reference/anomaly/ae/#deep_river.anomaly.ae.Autoencoder.clone","title":"clone","text":"<pre><code>clone(\n    new_params=None,\n    include_attributes: bool = False,\n    copy_weights: bool = False,\n)\n</code></pre> <p>Return a fresh estimator instance with (optionally) copied state.</p> <p>Parameters:</p> Name Type Description Default <code>new_params</code> <code>dict | None</code> <p>Parameter overrides for the cloned instance.</p> <code>None</code> <code>include_attributes</code> <code>bool</code> <p>If True, runtime state (observed features, buffers) is also copied.</p> <code>False</code> <code>copy_weights</code> <code>bool</code> <p>If True, model weights are copied (otherwise the module is re\u2011initialised).</p> <code>False</code> Source code in <code>deep_river/base.py</code> <pre><code>def clone(\n    self,\n    new_params=None,\n    include_attributes: bool = False,\n    copy_weights: bool = False,\n):\n    \"\"\"Return a fresh estimator instance with (optionally) copied state.\n\n    Parameters\n    ----------\n    new_params : dict | None\n        Parameter overrides for the cloned instance.\n    include_attributes : bool, default=False\n        If True, runtime state (observed features, buffers) is also copied.\n    copy_weights : bool, default=False\n        If True, model weights are copied (otherwise the module is re\u2011initialised).\n    \"\"\"\n    new_params = new_params or {}\n    copy_weights = new_params.pop(\"copy_weights\", copy_weights)\n\n    params = {**self._get_all_init_params(), **new_params}\n\n    if \"module\" not in new_params:\n        params[\"module\"] = self._rebuild_module()\n\n    new_est = self.__class__(**self._filter_kwargs(self.__class__.__init__, params))\n\n    if copy_weights and hasattr(self.module, \"state_dict\"):\n        new_est.module.load_state_dict(self.module.state_dict())\n\n    if include_attributes:\n        new_est._restore_runtime_state(self._get_runtime_state())\n\n    return new_est\n</code></pre>"},{"location":"reference/anomaly/ae/#deep_river.anomaly.ae.Autoencoder.draw","title":"draw","text":"<pre><code>draw()\n</code></pre> <p>Render a (partial) computational graph of the wrapped model.</p> <p>Imports <code>graphviz</code> and <code>torchviz</code> lazily. Raises an informative ImportError if the optional dependencies are not installed.</p> Source code in <code>deep_river/base.py</code> <pre><code>def draw(self):  # type: ignore[override]\n    \"\"\"Render a (partial) computational graph of the wrapped model.\n\n    Imports ``graphviz`` and ``torchviz`` lazily. Raises an informative\n    ImportError if the optional dependencies are not installed.\n    \"\"\"\n    try:  # pragma: no cover\n        from torchviz import make_dot  # type: ignore\n    except Exception as err:  # noqa: BLE001\n        raise ImportError(\n            \"graphviz and torchviz must be installed to draw the model.\"\n        ) from err\n\n    first_parameter = next(self.module.parameters())\n    input_shape = first_parameter.size()\n    y_pred = self.module(torch.rand(input_shape))\n    return make_dot(y_pred.mean(), params=dict(self.module.named_parameters()))\n</code></pre>"},{"location":"reference/anomaly/ae/#deep_river.anomaly.ae.Autoencoder.learn_many","title":"learn_many","text":"<pre><code>learn_many(X: DataFrame) -&gt; None\n</code></pre> <p>Performs one step of training with a batch of examples.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>DataFrame</code> <p>Input batch of examples.</p> required Source code in <code>deep_river/anomaly/ae.py</code> <pre><code>def learn_many(self, X: pd.DataFrame) -&gt; None:\n    \"\"\"\n    Performs one step of training with a batch of examples.\n\n    Parameters\n    ----------\n    X\n        Input batch of examples.\n    \"\"\"\n\n    self._update_observed_features(X)\n    X_t = self._df2tensor(X)\n    self._learn(X_t)\n</code></pre>"},{"location":"reference/anomaly/ae/#deep_river.anomaly.ae.Autoencoder.learn_one","title":"learn_one","text":"<pre><code>learn_one(x: dict, y: Any = None, **kwargs) -&gt; None\n</code></pre> <p>Performs one step of training with a single example.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>dict</code> <p>Input example.</p> required <code>**kwargs</code> <code>{}</code> Source code in <code>deep_river/anomaly/ae.py</code> <pre><code>def learn_one(self, x: dict, y: Any = None, **kwargs) -&gt; None:\n    \"\"\"\n    Performs one step of training with a single example.\n\n    Parameters\n    ----------\n    x\n        Input example.\n\n    **kwargs\n    \"\"\"\n    self._update_observed_features(x)\n    self._learn(self._dict2tensor(x))\n</code></pre>"},{"location":"reference/anomaly/ae/#deep_river.anomaly.ae.Autoencoder.load","title":"load  <code>classmethod</code>","text":"<pre><code>load(filepath: Union[str, Path])\n</code></pre> <p>Load a previously saved estimator.</p> <p>The method reconstructs the estimator class, its wrapped module, optimiser state and runtime information (feature names, buffers, etc.).</p> Source code in <code>deep_river/base.py</code> <pre><code>@classmethod\ndef load(cls, filepath: Union[str, Path]):\n    \"\"\"Load a previously saved estimator.\n\n    The method reconstructs the estimator class, its wrapped module, optimiser\n    state and runtime information (feature names, buffers, etc.).\n    \"\"\"\n    with open(filepath, \"rb\") as f:\n        state = pickle.load(f)\n\n    estimator_cls = cls._import_from_path(state[\"estimator_class\"])\n    init_params = state[\"init_params\"]\n\n    # Rebuild module if needed\n    if \"module\" in init_params and isinstance(init_params[\"module\"], dict):\n        module_info = init_params.pop(\"module\")\n        module_cls = cls._import_from_path(module_info[\"class\"])\n        module = module_cls(\n            **cls._filter_kwargs(module_cls.__init__, module_info[\"kwargs\"])\n        )\n        if state.get(\"model_state_dict\"):\n            module.load_state_dict(state[\"model_state_dict\"])\n        init_params[\"module\"] = module\n\n    estimator = estimator_cls(\n        **cls._filter_kwargs(estimator_cls.__init__, init_params)\n    )\n\n    if state.get(\"optimizer_state_dict\") and hasattr(estimator, \"optimizer\"):\n        try:\n            estimator.optimizer.load_state_dict(\n                state[\"optimizer_state_dict\"]  # type: ignore[arg-type]\n            )\n        except Exception:  # noqa: E722\n            pass\n\n    estimator._restore_runtime_state(state.get(\"runtime_state\", {}))\n    return estimator\n</code></pre>"},{"location":"reference/anomaly/ae/#deep_river.anomaly.ae.Autoencoder.save","title":"save","text":"<pre><code>save(filepath: Union[str, Path]) -&gt; None\n</code></pre> <p>Persist the estimator (architecture, weights, optimiser &amp; runtime state).</p> <p>Parameters:</p> Name Type Description Default <code>filepath</code> <code>str | Path</code> <p>Destination file. Parent directories are created automatically.</p> required Source code in <code>deep_river/base.py</code> <pre><code>def save(self, filepath: Union[str, Path]) -&gt; None:\n    \"\"\"Persist the estimator (architecture, weights, optimiser &amp; runtime state).\n\n    Parameters\n    ----------\n    filepath : str | Path\n        Destination file. Parent directories are created automatically.\n    \"\"\"\n    filepath = Path(filepath)\n    filepath.parent.mkdir(parents=True, exist_ok=True)\n\n    state = {\n        \"estimator_class\": f\"{type(self).__module__}.{type(self).__name__}\",\n        \"init_params\": self._get_all_init_params(),\n        \"model_state_dict\": getattr(self.module, \"state_dict\", lambda: {})(),\n        \"optimizer_state_dict\": getattr(self.optimizer, \"state_dict\", lambda: {})(),\n        \"runtime_state\": self._get_runtime_state(),\n    }\n\n    with open(filepath, \"wb\") as f:\n        pickle.dump(state, f)\n</code></pre>"},{"location":"reference/anomaly/ae/#deep_river.anomaly.ae.Autoencoder.score_many","title":"score_many","text":"<pre><code>score_many(X: DataFrame) -&gt; ndarray\n</code></pre> <p>Returns an anomaly score for the provided batch of examples in the form of the autoencoder's reconstruction error.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <p>Input batch of examples.</p> required <p>Returns:</p> Type Description <code>float</code> <p>Anomaly scores for the given batch of examples. Larger values indicate more anomalous examples.</p> Source code in <code>deep_river/anomaly/ae.py</code> <pre><code>def score_many(self, X: pd.DataFrame) -&gt; np.ndarray:\n    \"\"\"\n    Returns an anomaly score for the provided batch of examples in\n    the form of the autoencoder's reconstruction error.\n\n    Parameters\n    ----------\n    x\n        Input batch of examples.\n\n    Returns\n    -------\n    float\n        Anomaly scores for the given batch of examples. Larger values\n        indicate more anomalous examples.\n    \"\"\"\n    self._update_observed_features(X)\n    x_t = self._df2tensor(X)\n\n    self.module.eval()\n    with torch.inference_mode():\n        x_pred = self.module(x_t)\n    loss = torch.mean(\n        self.loss_func(x_pred, x_t, reduction=\"none\"),\n        dim=list(range(1, x_t.dim())),\n    )\n    score = loss.cpu().detach().numpy()\n    return score\n</code></pre>"},{"location":"reference/anomaly/ae/#deep_river.anomaly.ae.Autoencoder.score_one","title":"score_one","text":"<pre><code>score_one(x: dict) -&gt; float\n</code></pre> <p>Returns an anomaly score for the provided example in the form of the autoencoder's reconstruction error.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>dict</code> <p>Input example.</p> required <p>Returns:</p> Type Description <code>float</code> <p>Anomaly score for the given example. Larger values indicate more anomalous examples.</p> Source code in <code>deep_river/anomaly/ae.py</code> <pre><code>def score_one(self, x: dict) -&gt; float:\n    \"\"\"\n    Returns an anomaly score for the provided example in the form of\n    the autoencoder's reconstruction error.\n\n    Parameters\n    ----------\n    x\n        Input example.\n\n    Returns\n    -------\n    float\n        Anomaly score for the given example. Larger values indicate\n        more anomalous examples.\n\n    \"\"\"\n\n    self._update_observed_features(x)\n    x_t = self._dict2tensor(x)\n    self.module.eval()\n    with torch.inference_mode():\n        x_pred = self.module(x_t)\n    loss = self.loss_func(x_pred, x_t).item()\n    return loss\n</code></pre>"},{"location":"reference/anomaly/probability_weighted_ae/","title":"probability_weighted_ae","text":""},{"location":"reference/anomaly/probability_weighted_ae/#deep_river.anomaly.probability_weighted_ae","title":"probability_weighted_ae","text":"<p>Classes:</p> Name Description <code>ProbabilityWeightedAutoencoder</code>"},{"location":"reference/anomaly/probability_weighted_ae/#deep_river.anomaly.probability_weighted_ae.ProbabilityWeightedAutoencoder","title":"ProbabilityWeightedAutoencoder","text":"<pre><code>ProbabilityWeightedAutoencoder(\n    module: Module,\n    loss_fn: Union[str, Callable] = \"mse\",\n    optimizer_fn: Union[str, Callable] = \"sgd\",\n    lr: float = 0.001,\n    device: str = \"cpu\",\n    seed: int = 42,\n    skip_threshold: float = 0.9,\n    window_size=250,\n    **kwargs\n)\n</code></pre> <p>               Bases: <code>Autoencoder</code></p> <p>Methods:</p> Name Description <code>clone</code> <p>Return a fresh estimator instance with (optionally) copied state.</p> <code>draw</code> <p>Render a (partial) computational graph of the wrapped model.</p> <code>learn_one</code> <p>Performs one step of training with a single example,</p> <code>load</code> <p>Load a previously saved estimator.</p> <code>save</code> <p>Persist the estimator (architecture, weights, optimiser &amp; runtime state).</p> <code>score_many</code> <p>Returns an anomaly score for the provided batch of examples in</p> <code>score_one</code> <p>Returns an anomaly score for the provided example in the form of</p> Source code in <code>deep_river/anomaly/probability_weighted_ae.py</code> <pre><code>def __init__(\n    self,\n    module: torch.nn.Module,\n    loss_fn: Union[str, Callable] = \"mse\",\n    optimizer_fn: Union[str, Callable] = \"sgd\",\n    lr: float = 1e-3,\n    device: str = \"cpu\",\n    seed: int = 42,\n    skip_threshold: float = 0.9,\n    window_size=250,\n    **kwargs,\n):\n    super().__init__(\n        module=module,\n        loss_fn=loss_fn,\n        optimizer_fn=optimizer_fn,\n        lr=lr,\n        device=device,\n        seed=seed,\n        **kwargs,\n    )\n    self.window_size = window_size\n    self.skip_threshold = skip_threshold\n    self.rolling_mean = utils.Rolling(stats.Mean(), window_size=window_size)\n    self.rolling_var = utils.Rolling(stats.Var(), window_size=window_size)\n</code></pre>"},{"location":"reference/anomaly/probability_weighted_ae/#deep_river.anomaly.probability_weighted_ae.ProbabilityWeightedAutoencoder.clone","title":"clone","text":"<pre><code>clone(\n    new_params=None,\n    include_attributes: bool = False,\n    copy_weights: bool = False,\n)\n</code></pre> <p>Return a fresh estimator instance with (optionally) copied state.</p> <p>Parameters:</p> Name Type Description Default <code>new_params</code> <code>dict | None</code> <p>Parameter overrides for the cloned instance.</p> <code>None</code> <code>include_attributes</code> <code>bool</code> <p>If True, runtime state (observed features, buffers) is also copied.</p> <code>False</code> <code>copy_weights</code> <code>bool</code> <p>If True, model weights are copied (otherwise the module is re\u2011initialised).</p> <code>False</code> Source code in <code>deep_river/base.py</code> <pre><code>def clone(\n    self,\n    new_params=None,\n    include_attributes: bool = False,\n    copy_weights: bool = False,\n):\n    \"\"\"Return a fresh estimator instance with (optionally) copied state.\n\n    Parameters\n    ----------\n    new_params : dict | None\n        Parameter overrides for the cloned instance.\n    include_attributes : bool, default=False\n        If True, runtime state (observed features, buffers) is also copied.\n    copy_weights : bool, default=False\n        If True, model weights are copied (otherwise the module is re\u2011initialised).\n    \"\"\"\n    new_params = new_params or {}\n    copy_weights = new_params.pop(\"copy_weights\", copy_weights)\n\n    params = {**self._get_all_init_params(), **new_params}\n\n    if \"module\" not in new_params:\n        params[\"module\"] = self._rebuild_module()\n\n    new_est = self.__class__(**self._filter_kwargs(self.__class__.__init__, params))\n\n    if copy_weights and hasattr(self.module, \"state_dict\"):\n        new_est.module.load_state_dict(self.module.state_dict())\n\n    if include_attributes:\n        new_est._restore_runtime_state(self._get_runtime_state())\n\n    return new_est\n</code></pre>"},{"location":"reference/anomaly/probability_weighted_ae/#deep_river.anomaly.probability_weighted_ae.ProbabilityWeightedAutoencoder.draw","title":"draw","text":"<pre><code>draw()\n</code></pre> <p>Render a (partial) computational graph of the wrapped model.</p> <p>Imports <code>graphviz</code> and <code>torchviz</code> lazily. Raises an informative ImportError if the optional dependencies are not installed.</p> Source code in <code>deep_river/base.py</code> <pre><code>def draw(self):  # type: ignore[override]\n    \"\"\"Render a (partial) computational graph of the wrapped model.\n\n    Imports ``graphviz`` and ``torchviz`` lazily. Raises an informative\n    ImportError if the optional dependencies are not installed.\n    \"\"\"\n    try:  # pragma: no cover\n        from torchviz import make_dot  # type: ignore\n    except Exception as err:  # noqa: BLE001\n        raise ImportError(\n            \"graphviz and torchviz must be installed to draw the model.\"\n        ) from err\n\n    first_parameter = next(self.module.parameters())\n    input_shape = first_parameter.size()\n    y_pred = self.module(torch.rand(input_shape))\n    return make_dot(y_pred.mean(), params=dict(self.module.named_parameters()))\n</code></pre>"},{"location":"reference/anomaly/probability_weighted_ae/#deep_river.anomaly.probability_weighted_ae.ProbabilityWeightedAutoencoder.learn_one","title":"learn_one","text":"<pre><code>learn_one(x: dict, y: Any = None, **kwargs) -&gt; None\n</code></pre> <p>Performs one step of training with a single example, scaling the employed learning rate based on the outlier probability estimate of the input example.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <code>{}</code> <code>x</code> <code>dict</code> <p>Input example.</p> required <p>Returns:</p> Type Description <code>ProbabilityWeightedAutoencoder</code> <p>The autoencoder itself.</p> Source code in <code>deep_river/anomaly/probability_weighted_ae.py</code> <pre><code>def learn_one(self, x: dict, y: Any = None, **kwargs) -&gt; None:\n    \"\"\"\n    Performs one step of training with a single example,\n    scaling the employed learning rate based on the outlier\n    probability estimate of the input example.\n\n    Parameters\n    ----------\n    **kwargs\n    x\n        Input example.\n\n    Returns\n    -------\n    ProbabilityWeightedAutoencoder\n        The autoencoder itself.\n    \"\"\"\n\n    self._update_observed_features(x)\n    x_t = self._dict2tensor(x)\n\n    self.module.train()\n    x_pred = self.module(x_t)\n    loss = self.loss_func(x_pred, x_t)\n    self._apply_loss(loss)\n</code></pre>"},{"location":"reference/anomaly/probability_weighted_ae/#deep_river.anomaly.probability_weighted_ae.ProbabilityWeightedAutoencoder.load","title":"load  <code>classmethod</code>","text":"<pre><code>load(filepath: Union[str, Path])\n</code></pre> <p>Load a previously saved estimator.</p> <p>The method reconstructs the estimator class, its wrapped module, optimiser state and runtime information (feature names, buffers, etc.).</p> Source code in <code>deep_river/base.py</code> <pre><code>@classmethod\ndef load(cls, filepath: Union[str, Path]):\n    \"\"\"Load a previously saved estimator.\n\n    The method reconstructs the estimator class, its wrapped module, optimiser\n    state and runtime information (feature names, buffers, etc.).\n    \"\"\"\n    with open(filepath, \"rb\") as f:\n        state = pickle.load(f)\n\n    estimator_cls = cls._import_from_path(state[\"estimator_class\"])\n    init_params = state[\"init_params\"]\n\n    # Rebuild module if needed\n    if \"module\" in init_params and isinstance(init_params[\"module\"], dict):\n        module_info = init_params.pop(\"module\")\n        module_cls = cls._import_from_path(module_info[\"class\"])\n        module = module_cls(\n            **cls._filter_kwargs(module_cls.__init__, module_info[\"kwargs\"])\n        )\n        if state.get(\"model_state_dict\"):\n            module.load_state_dict(state[\"model_state_dict\"])\n        init_params[\"module\"] = module\n\n    estimator = estimator_cls(\n        **cls._filter_kwargs(estimator_cls.__init__, init_params)\n    )\n\n    if state.get(\"optimizer_state_dict\") and hasattr(estimator, \"optimizer\"):\n        try:\n            estimator.optimizer.load_state_dict(\n                state[\"optimizer_state_dict\"]  # type: ignore[arg-type]\n            )\n        except Exception:  # noqa: E722\n            pass\n\n    estimator._restore_runtime_state(state.get(\"runtime_state\", {}))\n    return estimator\n</code></pre>"},{"location":"reference/anomaly/probability_weighted_ae/#deep_river.anomaly.probability_weighted_ae.ProbabilityWeightedAutoencoder.save","title":"save","text":"<pre><code>save(filepath: Union[str, Path]) -&gt; None\n</code></pre> <p>Persist the estimator (architecture, weights, optimiser &amp; runtime state).</p> <p>Parameters:</p> Name Type Description Default <code>filepath</code> <code>str | Path</code> <p>Destination file. Parent directories are created automatically.</p> required Source code in <code>deep_river/base.py</code> <pre><code>def save(self, filepath: Union[str, Path]) -&gt; None:\n    \"\"\"Persist the estimator (architecture, weights, optimiser &amp; runtime state).\n\n    Parameters\n    ----------\n    filepath : str | Path\n        Destination file. Parent directories are created automatically.\n    \"\"\"\n    filepath = Path(filepath)\n    filepath.parent.mkdir(parents=True, exist_ok=True)\n\n    state = {\n        \"estimator_class\": f\"{type(self).__module__}.{type(self).__name__}\",\n        \"init_params\": self._get_all_init_params(),\n        \"model_state_dict\": getattr(self.module, \"state_dict\", lambda: {})(),\n        \"optimizer_state_dict\": getattr(self.optimizer, \"state_dict\", lambda: {})(),\n        \"runtime_state\": self._get_runtime_state(),\n    }\n\n    with open(filepath, \"wb\") as f:\n        pickle.dump(state, f)\n</code></pre>"},{"location":"reference/anomaly/probability_weighted_ae/#deep_river.anomaly.probability_weighted_ae.ProbabilityWeightedAutoencoder.score_many","title":"score_many","text":"<pre><code>score_many(X: DataFrame) -&gt; ndarray\n</code></pre> <p>Returns an anomaly score for the provided batch of examples in the form of the autoencoder's reconstruction error.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <p>Input batch of examples.</p> required <p>Returns:</p> Type Description <code>float</code> <p>Anomaly scores for the given batch of examples. Larger values indicate more anomalous examples.</p> Source code in <code>deep_river/anomaly/ae.py</code> <pre><code>def score_many(self, X: pd.DataFrame) -&gt; np.ndarray:\n    \"\"\"\n    Returns an anomaly score for the provided batch of examples in\n    the form of the autoencoder's reconstruction error.\n\n    Parameters\n    ----------\n    x\n        Input batch of examples.\n\n    Returns\n    -------\n    float\n        Anomaly scores for the given batch of examples. Larger values\n        indicate more anomalous examples.\n    \"\"\"\n    self._update_observed_features(X)\n    x_t = self._df2tensor(X)\n\n    self.module.eval()\n    with torch.inference_mode():\n        x_pred = self.module(x_t)\n    loss = torch.mean(\n        self.loss_func(x_pred, x_t, reduction=\"none\"),\n        dim=list(range(1, x_t.dim())),\n    )\n    score = loss.cpu().detach().numpy()\n    return score\n</code></pre>"},{"location":"reference/anomaly/probability_weighted_ae/#deep_river.anomaly.probability_weighted_ae.ProbabilityWeightedAutoencoder.score_one","title":"score_one","text":"<pre><code>score_one(x: dict) -&gt; float\n</code></pre> <p>Returns an anomaly score for the provided example in the form of the autoencoder's reconstruction error.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>dict</code> <p>Input example.</p> required <p>Returns:</p> Type Description <code>float</code> <p>Anomaly score for the given example. Larger values indicate more anomalous examples.</p> Source code in <code>deep_river/anomaly/ae.py</code> <pre><code>def score_one(self, x: dict) -&gt; float:\n    \"\"\"\n    Returns an anomaly score for the provided example in the form of\n    the autoencoder's reconstruction error.\n\n    Parameters\n    ----------\n    x\n        Input example.\n\n    Returns\n    -------\n    float\n        Anomaly score for the given example. Larger values indicate\n        more anomalous examples.\n\n    \"\"\"\n\n    self._update_observed_features(x)\n    x_t = self._dict2tensor(x)\n    self.module.eval()\n    with torch.inference_mode():\n        x_pred = self.module(x_t)\n    loss = self.loss_func(x_pred, x_t).item()\n    return loss\n</code></pre>"},{"location":"reference/anomaly/rolling_ae/","title":"rolling_ae","text":""},{"location":"reference/anomaly/rolling_ae/#deep_river.anomaly.rolling_ae","title":"rolling_ae","text":"<p>Classes:</p> Name Description <code>RollingAutoencoder</code> <p>Rolling window autoencoder for streaming anomaly detection.</p>"},{"location":"reference/anomaly/rolling_ae/#deep_river.anomaly.rolling_ae.RollingAutoencoder","title":"RollingAutoencoder","text":"<pre><code>RollingAutoencoder(\n    module: Module,\n    loss_fn: Union[str, Callable] = \"mse\",\n    optimizer_fn: Union[str, Callable] = \"sgd\",\n    lr: float = 0.001,\n    device: str = \"cpu\",\n    seed: int = 42,\n    window_size: int = 10,\n    append_predict: bool = False,\n    **kwargs\n)\n</code></pre> <p>               Bases: <code>RollingDeepEstimator</code>, <code>AnomalyDetector</code></p> <p>Rolling window autoencoder for streaming anomaly detection.</p> <p>Maintains a fixed-size deque of the latest <code>window_size</code> observations and feeds them as a sequence tensor to the wrapped autoencoder module. The anomaly score is the reconstruction error for the current (or most recent) window. This design allows sequence context without retaining the full historical stream.</p> <p>Parameters:</p> Name Type Description Default <code>module</code> <code>Module</code> <p>Autoencoder (or encoder-only) style module operating on a rolling tensor.</p> required <code>loss_fn</code> <code>str | Callable</code> <p>Loss for reconstruction error measurement.</p> <code>'mse'</code> <code>optimizer_fn</code> <code>str | Callable</code> <p>Optimizer specification.</p> <code>'sgd'</code> <code>lr</code> <code>float</code> <p>Learning rate.</p> <code>1e-3</code> <code>device</code> <code>str</code> <p>Torch device.</p> <code>'cpu'</code> <code>seed</code> <code>int</code> <p>Random seed.</p> <code>42</code> <code>window_size</code> <code>int</code> <p>Number of past samples retained.</p> <code>10</code> <code>append_predict</code> <code>bool</code> <p>If True, the scored sample (during prediction) is appended to the window.</p> <code>False</code> <code>**kwargs</code> <p>Forwarded to :class:<code>~deep_river.base.RollingDeepEstimator</code>.</p> <code>{}</code> Notes <p>The provided module should expect input shape roughly (seq_len, batch=1, n_features) which is what :func:<code>deque2rolling_tensor</code> produces.</p> <p>Methods:</p> Name Description <code>clone</code> <p>Return a fresh estimator instance with (optionally) copied state.</p> <code>draw</code> <p>Render a (partial) computational graph of the wrapped model.</p> <code>learn_many</code> <p>Batch update; extends window with rows from X and learns if full.</p> <code>learn_one</code> <p>Update model using a single sample appended to the rolling window.</p> <code>load</code> <p>Load a previously saved estimator.</p> <code>save</code> <p>Persist the estimator (architecture, weights, optimiser &amp; runtime state).</p> <code>score_many</code> <p>Return list of reconstruction errors for each row in X.</p> <code>score_one</code> <p>Return reconstruction error for current window + candidate sample.</p> Source code in <code>deep_river/anomaly/rolling_ae.py</code> <pre><code>def __init__(\n    self,\n    module: torch.nn.Module,\n    loss_fn: Union[str, Callable] = \"mse\",\n    optimizer_fn: Union[str, Callable] = \"sgd\",\n    lr: float = 1e-3,\n    device: str = \"cpu\",\n    seed: int = 42,\n    window_size: int = 10,\n    append_predict: bool = False,\n    **kwargs,\n):\n    super().__init__(\n        module=module,\n        loss_fn=loss_fn,\n        optimizer_fn=optimizer_fn,\n        lr=lr,\n        device=device,\n        seed=seed,\n        window_size=window_size,\n        append_predict=append_predict,\n        **kwargs,\n    )\n</code></pre>"},{"location":"reference/anomaly/rolling_ae/#deep_river.anomaly.rolling_ae.RollingAutoencoder.clone","title":"clone","text":"<pre><code>clone(\n    new_params=None,\n    include_attributes: bool = False,\n    copy_weights: bool = False,\n)\n</code></pre> <p>Return a fresh estimator instance with (optionally) copied state.</p> <p>Parameters:</p> Name Type Description Default <code>new_params</code> <code>dict | None</code> <p>Parameter overrides for the cloned instance.</p> <code>None</code> <code>include_attributes</code> <code>bool</code> <p>If True, runtime state (observed features, buffers) is also copied.</p> <code>False</code> <code>copy_weights</code> <code>bool</code> <p>If True, model weights are copied (otherwise the module is re\u2011initialised).</p> <code>False</code> Source code in <code>deep_river/base.py</code> <pre><code>def clone(\n    self,\n    new_params=None,\n    include_attributes: bool = False,\n    copy_weights: bool = False,\n):\n    \"\"\"Return a fresh estimator instance with (optionally) copied state.\n\n    Parameters\n    ----------\n    new_params : dict | None\n        Parameter overrides for the cloned instance.\n    include_attributes : bool, default=False\n        If True, runtime state (observed features, buffers) is also copied.\n    copy_weights : bool, default=False\n        If True, model weights are copied (otherwise the module is re\u2011initialised).\n    \"\"\"\n    new_params = new_params or {}\n    copy_weights = new_params.pop(\"copy_weights\", copy_weights)\n\n    params = {**self._get_all_init_params(), **new_params}\n\n    if \"module\" not in new_params:\n        params[\"module\"] = self._rebuild_module()\n\n    new_est = self.__class__(**self._filter_kwargs(self.__class__.__init__, params))\n\n    if copy_weights and hasattr(self.module, \"state_dict\"):\n        new_est.module.load_state_dict(self.module.state_dict())\n\n    if include_attributes:\n        new_est._restore_runtime_state(self._get_runtime_state())\n\n    return new_est\n</code></pre>"},{"location":"reference/anomaly/rolling_ae/#deep_river.anomaly.rolling_ae.RollingAutoencoder.draw","title":"draw","text":"<pre><code>draw()\n</code></pre> <p>Render a (partial) computational graph of the wrapped model.</p> <p>Imports <code>graphviz</code> and <code>torchviz</code> lazily. Raises an informative ImportError if the optional dependencies are not installed.</p> Source code in <code>deep_river/base.py</code> <pre><code>def draw(self):  # type: ignore[override]\n    \"\"\"Render a (partial) computational graph of the wrapped model.\n\n    Imports ``graphviz`` and ``torchviz`` lazily. Raises an informative\n    ImportError if the optional dependencies are not installed.\n    \"\"\"\n    try:  # pragma: no cover\n        from torchviz import make_dot  # type: ignore\n    except Exception as err:  # noqa: BLE001\n        raise ImportError(\n            \"graphviz and torchviz must be installed to draw the model.\"\n        ) from err\n\n    first_parameter = next(self.module.parameters())\n    input_shape = first_parameter.size()\n    y_pred = self.module(torch.rand(input_shape))\n    return make_dot(y_pred.mean(), params=dict(self.module.named_parameters()))\n</code></pre>"},{"location":"reference/anomaly/rolling_ae/#deep_river.anomaly.rolling_ae.RollingAutoencoder.learn_many","title":"learn_many","text":"<pre><code>learn_many(X: DataFrame, y=None) -&gt; None\n</code></pre> <p>Batch update; extends window with rows from X and learns if full.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>DataFrame</code> <p>DataFrame containing the input features for each sample.</p> required <code>y</code> <code>None</code> <p>Ignored, present for compatibility.</p> <code>None</code> Source code in <code>deep_river/anomaly/rolling_ae.py</code> <pre><code>def learn_many(self, X: pd.DataFrame, y=None) -&gt; None:\n    \"\"\"Batch update; extends window with rows from X and learns if full.\n\n    Parameters\n    ----------\n    X : pd.DataFrame\n        DataFrame containing the input features for each sample.\n    y : None\n        Ignored, present for compatibility.\n    \"\"\"\n    self._update_observed_features(X)\n\n    self._x_window.append(X.values.tolist())\n    if len(self._x_window) == self.window_size:\n        X_t = deque2rolling_tensor(self._x_window, device=self.device)\n        self._learn(x=X_t)\n</code></pre>"},{"location":"reference/anomaly/rolling_ae/#deep_river.anomaly.rolling_ae.RollingAutoencoder.learn_one","title":"learn_one","text":"<pre><code>learn_one(x: dict, y: Any = None, **kwargs) -&gt; None\n</code></pre> <p>Update model using a single sample appended to the rolling window.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>dict</code> <p>Dictionary containing feature name-value pairs for the sample.</p> required <code>y</code> <code>Any</code> <p>Target value (not used in autoencoder training).</p> <code>None</code> <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> Source code in <code>deep_river/anomaly/rolling_ae.py</code> <pre><code>def learn_one(self, x: dict, y: Any = None, **kwargs) -&gt; None:\n    \"\"\"Update model using a single sample appended to the rolling window.\n\n    Parameters\n    ----------\n    x : dict\n        Dictionary containing feature name-value pairs for the sample.\n    y : Any, optional\n        Target value (not used in autoencoder training).\n    **kwargs\n        Additional keyword arguments.\n    \"\"\"\n    self._update_observed_features(x)\n    self._x_window.append(list(x.values()))\n\n    x_t = deque2rolling_tensor(self._x_window, device=self.device)\n    self._learn(x=x_t)\n</code></pre>"},{"location":"reference/anomaly/rolling_ae/#deep_river.anomaly.rolling_ae.RollingAutoencoder.load","title":"load  <code>classmethod</code>","text":"<pre><code>load(filepath: Union[str, Path])\n</code></pre> <p>Load a previously saved estimator.</p> <p>The method reconstructs the estimator class, its wrapped module, optimiser state and runtime information (feature names, buffers, etc.).</p> Source code in <code>deep_river/base.py</code> <pre><code>@classmethod\ndef load(cls, filepath: Union[str, Path]):\n    \"\"\"Load a previously saved estimator.\n\n    The method reconstructs the estimator class, its wrapped module, optimiser\n    state and runtime information (feature names, buffers, etc.).\n    \"\"\"\n    with open(filepath, \"rb\") as f:\n        state = pickle.load(f)\n\n    estimator_cls = cls._import_from_path(state[\"estimator_class\"])\n    init_params = state[\"init_params\"]\n\n    # Rebuild module if needed\n    if \"module\" in init_params and isinstance(init_params[\"module\"], dict):\n        module_info = init_params.pop(\"module\")\n        module_cls = cls._import_from_path(module_info[\"class\"])\n        module = module_cls(\n            **cls._filter_kwargs(module_cls.__init__, module_info[\"kwargs\"])\n        )\n        if state.get(\"model_state_dict\"):\n            module.load_state_dict(state[\"model_state_dict\"])\n        init_params[\"module\"] = module\n\n    estimator = estimator_cls(\n        **cls._filter_kwargs(estimator_cls.__init__, init_params)\n    )\n\n    if state.get(\"optimizer_state_dict\") and hasattr(estimator, \"optimizer\"):\n        try:\n            estimator.optimizer.load_state_dict(\n                state[\"optimizer_state_dict\"]  # type: ignore[arg-type]\n            )\n        except Exception:  # noqa: E722\n            pass\n\n    estimator._restore_runtime_state(state.get(\"runtime_state\", {}))\n    return estimator\n</code></pre>"},{"location":"reference/anomaly/rolling_ae/#deep_river.anomaly.rolling_ae.RollingAutoencoder.save","title":"save","text":"<pre><code>save(filepath: Union[str, Path]) -&gt; None\n</code></pre> <p>Persist the estimator (architecture, weights, optimiser &amp; runtime state).</p> <p>Parameters:</p> Name Type Description Default <code>filepath</code> <code>str | Path</code> <p>Destination file. Parent directories are created automatically.</p> required Source code in <code>deep_river/base.py</code> <pre><code>def save(self, filepath: Union[str, Path]) -&gt; None:\n    \"\"\"Persist the estimator (architecture, weights, optimiser &amp; runtime state).\n\n    Parameters\n    ----------\n    filepath : str | Path\n        Destination file. Parent directories are created automatically.\n    \"\"\"\n    filepath = Path(filepath)\n    filepath.parent.mkdir(parents=True, exist_ok=True)\n\n    state = {\n        \"estimator_class\": f\"{type(self).__module__}.{type(self).__name__}\",\n        \"init_params\": self._get_all_init_params(),\n        \"model_state_dict\": getattr(self.module, \"state_dict\", lambda: {})(),\n        \"optimizer_state_dict\": getattr(self.optimizer, \"state_dict\", lambda: {})(),\n        \"runtime_state\": self._get_runtime_state(),\n    }\n\n    with open(filepath, \"wb\") as f:\n        pickle.dump(state, f)\n</code></pre>"},{"location":"reference/anomaly/rolling_ae/#deep_river.anomaly.rolling_ae.RollingAutoencoder.score_many","title":"score_many","text":"<pre><code>score_many(X: DataFrame) -&gt; List[Any]\n</code></pre> <p>Return list of reconstruction errors for each row in X.</p> <p>If the window is not yet full, zeros are returned for alignment.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>DataFrame</code> <p>DataFrame containing the input features for each sample.</p> required <p>Returns:</p> Type Description <code>List[float]</code> <p>List of computed anomaly scores (reconstruction errors) for each sample in X.</p> Source code in <code>deep_river/anomaly/rolling_ae.py</code> <pre><code>def score_many(self, X: pd.DataFrame) -&gt; List[Any]:\n    \"\"\"Return list of reconstruction errors for each row in X.\n\n    If the window is not yet full, zeros are returned for alignment.\n\n    Parameters\n    ----------\n    X : pd.DataFrame\n        DataFrame containing the input features for each sample.\n\n    Returns\n    -------\n    List[float]\n        List of computed anomaly scores (reconstruction errors) for each sample in X.\n    \"\"\"\n    self._update_observed_features(X)\n    x_win = self._x_window.copy()\n    x_win.append(X.values.tolist())\n    if self.append_predict:\n        self._x_window.append(X.values.tolist())\n\n    if len(self._x_window) == self.window_size:\n        X_t = deque2rolling_tensor(x_win, device=self.device)\n        self.module.eval()\n        with torch.inference_mode():\n            x_pred = self.module(X_t)\n        loss = torch.mean(\n            self.loss_func(x_pred, x_pred, reduction=\"none\"),\n            dim=list(range(1, x_pred.dim())),\n        )\n        losses = loss.detach().numpy()\n        if len(losses) &lt; len(X):\n            losses = np.pad(losses, (len(X) - len(losses), 0))\n        return losses.tolist()\n    else:\n        return np.zeros(len(X)).tolist()\n</code></pre>"},{"location":"reference/anomaly/rolling_ae/#deep_river.anomaly.rolling_ae.RollingAutoencoder.score_one","title":"score_one","text":"<pre><code>score_one(x: dict) -&gt; float\n</code></pre> <p>Return reconstruction error for current window + candidate sample.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>dict</code> <p>Dictionary containing feature name-value pairs for the candidate sample.</p> required <p>Returns:</p> Type Description <code>float</code> <p>Computed anomaly score (reconstruction error).</p> Source code in <code>deep_river/anomaly/rolling_ae.py</code> <pre><code>def score_one(self, x: dict) -&gt; float:\n    \"\"\"Return reconstruction error for current window + candidate sample.\n\n    Parameters\n    ----------\n    x : dict\n        Dictionary containing feature name-value pairs for the candidate sample.\n\n    Returns\n    -------\n    float\n        Computed anomaly score (reconstruction error).\n    \"\"\"\n    res = 0.0\n    self._update_observed_features(x)\n    if len(self._x_window) == self.window_size:\n        x_win = self._x_window.copy()\n        x_win.append(list(x.values()))\n        x_t = deque2rolling_tensor(x_win, device=self.device)\n        self.module.eval()\n        with torch.inference_mode():\n            x_pred = self.module(x_t)\n        loss = self.loss_func(x_pred, x_t)\n        res = loss.item()\n\n    if self.append_predict:\n        self._x_window.append(list(x.values()))\n    return res\n</code></pre>"},{"location":"reference/anomaly/scaler/","title":"scaler","text":""},{"location":"reference/anomaly/scaler/#deep_river.anomaly.scaler","title":"scaler","text":"<p>Classes:</p> Name Description <code>AnomalyMeanScaler</code> <p>Wrapper around an anomaly detector that scales the model's output</p> <code>AnomalyMinMaxScaler</code> <p>Wrapper around an anomaly detector that scales the model's output to</p> <code>AnomalyScaler</code> <p>Wrapper around an anomaly detector that scales the output of the model</p> <code>AnomalyStandardScaler</code> <p>Wrapper around an anomaly detector that standardizes the model's output</p>"},{"location":"reference/anomaly/scaler/#deep_river.anomaly.scaler.AnomalyMeanScaler","title":"AnomalyMeanScaler","text":"<pre><code>AnomalyMeanScaler(\n    anomaly_detector: AnomalyDetector,\n    rolling: bool = True,\n    window_size=250,\n)\n</code></pre> <p>               Bases: <code>AnomalyScaler</code></p> <p>Wrapper around an anomaly detector that scales the model's output by the incremental mean of previous scores.</p> <p>Parameters:</p> Name Type Description Default <code>anomaly_detector</code> <code>AnomalyDetector</code> <p>The anomaly detector to wrap.</p> required <code>metric_type</code> <p>The type of metric to use.</p> required <code>rolling</code> <code>bool</code> <p>Choose whether the metrics are rolling metrics or not.</p> <code>True</code> <code>window_size</code> <p>The window size used for mean computation if rolling==True.</p> <code>250</code> <p>Methods:</p> Name Description <code>learn_one</code> <p>Update the scaler and the underlying anomaly scaler.</p> <code>score_many</code> <p>Return scaled anomaly scores based on raw score provided by</p> <code>score_one</code> <p>Return a scaled anomaly score based on raw score provided by the</p> Source code in <code>deep_river/anomaly/scaler.py</code> <pre><code>def __init__(\n    self,\n    anomaly_detector: AnomalyDetector,\n    rolling: bool = True,\n    window_size=250,\n):\n    super().__init__(anomaly_detector=anomaly_detector)\n    self.rolling = rolling\n    self.window_size = window_size\n    self.mean = utils.Rolling(Mean(), self.window_size) if self.rolling else Mean()\n</code></pre>"},{"location":"reference/anomaly/scaler/#deep_river.anomaly.scaler.AnomalyMeanScaler.learn_one","title":"learn_one","text":"<pre><code>learn_one(*args) -&gt; None\n</code></pre> <p>Update the scaler and the underlying anomaly scaler.</p> <p>Parameters:</p> Name Type Description Default <code>*args</code> <p>Depends on whether the underlying anomaly detector is supervised or not.</p> <code>()</code> <p>Returns:</p> Type Description <code>AnomalyScaler</code> <p>The model itself.</p> Source code in <code>deep_river/anomaly/scaler.py</code> <pre><code>def learn_one(self, *args) -&gt; None:\n    \"\"\"\n    Update the scaler and the underlying anomaly scaler.\n\n    Parameters\n    ----------\n    *args\n        Depends on whether the underlying anomaly detector\n        is supervised or not.\n\n    Returns\n    -------\n    AnomalyScaler\n        The model itself.\n    \"\"\"\n\n    self.anomaly_detector.learn_one(*args)\n</code></pre>"},{"location":"reference/anomaly/scaler/#deep_river.anomaly.scaler.AnomalyMeanScaler.score_many","title":"score_many  <code>abstractmethod</code>","text":"<pre><code>score_many(*args) -&gt; ndarray\n</code></pre> <p>Return scaled anomaly scores based on raw score provided by the wrapped anomaly detector.</p> <p>A high score is indicative of an anomaly. A low score corresponds to a normal observation.</p> <p>Parameters:</p> Name Type Description Default <code>*args</code> <p>Depends on whether the underlying anomaly detector is supervised or not.</p> <code>()</code> <p>Returns:</p> Type Description <code>Scaled anomaly scores. Larger values indicate more anomalous examples.</code> Source code in <code>deep_river/anomaly/scaler.py</code> <pre><code>@abc.abstractmethod\ndef score_many(self, *args) -&gt; np.ndarray:\n    \"\"\"Return scaled anomaly scores based on raw score provided by\n    the wrapped anomaly detector.\n\n    A high score is indicative of an anomaly. A low score corresponds\n    to a normal observation.\n\n    Parameters\n    ----------\n    *args\n        Depends on whether the underlying anomaly detector is\n        supervised or not.\n\n    Returns\n    -------\n    Scaled anomaly scores. Larger values indicate more anomalous examples.\n    \"\"\"\n</code></pre>"},{"location":"reference/anomaly/scaler/#deep_river.anomaly.scaler.AnomalyMeanScaler.score_one","title":"score_one","text":"<pre><code>score_one(*args)\n</code></pre> <p>Return a scaled anomaly score based on raw score provided by the wrapped anomaly detector. Larger values indicate more anomalous examples.</p> <p>Parameters:</p> Name Type Description Default <code>*args</code> <p>Depends on whether the underlying anomaly detector is supervised or not.</p> <code>()</code> <p>Returns:</p> Type Description <code>An scaled anomaly score. Larger values indicate more</code> <code>anomalous examples.</code> Source code in <code>deep_river/anomaly/scaler.py</code> <pre><code>def score_one(self, *args):\n    \"\"\"\n    Return a scaled anomaly score based on raw score provided by the\n    wrapped anomaly detector. Larger values indicate more\n    anomalous examples.\n\n    Parameters\n    ----------\n    *args\n        Depends on whether the underlying anomaly detector is\n        supervised or not.\n\n    Returns\n    -------\n    An scaled anomaly score. Larger values indicate more\n    anomalous examples.\n    \"\"\"\n    raw_score = self.anomaly_detector.score_one(*args)\n    mean = self.mean.update(raw_score).get()\n    score = raw_score / mean\n\n    return score\n</code></pre>"},{"location":"reference/anomaly/scaler/#deep_river.anomaly.scaler.AnomalyMinMaxScaler","title":"AnomalyMinMaxScaler","text":"<pre><code>AnomalyMinMaxScaler(\n    anomaly_detector: AnomalyDetector,\n    rolling: bool = True,\n    window_size: int = 250,\n)\n</code></pre> <p>               Bases: <code>AnomalyScaler</code></p> <p>Wrapper around an anomaly detector that scales the model's output to \\([0, 1]\\) using rolling min and max metrics.</p> <p>Parameters:</p> Name Type Description Default <code>anomaly_detector</code> <code>AnomalyDetector</code> <p>The anomaly detector to wrap.</p> required <code>rolling</code> <code>bool</code> <p>Choose whether the metrics are rolling metrics or not.</p> <code>True</code> <code>window_size</code> <code>int</code> <p>The window size used for the metrics if rolling==True</p> <code>250</code> <p>Methods:</p> Name Description <code>learn_one</code> <p>Update the scaler and the underlying anomaly scaler.</p> <code>score_many</code> <p>Return scaled anomaly scores based on raw score provided by</p> <code>score_one</code> <p>Return a scaled anomaly score based on raw score provided by the</p> Source code in <code>deep_river/anomaly/scaler.py</code> <pre><code>def __init__(\n    self,\n    anomaly_detector: AnomalyDetector,\n    rolling: bool = True,\n    window_size: int = 250,\n):\n    super().__init__(anomaly_detector)\n    self.rolling = rolling\n    self.window_size = window_size\n    self.min = RollingMin(self.window_size) if self.rolling else Min()\n    self.max = RollingMin(self.window_size) if self.rolling else Min()\n</code></pre>"},{"location":"reference/anomaly/scaler/#deep_river.anomaly.scaler.AnomalyMinMaxScaler.learn_one","title":"learn_one","text":"<pre><code>learn_one(*args) -&gt; None\n</code></pre> <p>Update the scaler and the underlying anomaly scaler.</p> <p>Parameters:</p> Name Type Description Default <code>*args</code> <p>Depends on whether the underlying anomaly detector is supervised or not.</p> <code>()</code> <p>Returns:</p> Type Description <code>AnomalyScaler</code> <p>The model itself.</p> Source code in <code>deep_river/anomaly/scaler.py</code> <pre><code>def learn_one(self, *args) -&gt; None:\n    \"\"\"\n    Update the scaler and the underlying anomaly scaler.\n\n    Parameters\n    ----------\n    *args\n        Depends on whether the underlying anomaly detector\n        is supervised or not.\n\n    Returns\n    -------\n    AnomalyScaler\n        The model itself.\n    \"\"\"\n\n    self.anomaly_detector.learn_one(*args)\n</code></pre>"},{"location":"reference/anomaly/scaler/#deep_river.anomaly.scaler.AnomalyMinMaxScaler.score_many","title":"score_many  <code>abstractmethod</code>","text":"<pre><code>score_many(*args) -&gt; ndarray\n</code></pre> <p>Return scaled anomaly scores based on raw score provided by the wrapped anomaly detector.</p> <p>A high score is indicative of an anomaly. A low score corresponds to a normal observation.</p> <p>Parameters:</p> Name Type Description Default <code>*args</code> <p>Depends on whether the underlying anomaly detector is supervised or not.</p> <code>()</code> <p>Returns:</p> Type Description <code>Scaled anomaly scores. Larger values indicate more anomalous examples.</code> Source code in <code>deep_river/anomaly/scaler.py</code> <pre><code>@abc.abstractmethod\ndef score_many(self, *args) -&gt; np.ndarray:\n    \"\"\"Return scaled anomaly scores based on raw score provided by\n    the wrapped anomaly detector.\n\n    A high score is indicative of an anomaly. A low score corresponds\n    to a normal observation.\n\n    Parameters\n    ----------\n    *args\n        Depends on whether the underlying anomaly detector is\n        supervised or not.\n\n    Returns\n    -------\n    Scaled anomaly scores. Larger values indicate more anomalous examples.\n    \"\"\"\n</code></pre>"},{"location":"reference/anomaly/scaler/#deep_river.anomaly.scaler.AnomalyMinMaxScaler.score_one","title":"score_one","text":"<pre><code>score_one(*args)\n</code></pre> <p>Return a scaled anomaly score based on raw score provided by the wrapped anomaly detector. Larger values indicate more anomalous examples.</p> <p>Parameters:</p> Name Type Description Default <code>*args</code> <p>Depends on whether the underlying anomaly detector is supervised or not.</p> <code>()</code> <p>Returns:</p> Type Description <code>An scaled anomaly score. Larger values indicate more</code> <code>anomalous examples.</code> Source code in <code>deep_river/anomaly/scaler.py</code> <pre><code>def score_one(self, *args):\n    \"\"\"\n    Return a scaled anomaly score based on raw score provided by the\n    wrapped anomaly detector. Larger values indicate more\n    anomalous examples.\n\n    Parameters\n    ----------\n    *args\n        Depends on whether the underlying anomaly detector is\n        supervised or not.\n\n    Returns\n    -------\n    An scaled anomaly score. Larger values indicate more\n    anomalous examples.\n    \"\"\"\n    raw_score = self.anomaly_detector.score_one(*args)\n    min = self.min.update(raw_score).get()\n    max = self.max.update(raw_score).get()\n    score = (raw_score - min) / (max - min)\n\n    return score\n</code></pre>"},{"location":"reference/anomaly/scaler/#deep_river.anomaly.scaler.AnomalyScaler","title":"AnomalyScaler","text":"<pre><code>AnomalyScaler(anomaly_detector: AnomalyDetector)\n</code></pre> <p>               Bases: <code>Wrapper</code>, <code>AnomalyDetector</code></p> <p>Wrapper around an anomaly detector that scales the output of the model to account for drift in the wrapped model's anomaly scores.</p> <p>Parameters:</p> Name Type Description Default <code>anomaly_detector</code> <code>AnomalyDetector</code> <p>Anomaly detector to be wrapped.</p> required <p>Methods:</p> Name Description <code>learn_one</code> <p>Update the scaler and the underlying anomaly scaler.</p> <code>score_many</code> <p>Return scaled anomaly scores based on raw score provided by</p> <code>score_one</code> <p>Return a scaled anomaly score based on raw score provided by</p> Source code in <code>deep_river/anomaly/scaler.py</code> <pre><code>def __init__(self, anomaly_detector: AnomalyDetector):\n    self.anomaly_detector = anomaly_detector\n</code></pre>"},{"location":"reference/anomaly/scaler/#deep_river.anomaly.scaler.AnomalyScaler.learn_one","title":"learn_one","text":"<pre><code>learn_one(*args) -&gt; None\n</code></pre> <p>Update the scaler and the underlying anomaly scaler.</p> <p>Parameters:</p> Name Type Description Default <code>*args</code> <p>Depends on whether the underlying anomaly detector is supervised or not.</p> <code>()</code> <p>Returns:</p> Type Description <code>AnomalyScaler</code> <p>The model itself.</p> Source code in <code>deep_river/anomaly/scaler.py</code> <pre><code>def learn_one(self, *args) -&gt; None:\n    \"\"\"\n    Update the scaler and the underlying anomaly scaler.\n\n    Parameters\n    ----------\n    *args\n        Depends on whether the underlying anomaly detector\n        is supervised or not.\n\n    Returns\n    -------\n    AnomalyScaler\n        The model itself.\n    \"\"\"\n\n    self.anomaly_detector.learn_one(*args)\n</code></pre>"},{"location":"reference/anomaly/scaler/#deep_river.anomaly.scaler.AnomalyScaler.score_many","title":"score_many  <code>abstractmethod</code>","text":"<pre><code>score_many(*args) -&gt; ndarray\n</code></pre> <p>Return scaled anomaly scores based on raw score provided by the wrapped anomaly detector.</p> <p>A high score is indicative of an anomaly. A low score corresponds to a normal observation.</p> <p>Parameters:</p> Name Type Description Default <code>*args</code> <p>Depends on whether the underlying anomaly detector is supervised or not.</p> <code>()</code> <p>Returns:</p> Type Description <code>Scaled anomaly scores. Larger values indicate more anomalous examples.</code> Source code in <code>deep_river/anomaly/scaler.py</code> <pre><code>@abc.abstractmethod\ndef score_many(self, *args) -&gt; np.ndarray:\n    \"\"\"Return scaled anomaly scores based on raw score provided by\n    the wrapped anomaly detector.\n\n    A high score is indicative of an anomaly. A low score corresponds\n    to a normal observation.\n\n    Parameters\n    ----------\n    *args\n        Depends on whether the underlying anomaly detector is\n        supervised or not.\n\n    Returns\n    -------\n    Scaled anomaly scores. Larger values indicate more anomalous examples.\n    \"\"\"\n</code></pre>"},{"location":"reference/anomaly/scaler/#deep_river.anomaly.scaler.AnomalyScaler.score_one","title":"score_one  <code>abstractmethod</code>","text":"<pre><code>score_one(*args) -&gt; float\n</code></pre> <p>Return a scaled anomaly score based on raw score provided by the wrapped anomaly detector.</p> <p>A high score is indicative of an anomaly. A low score corresponds to a normal observation.</p> <p>Parameters:</p> Name Type Description Default <code>*args</code> <p>Depends on whether the underlying anomaly detector is supervised or not.</p> <code>()</code> <p>Returns:</p> Type Description <code>An scaled anomaly score. Larger values indicate</code> <code>more anomalous examples.</code> Source code in <code>deep_river/anomaly/scaler.py</code> <pre><code>@abc.abstractmethod\ndef score_one(self, *args) -&gt; float:\n    \"\"\"Return a scaled anomaly score based on raw score provided by\n    the wrapped anomaly detector.\n\n    A high score is indicative of an anomaly. A low score corresponds\n    to a normal observation.\n\n    Parameters\n    ----------\n    *args\n        Depends on whether the underlying anomaly detector\n        is supervised or not.\n\n    Returns\n    -------\n    An scaled anomaly score. Larger values indicate\n    more anomalous examples.\n    \"\"\"\n</code></pre>"},{"location":"reference/anomaly/scaler/#deep_river.anomaly.scaler.AnomalyStandardScaler","title":"AnomalyStandardScaler","text":"<pre><code>AnomalyStandardScaler(\n    anomaly_detector: AnomalyDetector,\n    with_std: bool = True,\n    rolling: bool = True,\n    window_size: int = 250,\n)\n</code></pre> <p>               Bases: <code>AnomalyScaler</code></p> <p>Wrapper around an anomaly detector that standardizes the model's output using incremental mean and variance metrics.</p> <p>Parameters:</p> Name Type Description Default <code>anomaly_detector</code> <code>AnomalyDetector</code> <p>The anomaly detector to wrap.</p> required <code>with_std</code> <code>bool</code> <p>Whether to use standard deviation for scaling.</p> <code>True</code> <code>rolling</code> <code>bool</code> <p>Choose whether the metrics are rolling metrics or not.</p> <code>True</code> <code>window_size</code> <code>int</code> <p>The window size used for the metrics if rolling==True.</p> <code>250</code> <p>Methods:</p> Name Description <code>learn_one</code> <p>Update the scaler and the underlying anomaly scaler.</p> <code>score_many</code> <p>Return scaled anomaly scores based on raw score provided by</p> <code>score_one</code> <p>Return a scaled anomaly score based on raw score provided by the</p> Source code in <code>deep_river/anomaly/scaler.py</code> <pre><code>def __init__(\n    self,\n    anomaly_detector: AnomalyDetector,\n    with_std: bool = True,\n    rolling: bool = True,\n    window_size: int = 250,\n):\n    super().__init__(anomaly_detector)\n    self.rolling = rolling\n    self.window_size = window_size\n    self.mean = utils.Rolling(Mean(), self.window_size) if self.rolling else Mean()\n    self.sq_mean = (\n        utils.Rolling(Mean(), self.window_size) if self.rolling else Mean()\n    )\n    self.with_std = with_std\n</code></pre>"},{"location":"reference/anomaly/scaler/#deep_river.anomaly.scaler.AnomalyStandardScaler.learn_one","title":"learn_one","text":"<pre><code>learn_one(*args) -&gt; None\n</code></pre> <p>Update the scaler and the underlying anomaly scaler.</p> <p>Parameters:</p> Name Type Description Default <code>*args</code> <p>Depends on whether the underlying anomaly detector is supervised or not.</p> <code>()</code> <p>Returns:</p> Type Description <code>AnomalyScaler</code> <p>The model itself.</p> Source code in <code>deep_river/anomaly/scaler.py</code> <pre><code>def learn_one(self, *args) -&gt; None:\n    \"\"\"\n    Update the scaler and the underlying anomaly scaler.\n\n    Parameters\n    ----------\n    *args\n        Depends on whether the underlying anomaly detector\n        is supervised or not.\n\n    Returns\n    -------\n    AnomalyScaler\n        The model itself.\n    \"\"\"\n\n    self.anomaly_detector.learn_one(*args)\n</code></pre>"},{"location":"reference/anomaly/scaler/#deep_river.anomaly.scaler.AnomalyStandardScaler.score_many","title":"score_many  <code>abstractmethod</code>","text":"<pre><code>score_many(*args) -&gt; ndarray\n</code></pre> <p>Return scaled anomaly scores based on raw score provided by the wrapped anomaly detector.</p> <p>A high score is indicative of an anomaly. A low score corresponds to a normal observation.</p> <p>Parameters:</p> Name Type Description Default <code>*args</code> <p>Depends on whether the underlying anomaly detector is supervised or not.</p> <code>()</code> <p>Returns:</p> Type Description <code>Scaled anomaly scores. Larger values indicate more anomalous examples.</code> Source code in <code>deep_river/anomaly/scaler.py</code> <pre><code>@abc.abstractmethod\ndef score_many(self, *args) -&gt; np.ndarray:\n    \"\"\"Return scaled anomaly scores based on raw score provided by\n    the wrapped anomaly detector.\n\n    A high score is indicative of an anomaly. A low score corresponds\n    to a normal observation.\n\n    Parameters\n    ----------\n    *args\n        Depends on whether the underlying anomaly detector is\n        supervised or not.\n\n    Returns\n    -------\n    Scaled anomaly scores. Larger values indicate more anomalous examples.\n    \"\"\"\n</code></pre>"},{"location":"reference/anomaly/scaler/#deep_river.anomaly.scaler.AnomalyStandardScaler.score_one","title":"score_one","text":"<pre><code>score_one(*args)\n</code></pre> <p>Return a scaled anomaly score based on raw score provided by the wrapped anomaly detector. Larger values indicate more anomalous examples.</p> <p>Parameters:</p> Name Type Description Default <code>*args</code> <p>Depends on whether the underlying anomaly detector is supervised or not.</p> <code>()</code> <p>Returns:</p> Type Description <code>An scaled anomaly score. Larger values indicate more</code> <code>anomalous examples.</code> Source code in <code>deep_river/anomaly/scaler.py</code> <pre><code>def score_one(self, *args):\n    \"\"\"\n    Return a scaled anomaly score based on raw score provided by the\n    wrapped anomaly detector. Larger values indicate more\n    anomalous examples.\n\n    Parameters\n    ----------\n    *args\n        Depends on whether the underlying anomaly detector\n        is supervised or not.\n\n    Returns\n    -------\n    An scaled anomaly score. Larger values indicate more\n    anomalous examples.\n    \"\"\"\n    raw_score = self.anomaly_detector.score_one(*args)\n    mean = self.mean.update(raw_score).get()\n    if self.with_std:\n        var = (\n            self.sq_mean.update(raw_score**2).get() - mean**2\n        )  # todo is this correct?\n        score = (raw_score - mean) / var**0.5\n    else:\n        score = raw_score - mean\n\n    return score\n</code></pre>"},{"location":"reference/classification/classifier/","title":"classifier","text":""},{"location":"reference/classification/classifier/#deep_river.classification.classifier","title":"classifier","text":"<p>Classes:</p> Name Description <code>Classifier</code> <p>Incremental PyTorch classifier with optional dynamic feature &amp; class growth.</p>"},{"location":"reference/classification/classifier/#deep_river.classification.classifier.Classifier","title":"Classifier","text":"<pre><code>Classifier(\n    module: Module,\n    loss_fn: Union[str, Callable],\n    optimizer_fn: Union[str, type],\n    lr: float = 0.001,\n    output_is_logit: bool = True,\n    is_class_incremental: bool = False,\n    is_feature_incremental: bool = False,\n    device: str = \"cpu\",\n    seed: int = 42,\n    gradient_clip_value: float | None = None,\n    **kwargs\n)\n</code></pre> <p>               Bases: <code>DeepEstimator</code>, <code>MiniBatchClassifier</code></p> <p>Incremental PyTorch classifier with optional dynamic feature &amp; class growth.</p> <p>This wrapper turns an arbitrary <code>torch.nn.Module</code> into an incremental classifier that follows the :mod:<code>river</code> API. It can optionally expand its input dimensionality when previously unseen feature names occur (<code>is_feature_incremental=True</code>) and expand the output layer when new class labels appear (<code>is_class_incremental=True</code>).</p> <p>When <code>loss_fn='cross_entropy'</code> targets are handled as integer class indices; otherwise they are converted to one-hot vectors to match the output dimension.</p> <p>Parameters:</p> Name Type Description Default <code>module</code> <code>Module</code> <p>The underlying PyTorch model producing (logit) outputs.</p> required <code>loss_fn</code> <code>str | Callable</code> <p>Loss identifier (e.g. <code>'cross_entropy'</code>, <code>'mse'</code>) or a callable.</p> required <code>optimizer_fn</code> <code>str | type</code> <p>Optimizer identifier (<code>'adam'</code>, <code>'sgd'</code>, etc.) or an optimizer class.</p> required <code>lr</code> <code>float</code> <p>Learning rate passed to the optimizer.</p> <code>1e-3</code> <code>output_is_logit</code> <code>bool</code> <p>If True, <code>predict_proba_*</code> will apply a softmax (multi-class) or sigmoid (binary) as needed using :func:<code>output2proba</code>.</p> <code>True</code> <code>is_class_incremental</code> <code>bool</code> <p>Whether to expand the output layer when new class labels appear.</p> <code>False</code> <code>is_feature_incremental</code> <code>bool</code> <p>Whether to expand the input layer when new feature names are observed.</p> <code>False</code> <code>device</code> <code>str</code> <p>Runtime device.</p> <code>'cpu'</code> <code>seed</code> <code>int</code> <p>Random seed.</p> <code>42</code> <code>gradient_clip_value</code> <code>float | None</code> <p>Norm to clip gradients to (disabled if <code>None</code>).</p> <code>None</code> <code>**kwargs</code> <p>Extra parameters retained for reconstruction.</p> <code>{}</code> <p>Examples:</p> <pre><code>Online binary classification on the Phishing dataset from :mod:`river`.\nWe build a tiny MLP and maintain an online Accuracy metric. The exact value\nmay vary depending on library version and hardware::\n\n&gt;&gt;&gt; import random, numpy as np\n&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from torch import nn, manual_seed\n&gt;&gt;&gt; from river import datasets, metrics\n&gt;&gt;&gt; from deep_river.classification import Classifier\n&gt;&gt;&gt; _ = manual_seed(42); random.seed(42); np.random.seed(42)\n&gt;&gt;&gt; first_x, _ = next(iter(datasets.Phishing()))\n&gt;&gt;&gt; n_features = len(first_x)\n&gt;&gt;&gt; class SmallMLP(nn.Module):\n...     def __init__(self, n_features):\n...         super().__init__()\n...         self.net = nn.Sequential(\n...             nn.Linear(n_features, 16),\n...             nn.ReLU(),\n...             nn.Linear(16, 2)\n...         )\n...     def forward(self, x):\n...         return self.net(x)  # raw logits\n&gt;&gt;&gt; clf = Classifier(\n...     module=SmallMLP(n_features),\n...     loss_fn='cross_entropy',\n...     optimizer_fn='sgd',\n...     lr=1e-2,\n...     is_class_incremental=True\n... )\n&gt;&gt;&gt; acc = metrics.Accuracy()\n&gt;&gt;&gt; for i, (x, y) in enumerate(datasets.Phishing().take(200)):\n...     if i &gt; 0:  # only predict after first sample is seen\n...         y_pred = clf.predict_one(x)\n...         acc.update(y, y_pred)\n...     clf.learn_one(x, y)\n&gt;&gt;&gt; print(f\"Accuracy: {acc.get():.4f}\")  # doctest: +ELLIPSIS\nAccuracy: ...\n</code></pre> <p>Methods:</p> Name Description <code>clone</code> <p>Return a fresh estimator instance with (optionally) copied state.</p> <code>draw</code> <p>Render a (partial) computational graph of the wrapped model.</p> <code>learn_many</code> <p>Learn from a batch of instances.</p> <code>learn_one</code> <p>Learn from a single instance.</p> <code>load</code> <p>Load a previously saved estimator.</p> <code>predict_proba_many</code> <p>Predict probabilities for a batch of instances.</p> <code>predict_proba_one</code> <p>Predict class membership probabilities for one instance.</p> <code>save</code> <p>Persist the estimator (architecture, weights, optimiser &amp; runtime state).</p> Source code in <code>deep_river/classification/classifier.py</code> <pre><code>def __init__(\n    self,\n    module: torch.nn.Module,\n    loss_fn: Union[str, Callable],\n    optimizer_fn: Union[str, type],\n    lr: float = 0.001,\n    output_is_logit: bool = True,\n    is_class_incremental: bool = False,\n    is_feature_incremental: bool = False,\n    device: str = \"cpu\",\n    seed: int = 42,\n    gradient_clip_value: float | None = None,\n    **kwargs,\n):\n    super().__init__(\n        module=module,\n        loss_fn=loss_fn,\n        optimizer_fn=optimizer_fn,\n        lr=lr,\n        device=device,\n        seed=seed,\n        is_feature_incremental=is_feature_incremental,\n        gradient_clip_value=gradient_clip_value,\n        **kwargs,\n    )\n    self.output_is_logit = output_is_logit\n    self.is_class_incremental = is_class_incremental\n    self.observed_classes: SortedSet = SortedSet()\n</code></pre>"},{"location":"reference/classification/classifier/#deep_river.classification.classifier.Classifier.clone","title":"clone","text":"<pre><code>clone(\n    new_params=None,\n    include_attributes: bool = False,\n    copy_weights: bool = False,\n)\n</code></pre> <p>Return a fresh estimator instance with (optionally) copied state.</p> <p>Parameters:</p> Name Type Description Default <code>new_params</code> <code>dict | None</code> <p>Parameter overrides for the cloned instance.</p> <code>None</code> <code>include_attributes</code> <code>bool</code> <p>If True, runtime state (observed features, buffers) is also copied.</p> <code>False</code> <code>copy_weights</code> <code>bool</code> <p>If True, model weights are copied (otherwise the module is re\u2011initialised).</p> <code>False</code> Source code in <code>deep_river/base.py</code> <pre><code>def clone(\n    self,\n    new_params=None,\n    include_attributes: bool = False,\n    copy_weights: bool = False,\n):\n    \"\"\"Return a fresh estimator instance with (optionally) copied state.\n\n    Parameters\n    ----------\n    new_params : dict | None\n        Parameter overrides for the cloned instance.\n    include_attributes : bool, default=False\n        If True, runtime state (observed features, buffers) is also copied.\n    copy_weights : bool, default=False\n        If True, model weights are copied (otherwise the module is re\u2011initialised).\n    \"\"\"\n    new_params = new_params or {}\n    copy_weights = new_params.pop(\"copy_weights\", copy_weights)\n\n    params = {**self._get_all_init_params(), **new_params}\n\n    if \"module\" not in new_params:\n        params[\"module\"] = self._rebuild_module()\n\n    new_est = self.__class__(**self._filter_kwargs(self.__class__.__init__, params))\n\n    if copy_weights and hasattr(self.module, \"state_dict\"):\n        new_est.module.load_state_dict(self.module.state_dict())\n\n    if include_attributes:\n        new_est._restore_runtime_state(self._get_runtime_state())\n\n    return new_est\n</code></pre>"},{"location":"reference/classification/classifier/#deep_river.classification.classifier.Classifier.draw","title":"draw","text":"<pre><code>draw()\n</code></pre> <p>Render a (partial) computational graph of the wrapped model.</p> <p>Imports <code>graphviz</code> and <code>torchviz</code> lazily. Raises an informative ImportError if the optional dependencies are not installed.</p> Source code in <code>deep_river/base.py</code> <pre><code>def draw(self):  # type: ignore[override]\n    \"\"\"Render a (partial) computational graph of the wrapped model.\n\n    Imports ``graphviz`` and ``torchviz`` lazily. Raises an informative\n    ImportError if the optional dependencies are not installed.\n    \"\"\"\n    try:  # pragma: no cover\n        from torchviz import make_dot  # type: ignore\n    except Exception as err:  # noqa: BLE001\n        raise ImportError(\n            \"graphviz and torchviz must be installed to draw the model.\"\n        ) from err\n\n    first_parameter = next(self.module.parameters())\n    input_shape = first_parameter.size()\n    y_pred = self.module(torch.rand(input_shape))\n    return make_dot(y_pred.mean(), params=dict(self.module.named_parameters()))\n</code></pre>"},{"location":"reference/classification/classifier/#deep_river.classification.classifier.Classifier.learn_many","title":"learn_many","text":"<pre><code>learn_many(X: DataFrame, y: Series) -&gt; None\n</code></pre> <p>Learn from a batch of instances.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>DataFrame</code> <p>Batch of feature rows.</p> required <code>y</code> <code>Series</code> <p>Corresponding labels.</p> required Source code in <code>deep_river/classification/classifier.py</code> <pre><code>def learn_many(self, X: pd.DataFrame, y: pd.Series) -&gt; None:\n    \"\"\"Learn from a batch of instances.\n\n    Parameters\n    ----------\n    X : pandas.DataFrame\n        Batch of feature rows.\n    y : pandas.Series\n        Corresponding labels.\n    \"\"\"\n    self._update_observed_features(X)\n    self._update_observed_targets(y)\n    x_t = self._df2tensor(X)\n    if self.loss_fn == \"cross_entropy\":\n        self._classification_step_cross_entropy(x_t, y)\n    else:\n        self._learn(x_t, y)\n</code></pre>"},{"location":"reference/classification/classifier/#deep_river.classification.classifier.Classifier.learn_one","title":"learn_one","text":"<pre><code>learn_one(x: dict, y: ClfTarget) -&gt; None\n</code></pre> <p>Learn from a single instance.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>dict</code> <p>Feature dictionary.</p> required <code>y</code> <code>hashable</code> <p>Class label.</p> required Source code in <code>deep_river/classification/classifier.py</code> <pre><code>def learn_one(self, x: dict, y: base.typing.ClfTarget) -&gt; None:\n    \"\"\"Learn from a single instance.\n\n    Parameters\n    ----------\n    x : dict\n        Feature dictionary.\n    y : hashable\n        Class label.\n    \"\"\"\n    self._update_observed_features(x)\n    self._update_observed_targets(y)\n    x_t = self._dict2tensor(x)\n    if self.loss_fn == \"cross_entropy\":\n        self._classification_step_cross_entropy(x_t, y)\n    else:\n        # One-hot pathway / other losses\n        self._learn(x_t, y)\n</code></pre>"},{"location":"reference/classification/classifier/#deep_river.classification.classifier.Classifier.load","title":"load  <code>classmethod</code>","text":"<pre><code>load(filepath: Union[str, Path])\n</code></pre> <p>Load a previously saved estimator.</p> <p>The method reconstructs the estimator class, its wrapped module, optimiser state and runtime information (feature names, buffers, etc.).</p> Source code in <code>deep_river/base.py</code> <pre><code>@classmethod\ndef load(cls, filepath: Union[str, Path]):\n    \"\"\"Load a previously saved estimator.\n\n    The method reconstructs the estimator class, its wrapped module, optimiser\n    state and runtime information (feature names, buffers, etc.).\n    \"\"\"\n    with open(filepath, \"rb\") as f:\n        state = pickle.load(f)\n\n    estimator_cls = cls._import_from_path(state[\"estimator_class\"])\n    init_params = state[\"init_params\"]\n\n    # Rebuild module if needed\n    if \"module\" in init_params and isinstance(init_params[\"module\"], dict):\n        module_info = init_params.pop(\"module\")\n        module_cls = cls._import_from_path(module_info[\"class\"])\n        module = module_cls(\n            **cls._filter_kwargs(module_cls.__init__, module_info[\"kwargs\"])\n        )\n        if state.get(\"model_state_dict\"):\n            module.load_state_dict(state[\"model_state_dict\"])\n        init_params[\"module\"] = module\n\n    estimator = estimator_cls(\n        **cls._filter_kwargs(estimator_cls.__init__, init_params)\n    )\n\n    if state.get(\"optimizer_state_dict\") and hasattr(estimator, \"optimizer\"):\n        try:\n            estimator.optimizer.load_state_dict(\n                state[\"optimizer_state_dict\"]  # type: ignore[arg-type]\n            )\n        except Exception:  # noqa: E722\n            pass\n\n    estimator._restore_runtime_state(state.get(\"runtime_state\", {}))\n    return estimator\n</code></pre>"},{"location":"reference/classification/classifier/#deep_river.classification.classifier.Classifier.predict_proba_many","title":"predict_proba_many","text":"<pre><code>predict_proba_many(X: DataFrame) -&gt; DataFrame\n</code></pre> <p>Predict probabilities for a batch of instances.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>DataFrame</code> <p>Feature matrix.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>Each row sums to 1 (multi-class) or has two columns for binary.</p> Source code in <code>deep_river/classification/classifier.py</code> <pre><code>def predict_proba_many(self, X: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"Predict probabilities for a batch of instances.\n\n    Parameters\n    ----------\n    X : pandas.DataFrame\n        Feature matrix.\n\n    Returns\n    -------\n    pandas.DataFrame\n        Each row sums to 1 (multi-class) or has two columns for binary.\n    \"\"\"\n    self._update_observed_features(X)\n    x_t = self._df2tensor(X)\n    self.module.eval()\n    with torch.inference_mode():\n        y_preds = self.module(x_t)\n    return pd.DataFrame(\n        output2proba(y_preds, self.observed_classes, self.output_is_logit)\n    )\n</code></pre>"},{"location":"reference/classification/classifier/#deep_river.classification.classifier.Classifier.predict_proba_one","title":"predict_proba_one","text":"<pre><code>predict_proba_one(x: dict) -&gt; dict[ClfTarget, float]\n</code></pre> <p>Predict class membership probabilities for one instance.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>dict</code> <p>Feature dictionary.</p> required <p>Returns:</p> Type Description <code>dict</code> <p>Mapping from label -&gt; probability.</p> Source code in <code>deep_river/classification/classifier.py</code> <pre><code>def predict_proba_one(self, x: dict) -&gt; dict[base.typing.ClfTarget, float]:\n    \"\"\"Predict class membership probabilities for one instance.\n\n    Parameters\n    ----------\n    x : dict\n        Feature dictionary.\n\n    Returns\n    -------\n    dict\n        Mapping from label -&gt; probability.\n    \"\"\"\n    self._update_observed_features(x)\n    x_t = self._dict2tensor(x)\n    self.module.eval()\n    with torch.inference_mode():\n        y_pred = self.module(x_t)\n    raw = output2proba(y_pred, self.observed_classes, self.output_is_logit)[0]\n    return cast(dict[base.typing.ClfTarget, float], raw)\n</code></pre>"},{"location":"reference/classification/classifier/#deep_river.classification.classifier.Classifier.save","title":"save","text":"<pre><code>save(filepath: Union[str, Path]) -&gt; None\n</code></pre> <p>Persist the estimator (architecture, weights, optimiser &amp; runtime state).</p> <p>Parameters:</p> Name Type Description Default <code>filepath</code> <code>str | Path</code> <p>Destination file. Parent directories are created automatically.</p> required Source code in <code>deep_river/base.py</code> <pre><code>def save(self, filepath: Union[str, Path]) -&gt; None:\n    \"\"\"Persist the estimator (architecture, weights, optimiser &amp; runtime state).\n\n    Parameters\n    ----------\n    filepath : str | Path\n        Destination file. Parent directories are created automatically.\n    \"\"\"\n    filepath = Path(filepath)\n    filepath.parent.mkdir(parents=True, exist_ok=True)\n\n    state = {\n        \"estimator_class\": f\"{type(self).__module__}.{type(self).__name__}\",\n        \"init_params\": self._get_all_init_params(),\n        \"model_state_dict\": getattr(self.module, \"state_dict\", lambda: {})(),\n        \"optimizer_state_dict\": getattr(self.optimizer, \"state_dict\", lambda: {})(),\n        \"runtime_state\": self._get_runtime_state(),\n    }\n\n    with open(filepath, \"wb\") as f:\n        pickle.dump(state, f)\n</code></pre>"},{"location":"reference/classification/rolling_classifier/","title":"rolling_classifier","text":""},{"location":"reference/classification/rolling_classifier/#deep_river.classification.rolling_classifier","title":"rolling_classifier","text":"<p>Classes:</p> Name Description <code>RollingClassifier</code> <p>Rolling window variant of :class:<code>Classifier</code>.</p>"},{"location":"reference/classification/rolling_classifier/#deep_river.classification.rolling_classifier.RollingClassifier","title":"RollingClassifier","text":"<pre><code>RollingClassifier(\n    module: Module,\n    loss_fn: Union[\n        str, Callable\n    ] = \"binary_cross_entropy_with_logits\",\n    optimizer_fn: Union[str, Type[Optimizer]] = \"sgd\",\n    lr: float = 0.001,\n    output_is_logit: bool = True,\n    is_class_incremental: bool = False,\n    is_feature_incremental: bool = False,\n    device: str = \"cpu\",\n    seed: int = 42,\n    window_size: int = 10,\n    append_predict: bool = False,\n    gradient_clip_value: float | None = None,\n    **kwargs\n)\n</code></pre> <p>               Bases: <code>Classifier</code>, <code>RollingDeepEstimator</code></p> <p>Rolling window variant of :class:<code>Classifier</code>.</p> <p>Maintains a fixed-size deque of the most recent observations (<code>window_size</code>) and feeds them as a temporal slice to the underlying module. This enables simple short-term sequence conditioning without explicit recurrent state handling on the user side.</p> <p>Parameters:</p> Name Type Description Default <code>module</code> <code>Module</code> <p>Classification module consuming a rolling tensor shaped roughly as <code>(seq_len, batch=1, n_features)</code> depending on internal conversion.</p> required <code>loss_fn</code> <code>str | Callable</code> <p>Loss identifier or callable.</p> <code>'binary_cross_entropy_with_logits'</code> <code>optimizer_fn</code> <code>str | type</code> <p>Optimizer specification.</p> <code>'sgd'</code> <code>lr</code> <code>float</code> <p>Learning rate.</p> <code>1e-3</code> <code>output_is_logit</code> <code>bool</code> <p>Whether raw logits are produced (enables post-softmax via <code>output2proba</code>).</p> <code>True</code> <code>is_class_incremental</code> <code>bool</code> <p>Expand output layer when new class labels appear.</p> <code>False</code> <code>is_feature_incremental</code> <code>bool</code> <p>Expand input layer when new feature names appear.</p> <code>False</code> <code>device</code> <code>str</code> <p>Torch device.</p> <code>'cpu'</code> <code>seed</code> <code>int</code> <p>Random seed.</p> <code>42</code> <code>window_size</code> <code>int</code> <p>Number of past samples kept.</p> <code>10</code> <code>append_predict</code> <code>bool</code> <p>If True, predictions are appended to internal window during inference (useful for autoregressive generation).</p> <code>False</code> <code>gradient_clip_value</code> <code>float | None</code> <p>Optional gradient clipping threshold.</p> <code>None</code> <code>**kwargs</code> <p>Forwarded to parent constructors.</p> <code>{}</code> <p>Examples:</p> <pre><code>Streaming binary classification on the Phishing dataset with a tiny RNN.\nWe only assert the final Accuracy lies in ``[0, 1]`` for doctest stability.\n\n&gt;&gt;&gt; import random, numpy as np, torch\n&gt;&gt;&gt; from torch import nn, manual_seed\n&gt;&gt;&gt; from river import datasets, metrics\n&gt;&gt;&gt; from deep_river.classification import RollingClassifier\n&gt;&gt;&gt; _ = manual_seed(42); random.seed(42); np.random.seed(42)\n&gt;&gt;&gt; first_x, _ = next(iter(datasets.Phishing()))\n&gt;&gt;&gt; n_features = len(first_x)\n&gt;&gt;&gt; class TinyRNN(nn.Module):\n...     def __init__(self, n_features):\n...         super().__init__()\n...         self.rnn = nn.RNN(n_features, 8)\n...         self.head = nn.Linear(8, 2)\n...     def forward(self, x):\n...         out, _ = self.rnn(x)\n...         return self.head(out[-1])  # logits\n&gt;&gt;&gt; rclf = RollingClassifier(\n...     module=TinyRNN(n_features),\n...     loss_fn='cross_entropy',\n...     optimizer_fn='sgd',\n...     lr=5e-3,\n...     window_size=8,\n...     is_class_incremental=True\n... )\n&gt;&gt;&gt; acc = metrics.Accuracy()\n&gt;&gt;&gt; for i, (x, y) in enumerate(datasets.Phishing().take(200)):\n...     if i &gt; 0:\n...         y_pred = rclf.predict_one(x)\n...         acc.update(y, y_pred)\n...     rclf.learn_one(x, y)\n&gt;&gt;&gt; print(f\"Accuracy: {acc.get():.4f}\")\nAccuracy: ...\n</code></pre> <p>Methods:</p> Name Description <code>clone</code> <p>Return a fresh estimator instance with (optionally) copied state.</p> <code>draw</code> <p>Render a (partial) computational graph of the wrapped model.</p> <code>learn_many</code> <p>Batch update: extend window with rows of X and perform a step.</p> <code>learn_one</code> <p>Learn from a single (x, y) updating the rolling window.</p> <code>load</code> <p>Load a previously saved estimator.</p> <code>predict_proba_many</code> <p>Return probability DataFrame for multiple samples with rolling context.</p> <code>predict_proba_one</code> <p>Return class probability mapping for one sample using rolling context.</p> <code>save</code> <p>Persist the estimator (architecture, weights, optimiser &amp; runtime state).</p> Source code in <code>deep_river/classification/rolling_classifier.py</code> <pre><code>def __init__(\n    self,\n    module: torch.nn.Module,\n    loss_fn: Union[str, Callable] = \"binary_cross_entropy_with_logits\",\n    optimizer_fn: Union[str, Type[optim.Optimizer]] = \"sgd\",\n    lr: float = 1e-3,\n    output_is_logit: bool = True,\n    is_class_incremental: bool = False,\n    is_feature_incremental: bool = False,\n    device: str = \"cpu\",\n    seed: int = 42,\n    window_size: int = 10,\n    append_predict: bool = False,\n    gradient_clip_value: float | None = None,\n    **kwargs,\n):\n    # Use RollingDeepEstimator init to build window + base functionality\n    RollingDeepEstimator.__init__(\n        self,\n        module=module,\n        loss_fn=loss_fn,\n        optimizer_fn=optimizer_fn,\n        lr=lr,\n        device=device,\n        seed=seed,\n        window_size=window_size,\n        append_predict=append_predict,\n        is_feature_incremental=is_feature_incremental,\n        gradient_clip_value=gradient_clip_value,\n        **kwargs,\n    )\n    # Classification specific attributes (mirror Classifier.__init__)\n    self.output_is_logit = output_is_logit\n    self.is_class_incremental = is_class_incremental\n    self.observed_classes: SortedSet = SortedSet()\n</code></pre>"},{"location":"reference/classification/rolling_classifier/#deep_river.classification.rolling_classifier.RollingClassifier.clone","title":"clone","text":"<pre><code>clone(\n    new_params=None,\n    include_attributes: bool = False,\n    copy_weights: bool = False,\n)\n</code></pre> <p>Return a fresh estimator instance with (optionally) copied state.</p> <p>Parameters:</p> Name Type Description Default <code>new_params</code> <code>dict | None</code> <p>Parameter overrides for the cloned instance.</p> <code>None</code> <code>include_attributes</code> <code>bool</code> <p>If True, runtime state (observed features, buffers) is also copied.</p> <code>False</code> <code>copy_weights</code> <code>bool</code> <p>If True, model weights are copied (otherwise the module is re\u2011initialised).</p> <code>False</code> Source code in <code>deep_river/base.py</code> <pre><code>def clone(\n    self,\n    new_params=None,\n    include_attributes: bool = False,\n    copy_weights: bool = False,\n):\n    \"\"\"Return a fresh estimator instance with (optionally) copied state.\n\n    Parameters\n    ----------\n    new_params : dict | None\n        Parameter overrides for the cloned instance.\n    include_attributes : bool, default=False\n        If True, runtime state (observed features, buffers) is also copied.\n    copy_weights : bool, default=False\n        If True, model weights are copied (otherwise the module is re\u2011initialised).\n    \"\"\"\n    new_params = new_params or {}\n    copy_weights = new_params.pop(\"copy_weights\", copy_weights)\n\n    params = {**self._get_all_init_params(), **new_params}\n\n    if \"module\" not in new_params:\n        params[\"module\"] = self._rebuild_module()\n\n    new_est = self.__class__(**self._filter_kwargs(self.__class__.__init__, params))\n\n    if copy_weights and hasattr(self.module, \"state_dict\"):\n        new_est.module.load_state_dict(self.module.state_dict())\n\n    if include_attributes:\n        new_est._restore_runtime_state(self._get_runtime_state())\n\n    return new_est\n</code></pre>"},{"location":"reference/classification/rolling_classifier/#deep_river.classification.rolling_classifier.RollingClassifier.draw","title":"draw","text":"<pre><code>draw()\n</code></pre> <p>Render a (partial) computational graph of the wrapped model.</p> <p>Imports <code>graphviz</code> and <code>torchviz</code> lazily. Raises an informative ImportError if the optional dependencies are not installed.</p> Source code in <code>deep_river/base.py</code> <pre><code>def draw(self):  # type: ignore[override]\n    \"\"\"Render a (partial) computational graph of the wrapped model.\n\n    Imports ``graphviz`` and ``torchviz`` lazily. Raises an informative\n    ImportError if the optional dependencies are not installed.\n    \"\"\"\n    try:  # pragma: no cover\n        from torchviz import make_dot  # type: ignore\n    except Exception as err:  # noqa: BLE001\n        raise ImportError(\n            \"graphviz and torchviz must be installed to draw the model.\"\n        ) from err\n\n    first_parameter = next(self.module.parameters())\n    input_shape = first_parameter.size()\n    y_pred = self.module(torch.rand(input_shape))\n    return make_dot(y_pred.mean(), params=dict(self.module.named_parameters()))\n</code></pre>"},{"location":"reference/classification/rolling_classifier/#deep_river.classification.rolling_classifier.RollingClassifier.learn_many","title":"learn_many","text":"<pre><code>learn_many(X: DataFrame, y: Series) -&gt; None\n</code></pre> <p>Batch update: extend window with rows of X and perform a step.</p> Source code in <code>deep_river/classification/rolling_classifier.py</code> <pre><code>def learn_many(self, X: pd.DataFrame, y: pd.Series) -&gt; None:\n    \"\"\"Batch update: extend window with rows of X and perform a step.\"\"\"\n    self._update_observed_targets(y)\n    self._update_observed_features(X)\n    X = X[list(self.observed_features)]\n    self._x_window.extend(X.values.tolist())\n    X_t = self._deque2rolling_tensor(self._x_window)\n    self._learn(x=X_t, y=y)\n</code></pre>"},{"location":"reference/classification/rolling_classifier/#deep_river.classification.rolling_classifier.RollingClassifier.learn_one","title":"learn_one","text":"<pre><code>learn_one(x: dict, y: ClfTarget, **kwargs) -&gt; None\n</code></pre> <p>Learn from a single (x, y) updating the rolling window.</p> Source code in <code>deep_river/classification/rolling_classifier.py</code> <pre><code>def learn_one(self, x: dict, y: ClfTarget, **kwargs) -&gt; None:\n    \"\"\"Learn from a single (x, y) updating the rolling window.\"\"\"\n    self._update_observed_features(x)\n    self._update_observed_targets(y)\n    self._x_window.append([x.get(feature, 0) for feature in self.observed_features])\n    x_t = self._deque2rolling_tensor(self._x_window)\n    self._learn(x=x_t, y=y)\n</code></pre>"},{"location":"reference/classification/rolling_classifier/#deep_river.classification.rolling_classifier.RollingClassifier.load","title":"load  <code>classmethod</code>","text":"<pre><code>load(filepath: Union[str, Path])\n</code></pre> <p>Load a previously saved estimator.</p> <p>The method reconstructs the estimator class, its wrapped module, optimiser state and runtime information (feature names, buffers, etc.).</p> Source code in <code>deep_river/base.py</code> <pre><code>@classmethod\ndef load(cls, filepath: Union[str, Path]):\n    \"\"\"Load a previously saved estimator.\n\n    The method reconstructs the estimator class, its wrapped module, optimiser\n    state and runtime information (feature names, buffers, etc.).\n    \"\"\"\n    with open(filepath, \"rb\") as f:\n        state = pickle.load(f)\n\n    estimator_cls = cls._import_from_path(state[\"estimator_class\"])\n    init_params = state[\"init_params\"]\n\n    # Rebuild module if needed\n    if \"module\" in init_params and isinstance(init_params[\"module\"], dict):\n        module_info = init_params.pop(\"module\")\n        module_cls = cls._import_from_path(module_info[\"class\"])\n        module = module_cls(\n            **cls._filter_kwargs(module_cls.__init__, module_info[\"kwargs\"])\n        )\n        if state.get(\"model_state_dict\"):\n            module.load_state_dict(state[\"model_state_dict\"])\n        init_params[\"module\"] = module\n\n    estimator = estimator_cls(\n        **cls._filter_kwargs(estimator_cls.__init__, init_params)\n    )\n\n    if state.get(\"optimizer_state_dict\") and hasattr(estimator, \"optimizer\"):\n        try:\n            estimator.optimizer.load_state_dict(\n                state[\"optimizer_state_dict\"]  # type: ignore[arg-type]\n            )\n        except Exception:  # noqa: E722\n            pass\n\n    estimator._restore_runtime_state(state.get(\"runtime_state\", {}))\n    return estimator\n</code></pre>"},{"location":"reference/classification/rolling_classifier/#deep_river.classification.rolling_classifier.RollingClassifier.predict_proba_many","title":"predict_proba_many","text":"<pre><code>predict_proba_many(X: DataFrame) -&gt; DataFrame\n</code></pre> <p>Return probability DataFrame for multiple samples with rolling context.</p> Source code in <code>deep_river/classification/rolling_classifier.py</code> <pre><code>def predict_proba_many(self, X: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"Return probability DataFrame for multiple samples with rolling context.\"\"\"\n    self._update_observed_features(X)\n    X = X[list(self.observed_features)]\n    x_win = self._x_window.copy()\n    x_win.extend(X.values.tolist())\n    if self.append_predict:\n        self._x_window = x_win\n    self.module.eval()\n    with torch.inference_mode():\n        x_t = self._deque2rolling_tensor(x_win)\n        probas = self.module(x_t).detach().tolist()\n    return pd.DataFrame(probas)\n</code></pre>"},{"location":"reference/classification/rolling_classifier/#deep_river.classification.rolling_classifier.RollingClassifier.predict_proba_one","title":"predict_proba_one","text":"<pre><code>predict_proba_one(x: dict) -&gt; Dict[ClfTarget, float]\n</code></pre> <p>Return class probability mapping for one sample using rolling context.</p> Source code in <code>deep_river/classification/rolling_classifier.py</code> <pre><code>def predict_proba_one(self, x: dict) -&gt; Dict[ClfTarget, float]:\n    \"\"\"Return class probability mapping for one sample using rolling context.\"\"\"\n    self._update_observed_features(x)\n    x_win = self._x_window.copy()\n    x_win.append([x.get(feature, 0) for feature in self.observed_features])\n    if self.append_predict:\n        self._x_window = x_win\n    self.module.eval()\n    with torch.inference_mode():\n        x_t = self._deque2rolling_tensor(x_win)\n        y_pred = self.module(x_t)\n        proba = output2proba(y_pred, self.observed_classes, self.output_is_logit)\n    return cast(Dict[ClfTarget, float], proba[0])\n</code></pre>"},{"location":"reference/classification/rolling_classifier/#deep_river.classification.rolling_classifier.RollingClassifier.save","title":"save","text":"<pre><code>save(filepath: Union[str, Path]) -&gt; None\n</code></pre> <p>Persist the estimator (architecture, weights, optimiser &amp; runtime state).</p> <p>Parameters:</p> Name Type Description Default <code>filepath</code> <code>str | Path</code> <p>Destination file. Parent directories are created automatically.</p> required Source code in <code>deep_river/base.py</code> <pre><code>def save(self, filepath: Union[str, Path]) -&gt; None:\n    \"\"\"Persist the estimator (architecture, weights, optimiser &amp; runtime state).\n\n    Parameters\n    ----------\n    filepath : str | Path\n        Destination file. Parent directories are created automatically.\n    \"\"\"\n    filepath = Path(filepath)\n    filepath.parent.mkdir(parents=True, exist_ok=True)\n\n    state = {\n        \"estimator_class\": f\"{type(self).__module__}.{type(self).__name__}\",\n        \"init_params\": self._get_all_init_params(),\n        \"model_state_dict\": getattr(self.module, \"state_dict\", lambda: {})(),\n        \"optimizer_state_dict\": getattr(self.optimizer, \"state_dict\", lambda: {})(),\n        \"runtime_state\": self._get_runtime_state(),\n    }\n\n    with open(filepath, \"wb\") as f:\n        pickle.dump(state, f)\n</code></pre>"},{"location":"reference/classification/zoo/","title":"zoo","text":""},{"location":"reference/classification/zoo/#deep_river.classification.zoo","title":"zoo","text":"<p>Classes:</p> Name Description <code>LSTMClassifier</code> <p>Rolling LSTM classifier with dynamic class expansion.</p> <code>LogisticRegression</code> <p>Incremental logistic regression with optional dynamic class expansion.</p> <code>MultiLayerPerceptron</code> <p>Configurable multi-layer perceptron with dynamic class expansion.</p> <code>RNNClassifier</code> <p>Rolling RNN classifier with dynamic class expansion.</p>"},{"location":"reference/classification/zoo/#deep_river.classification.zoo.LSTMClassifier","title":"LSTMClassifier","text":"<pre><code>LSTMClassifier(\n    n_features: int = 10,\n    hidden_size: int = 16,\n    n_init_classes: int = 2,\n    loss_fn: Union[str, Callable] = \"cross_entropy\",\n    optimizer_fn: Union[str, Type[Optimizer]] = \"sgd\",\n    lr: float = 0.001,\n    output_is_logit: bool = True,\n    is_feature_incremental: bool = False,\n    is_class_incremental: bool = True,\n    device: str = \"cpu\",\n    seed: int = 42,\n    gradient_clip_value: float | None = None,\n    **kwargs\n)\n</code></pre> <p>               Bases: <code>RollingClassifier</code></p> <p>Rolling LSTM classifier with dynamic class expansion.</p> <p>An LSTM backbone feeds into a linear head that produces logits. Designed for sequential/temporal streams processed via a rolling window (see :class:<code>RollingClassifier</code>). The output layer (<code>head</code>) expands when new classes are observed (if enabled).</p> <p>Parameters:</p> Name Type Description Default <code>n_features</code> <code>int</code> <p>Number of input features per timestep.</p> <code>10</code> <code>hidden_size</code> <code>int</code> <p>Hidden state dimensionality of the LSTM.</p> <code>16</code> <code>n_init_classes</code> <code>int</code> <p>Initial number of output classes.</p> <code>2</code> <code>loss_fn</code> <code>str | Callable</code> <p>Training loss.</p> <code>'cross_entropy'</code> <code>optimizer_fn</code> <code>str | type</code> <p>Optimizer specification.</p> <code>'sgd'</code> <code>lr</code> <code>float</code> <p>Learning rate.</p> <code>1e-3</code> <code>output_is_logit</code> <code>bool</code> <p>Indicates outputs are logits (enables proper conversion in <code>predict_proba</code>).</p> <code>True</code> <code>is_feature_incremental</code> <code>bool</code> <p>Whether to dynamically expand the input layer when new features appear.</p> <code>False</code> <code>is_class_incremental</code> <code>bool</code> <p>Whether to expand the output layer for new class labels.</p> <code>True</code> <code>device</code> <code>str</code> <p>Torch device.</p> <code>'cpu'</code> <code>seed</code> <code>int</code> <p>Random seed.</p> <code>42</code> <code>gradient_clip_value</code> <code>float | None</code> <p>Optional gradient norm clipping value.</p> <code>None</code> <p>Examples:</p> <p>Deterministischer Test mit dem Phishing-Datenstrom: Rekurrente Gewichte &amp; Kopf-Parameter werden genullt; Bias erzwingt Klasse 0 unabh\u00e4ngig vom Input. (Nur zur Illustration; Lernrate 0 verhindert Updates.)::</p> <pre><code>&gt;&gt;&gt; import torch, random, numpy as np\n&gt;&gt;&gt; from torch import manual_seed\n&gt;&gt;&gt; from river import datasets\n&gt;&gt;&gt; from river import metrics\n&gt;&gt;&gt; from deep_river.classification.zoo import LSTMClassifier\n&gt;&gt;&gt; _ = manual_seed(42); random.seed(42); np.random.seed(42)\n&gt;&gt;&gt; stream = datasets.Phishing()\n&gt;&gt;&gt; samples = {}\n&gt;&gt;&gt; for x, y in stream:\n...     if y not in samples:\n...         samples[y] = x\n...     if len(samples) == 2:\n...         break\n&gt;&gt;&gt; x0, x1 = samples[0], samples[1]\n&gt;&gt;&gt; n_features = len(x0)\n&gt;&gt;&gt; lstm_clf = LSTMClassifier(n_features=n_features, hidden_size=3, n_init_classes=2,\n...                           is_class_incremental=False, is_feature_incremental=False,\n...                           lr=0.0, optimizer_fn='sgd')\n&gt;&gt;&gt; lstm_clf.learn_one(x0, 0)\n&gt;&gt;&gt; acc = metrics.Accuracy()\n&gt;&gt;&gt; for i, (x, y) in enumerate(datasets.Phishing().take(200)):\n...     lstm_clf.learn_one(x, y)\n...     if i &gt; 0:\n...         y_pred = lstm_clf.predict_one(x)\n...         acc.update(y, y_pred)\n&gt;&gt;&gt; assert 0.0 &lt;= acc.get() &lt;= 1.0\n&gt;&gt;&gt; print(f\"Accuracy: {acc.get():.4f}\")  # doctest: +ELLIPSIS\nAccuracy: ...\n</code></pre> <p>Methods:</p> Name Description <code>clone</code> <p>Return a fresh estimator instance with (optionally) copied state.</p> <code>draw</code> <p>Render a (partial) computational graph of the wrapped model.</p> <code>learn_many</code> <p>Batch update: extend window with rows of X and perform a step.</p> <code>learn_one</code> <p>Learn from a single (x, y) updating the rolling window.</p> <code>load</code> <p>Load a previously saved estimator.</p> <code>predict_proba_many</code> <p>Return probability DataFrame for multiple samples with rolling context.</p> <code>predict_proba_one</code> <p>Return class probability mapping for one sample using rolling context.</p> <code>save</code> <p>Persist the estimator (architecture, weights, optimiser &amp; runtime state).</p> Source code in <code>deep_river/classification/zoo.py</code> <pre><code>def __init__(\n    self,\n    n_features: int = 10,\n    hidden_size: int = 16,\n    n_init_classes: int = 2,\n    loss_fn: Union[str, Callable] = \"cross_entropy\",\n    optimizer_fn: Union[str, Type[optim.Optimizer]] = \"sgd\",\n    lr: float = 1e-3,\n    output_is_logit: bool = True,\n    is_feature_incremental: bool = False,\n    is_class_incremental: bool = True,\n    device: str = \"cpu\",\n    seed: int = 42,\n    gradient_clip_value: float | None = None,\n    **kwargs,\n):\n    self.n_features = n_features\n    self.hidden_size = hidden_size\n    self.n_init_classes = n_init_classes\n    module = LSTMClassifier.LSTMModule(\n        n_features=n_features,\n        hidden_size=hidden_size,\n        n_init_classes=n_init_classes,\n    )\n    if \"module\" in kwargs:\n        del kwargs[\"module\"]\n    super().__init__(\n        module=module,\n        loss_fn=loss_fn,\n        optimizer_fn=optimizer_fn,\n        output_is_logit=output_is_logit,\n        is_feature_incremental=is_feature_incremental,\n        is_class_incremental=is_class_incremental,\n        device=device,\n        lr=lr,\n        seed=seed,\n        gradient_clip_value=gradient_clip_value,\n        **kwargs,\n    )\n</code></pre>"},{"location":"reference/classification/zoo/#deep_river.classification.zoo.LSTMClassifier.clone","title":"clone","text":"<pre><code>clone(\n    new_params=None,\n    include_attributes: bool = False,\n    copy_weights: bool = False,\n)\n</code></pre> <p>Return a fresh estimator instance with (optionally) copied state.</p> <p>Parameters:</p> Name Type Description Default <code>new_params</code> <code>dict | None</code> <p>Parameter overrides for the cloned instance.</p> <code>None</code> <code>include_attributes</code> <code>bool</code> <p>If True, runtime state (observed features, buffers) is also copied.</p> <code>False</code> <code>copy_weights</code> <code>bool</code> <p>If True, model weights are copied (otherwise the module is re\u2011initialised).</p> <code>False</code> Source code in <code>deep_river/base.py</code> <pre><code>def clone(\n    self,\n    new_params=None,\n    include_attributes: bool = False,\n    copy_weights: bool = False,\n):\n    \"\"\"Return a fresh estimator instance with (optionally) copied state.\n\n    Parameters\n    ----------\n    new_params : dict | None\n        Parameter overrides for the cloned instance.\n    include_attributes : bool, default=False\n        If True, runtime state (observed features, buffers) is also copied.\n    copy_weights : bool, default=False\n        If True, model weights are copied (otherwise the module is re\u2011initialised).\n    \"\"\"\n    new_params = new_params or {}\n    copy_weights = new_params.pop(\"copy_weights\", copy_weights)\n\n    params = {**self._get_all_init_params(), **new_params}\n\n    if \"module\" not in new_params:\n        params[\"module\"] = self._rebuild_module()\n\n    new_est = self.__class__(**self._filter_kwargs(self.__class__.__init__, params))\n\n    if copy_weights and hasattr(self.module, \"state_dict\"):\n        new_est.module.load_state_dict(self.module.state_dict())\n\n    if include_attributes:\n        new_est._restore_runtime_state(self._get_runtime_state())\n\n    return new_est\n</code></pre>"},{"location":"reference/classification/zoo/#deep_river.classification.zoo.LSTMClassifier.draw","title":"draw","text":"<pre><code>draw()\n</code></pre> <p>Render a (partial) computational graph of the wrapped model.</p> <p>Imports <code>graphviz</code> and <code>torchviz</code> lazily. Raises an informative ImportError if the optional dependencies are not installed.</p> Source code in <code>deep_river/base.py</code> <pre><code>def draw(self):  # type: ignore[override]\n    \"\"\"Render a (partial) computational graph of the wrapped model.\n\n    Imports ``graphviz`` and ``torchviz`` lazily. Raises an informative\n    ImportError if the optional dependencies are not installed.\n    \"\"\"\n    try:  # pragma: no cover\n        from torchviz import make_dot  # type: ignore\n    except Exception as err:  # noqa: BLE001\n        raise ImportError(\n            \"graphviz and torchviz must be installed to draw the model.\"\n        ) from err\n\n    first_parameter = next(self.module.parameters())\n    input_shape = first_parameter.size()\n    y_pred = self.module(torch.rand(input_shape))\n    return make_dot(y_pred.mean(), params=dict(self.module.named_parameters()))\n</code></pre>"},{"location":"reference/classification/zoo/#deep_river.classification.zoo.LSTMClassifier.learn_many","title":"learn_many","text":"<pre><code>learn_many(X: DataFrame, y: Series) -&gt; None\n</code></pre> <p>Batch update: extend window with rows of X and perform a step.</p> Source code in <code>deep_river/classification/rolling_classifier.py</code> <pre><code>def learn_many(self, X: pd.DataFrame, y: pd.Series) -&gt; None:\n    \"\"\"Batch update: extend window with rows of X and perform a step.\"\"\"\n    self._update_observed_targets(y)\n    self._update_observed_features(X)\n    X = X[list(self.observed_features)]\n    self._x_window.extend(X.values.tolist())\n    X_t = self._deque2rolling_tensor(self._x_window)\n    self._learn(x=X_t, y=y)\n</code></pre>"},{"location":"reference/classification/zoo/#deep_river.classification.zoo.LSTMClassifier.learn_one","title":"learn_one","text":"<pre><code>learn_one(x: dict, y: ClfTarget, **kwargs) -&gt; None\n</code></pre> <p>Learn from a single (x, y) updating the rolling window.</p> Source code in <code>deep_river/classification/rolling_classifier.py</code> <pre><code>def learn_one(self, x: dict, y: ClfTarget, **kwargs) -&gt; None:\n    \"\"\"Learn from a single (x, y) updating the rolling window.\"\"\"\n    self._update_observed_features(x)\n    self._update_observed_targets(y)\n    self._x_window.append([x.get(feature, 0) for feature in self.observed_features])\n    x_t = self._deque2rolling_tensor(self._x_window)\n    self._learn(x=x_t, y=y)\n</code></pre>"},{"location":"reference/classification/zoo/#deep_river.classification.zoo.LSTMClassifier.load","title":"load  <code>classmethod</code>","text":"<pre><code>load(filepath: Union[str, Path])\n</code></pre> <p>Load a previously saved estimator.</p> <p>The method reconstructs the estimator class, its wrapped module, optimiser state and runtime information (feature names, buffers, etc.).</p> Source code in <code>deep_river/base.py</code> <pre><code>@classmethod\ndef load(cls, filepath: Union[str, Path]):\n    \"\"\"Load a previously saved estimator.\n\n    The method reconstructs the estimator class, its wrapped module, optimiser\n    state and runtime information (feature names, buffers, etc.).\n    \"\"\"\n    with open(filepath, \"rb\") as f:\n        state = pickle.load(f)\n\n    estimator_cls = cls._import_from_path(state[\"estimator_class\"])\n    init_params = state[\"init_params\"]\n\n    # Rebuild module if needed\n    if \"module\" in init_params and isinstance(init_params[\"module\"], dict):\n        module_info = init_params.pop(\"module\")\n        module_cls = cls._import_from_path(module_info[\"class\"])\n        module = module_cls(\n            **cls._filter_kwargs(module_cls.__init__, module_info[\"kwargs\"])\n        )\n        if state.get(\"model_state_dict\"):\n            module.load_state_dict(state[\"model_state_dict\"])\n        init_params[\"module\"] = module\n\n    estimator = estimator_cls(\n        **cls._filter_kwargs(estimator_cls.__init__, init_params)\n    )\n\n    if state.get(\"optimizer_state_dict\") and hasattr(estimator, \"optimizer\"):\n        try:\n            estimator.optimizer.load_state_dict(\n                state[\"optimizer_state_dict\"]  # type: ignore[arg-type]\n            )\n        except Exception:  # noqa: E722\n            pass\n\n    estimator._restore_runtime_state(state.get(\"runtime_state\", {}))\n    return estimator\n</code></pre>"},{"location":"reference/classification/zoo/#deep_river.classification.zoo.LSTMClassifier.predict_proba_many","title":"predict_proba_many","text":"<pre><code>predict_proba_many(X: DataFrame) -&gt; DataFrame\n</code></pre> <p>Return probability DataFrame for multiple samples with rolling context.</p> Source code in <code>deep_river/classification/rolling_classifier.py</code> <pre><code>def predict_proba_many(self, X: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"Return probability DataFrame for multiple samples with rolling context.\"\"\"\n    self._update_observed_features(X)\n    X = X[list(self.observed_features)]\n    x_win = self._x_window.copy()\n    x_win.extend(X.values.tolist())\n    if self.append_predict:\n        self._x_window = x_win\n    self.module.eval()\n    with torch.inference_mode():\n        x_t = self._deque2rolling_tensor(x_win)\n        probas = self.module(x_t).detach().tolist()\n    return pd.DataFrame(probas)\n</code></pre>"},{"location":"reference/classification/zoo/#deep_river.classification.zoo.LSTMClassifier.predict_proba_one","title":"predict_proba_one","text":"<pre><code>predict_proba_one(x: dict) -&gt; Dict[ClfTarget, float]\n</code></pre> <p>Return class probability mapping for one sample using rolling context.</p> Source code in <code>deep_river/classification/rolling_classifier.py</code> <pre><code>def predict_proba_one(self, x: dict) -&gt; Dict[ClfTarget, float]:\n    \"\"\"Return class probability mapping for one sample using rolling context.\"\"\"\n    self._update_observed_features(x)\n    x_win = self._x_window.copy()\n    x_win.append([x.get(feature, 0) for feature in self.observed_features])\n    if self.append_predict:\n        self._x_window = x_win\n    self.module.eval()\n    with torch.inference_mode():\n        x_t = self._deque2rolling_tensor(x_win)\n        y_pred = self.module(x_t)\n        proba = output2proba(y_pred, self.observed_classes, self.output_is_logit)\n    return cast(Dict[ClfTarget, float], proba[0])\n</code></pre>"},{"location":"reference/classification/zoo/#deep_river.classification.zoo.LSTMClassifier.save","title":"save","text":"<pre><code>save(filepath: Union[str, Path]) -&gt; None\n</code></pre> <p>Persist the estimator (architecture, weights, optimiser &amp; runtime state).</p> <p>Parameters:</p> Name Type Description Default <code>filepath</code> <code>str | Path</code> <p>Destination file. Parent directories are created automatically.</p> required Source code in <code>deep_river/base.py</code> <pre><code>def save(self, filepath: Union[str, Path]) -&gt; None:\n    \"\"\"Persist the estimator (architecture, weights, optimiser &amp; runtime state).\n\n    Parameters\n    ----------\n    filepath : str | Path\n        Destination file. Parent directories are created automatically.\n    \"\"\"\n    filepath = Path(filepath)\n    filepath.parent.mkdir(parents=True, exist_ok=True)\n\n    state = {\n        \"estimator_class\": f\"{type(self).__module__}.{type(self).__name__}\",\n        \"init_params\": self._get_all_init_params(),\n        \"model_state_dict\": getattr(self.module, \"state_dict\", lambda: {})(),\n        \"optimizer_state_dict\": getattr(self.optimizer, \"state_dict\", lambda: {})(),\n        \"runtime_state\": self._get_runtime_state(),\n    }\n\n    with open(filepath, \"wb\") as f:\n        pickle.dump(state, f)\n</code></pre>"},{"location":"reference/classification/zoo/#deep_river.classification.zoo.LogisticRegression","title":"LogisticRegression","text":"<pre><code>LogisticRegression(\n    n_features: int = 10,\n    n_init_classes: int = 2,\n    loss_fn: Union[str, Callable] = \"cross_entropy\",\n    optimizer_fn: Union[str, Type[Optimizer]] = \"sgd\",\n    lr: float = 0.001,\n    output_is_logit: bool = True,\n    is_feature_incremental: bool = False,\n    is_class_incremental: bool = True,\n    device: str = \"cpu\",\n    seed: int = 42,\n    gradient_clip_value: float | None = None,\n    **kwargs\n)\n</code></pre> <p>               Bases: <code>Classifier</code></p> <p>Incremental logistic regression with optional dynamic class expansion.</p> <p>This variant outputs raw logits (no internal softmax) so that losses like <code>cross_entropy</code> can be applied directly. The output layer can grow in response to newly observed class labels when <code>is_class_incremental=True</code>.</p> <p>Parameters:</p> Name Type Description Default <code>n_features</code> <code>int</code> <p>Initial number of input features.</p> <code>10</code> <code>n_init_classes</code> <code>int</code> <p>Initial number of output units/classes. Expanded automatically if new classes appear and class incrementality is enabled.</p> <code>2</code> <code>loss_fn</code> <code>str | Callable</code> <p>Training loss.</p> <code>'cross_entropy'</code> <code>optimizer_fn</code> <code>str | type</code> <p>Optimizer specification.</p> <code>'sgd'</code> <code>lr</code> <code>float</code> <p>Learning rate.</p> <code>1e-3</code> <code>output_is_logit</code> <code>bool</code> <p>Indicates outputs are logits (enables proper conversion in <code>predict_proba</code>).</p> <code>True</code> <code>is_feature_incremental</code> <code>bool</code> <p>Whether to dynamically expand the input layer when new features appear.</p> <code>False</code> <code>is_class_incremental</code> <code>bool</code> <p>Whether to expand the output layer for new class labels.</p> <code>True</code> <code>device</code> <code>str</code> <p>Torch device.</p> <code>'cpu'</code> <code>seed</code> <code>int</code> <p>Random seed.</p> <code>42</code> <code>gradient_clip_value</code> <code>float | None</code> <p>Optional gradient norm clipping value.</p> <code>None</code> <code>**kwargs</code> <p>Forwarded to the parent constructor.</p> <code>{}</code> <p>Examples:</p> <p>Streaming binary classification on the Phishing dataset. The exact Accuracy value may vary depending on library version and hardware::</p> <pre><code>&gt;&gt;&gt; import random, numpy as np, torch\n&gt;&gt;&gt; from torch import manual_seed\n&gt;&gt;&gt; from river import datasets, metrics\n&gt;&gt;&gt; from deep_river.classification.zoo import LogisticRegression\n&gt;&gt;&gt; _ = manual_seed(42); random.seed(42); np.random.seed(42)\n&gt;&gt;&gt; first_x, _ = next(iter(datasets.Phishing()))\n&gt;&gt;&gt; clf = LogisticRegression(\n...     n_features=len(first_x), n_init_classes=2,\n...     optimizer_fn='sgd', lr=1e-2, is_class_incremental=True,\n... )\n&gt;&gt;&gt; acc = metrics.Accuracy()\n&gt;&gt;&gt; for i, (x, y) in enumerate(datasets.Phishing().take(200)):\n...     clf.learn_one(x, y)\n...     if i &gt; 0:\n...         y_pred = clf.predict_one(x)\n...         acc.update(y, y_pred)\n&gt;&gt;&gt; assert 0.5 &lt;= acc.get() &lt;= 1.0\n&gt;&gt;&gt; print(f\"Accuracy: {acc.get():.4f}\")  # doctest: +ELLIPSIS\nAccuracy: ...\n</code></pre> <p>Methods:</p> Name Description <code>clone</code> <p>Return a fresh estimator instance with (optionally) copied state.</p> <code>draw</code> <p>Render a (partial) computational graph of the wrapped model.</p> <code>learn_many</code> <p>Learn from a batch of instances.</p> <code>learn_one</code> <p>Learn from a single instance.</p> <code>load</code> <p>Load a previously saved estimator.</p> <code>predict_proba_many</code> <p>Predict probabilities for a batch of instances.</p> <code>predict_proba_one</code> <p>Predict class membership probabilities for one instance.</p> <code>save</code> <p>Persist the estimator (architecture, weights, optimiser &amp; runtime state).</p> Source code in <code>deep_river/classification/zoo.py</code> <pre><code>def __init__(\n    self,\n    n_features: int = 10,\n    n_init_classes: int = 2,\n    loss_fn: Union[str, Callable] = \"cross_entropy\",\n    optimizer_fn: Union[str, Type[optim.Optimizer]] = \"sgd\",\n    lr: float = 1e-3,\n    output_is_logit: bool = True,\n    is_feature_incremental: bool = False,\n    is_class_incremental: bool = True,\n    device: str = \"cpu\",\n    seed: int = 42,\n    gradient_clip_value: float | None = None,\n    **kwargs,\n):\n    self.n_features = n_features\n    self.n_init_classes = n_init_classes\n    module = LogisticRegression.LRModule(\n        n_features=n_features, n_init_classes=n_init_classes\n    )\n    if \"module\" in kwargs:\n        del kwargs[\"module\"]\n    super().__init__(\n        module=module,\n        loss_fn=loss_fn,\n        optimizer_fn=optimizer_fn,\n        output_is_logit=output_is_logit,\n        is_feature_incremental=is_feature_incremental,\n        is_class_incremental=is_class_incremental,\n        device=device,\n        lr=lr,\n        seed=seed,\n        gradient_clip_value=gradient_clip_value,\n        **kwargs,\n    )\n</code></pre>"},{"location":"reference/classification/zoo/#deep_river.classification.zoo.LogisticRegression.clone","title":"clone","text":"<pre><code>clone(\n    new_params=None,\n    include_attributes: bool = False,\n    copy_weights: bool = False,\n)\n</code></pre> <p>Return a fresh estimator instance with (optionally) copied state.</p> <p>Parameters:</p> Name Type Description Default <code>new_params</code> <code>dict | None</code> <p>Parameter overrides for the cloned instance.</p> <code>None</code> <code>include_attributes</code> <code>bool</code> <p>If True, runtime state (observed features, buffers) is also copied.</p> <code>False</code> <code>copy_weights</code> <code>bool</code> <p>If True, model weights are copied (otherwise the module is re\u2011initialised).</p> <code>False</code> Source code in <code>deep_river/base.py</code> <pre><code>def clone(\n    self,\n    new_params=None,\n    include_attributes: bool = False,\n    copy_weights: bool = False,\n):\n    \"\"\"Return a fresh estimator instance with (optionally) copied state.\n\n    Parameters\n    ----------\n    new_params : dict | None\n        Parameter overrides for the cloned instance.\n    include_attributes : bool, default=False\n        If True, runtime state (observed features, buffers) is also copied.\n    copy_weights : bool, default=False\n        If True, model weights are copied (otherwise the module is re\u2011initialised).\n    \"\"\"\n    new_params = new_params or {}\n    copy_weights = new_params.pop(\"copy_weights\", copy_weights)\n\n    params = {**self._get_all_init_params(), **new_params}\n\n    if \"module\" not in new_params:\n        params[\"module\"] = self._rebuild_module()\n\n    new_est = self.__class__(**self._filter_kwargs(self.__class__.__init__, params))\n\n    if copy_weights and hasattr(self.module, \"state_dict\"):\n        new_est.module.load_state_dict(self.module.state_dict())\n\n    if include_attributes:\n        new_est._restore_runtime_state(self._get_runtime_state())\n\n    return new_est\n</code></pre>"},{"location":"reference/classification/zoo/#deep_river.classification.zoo.LogisticRegression.draw","title":"draw","text":"<pre><code>draw()\n</code></pre> <p>Render a (partial) computational graph of the wrapped model.</p> <p>Imports <code>graphviz</code> and <code>torchviz</code> lazily. Raises an informative ImportError if the optional dependencies are not installed.</p> Source code in <code>deep_river/base.py</code> <pre><code>def draw(self):  # type: ignore[override]\n    \"\"\"Render a (partial) computational graph of the wrapped model.\n\n    Imports ``graphviz`` and ``torchviz`` lazily. Raises an informative\n    ImportError if the optional dependencies are not installed.\n    \"\"\"\n    try:  # pragma: no cover\n        from torchviz import make_dot  # type: ignore\n    except Exception as err:  # noqa: BLE001\n        raise ImportError(\n            \"graphviz and torchviz must be installed to draw the model.\"\n        ) from err\n\n    first_parameter = next(self.module.parameters())\n    input_shape = first_parameter.size()\n    y_pred = self.module(torch.rand(input_shape))\n    return make_dot(y_pred.mean(), params=dict(self.module.named_parameters()))\n</code></pre>"},{"location":"reference/classification/zoo/#deep_river.classification.zoo.LogisticRegression.learn_many","title":"learn_many","text":"<pre><code>learn_many(X: DataFrame, y: Series) -&gt; None\n</code></pre> <p>Learn from a batch of instances.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>DataFrame</code> <p>Batch of feature rows.</p> required <code>y</code> <code>Series</code> <p>Corresponding labels.</p> required Source code in <code>deep_river/classification/classifier.py</code> <pre><code>def learn_many(self, X: pd.DataFrame, y: pd.Series) -&gt; None:\n    \"\"\"Learn from a batch of instances.\n\n    Parameters\n    ----------\n    X : pandas.DataFrame\n        Batch of feature rows.\n    y : pandas.Series\n        Corresponding labels.\n    \"\"\"\n    self._update_observed_features(X)\n    self._update_observed_targets(y)\n    x_t = self._df2tensor(X)\n    if self.loss_fn == \"cross_entropy\":\n        self._classification_step_cross_entropy(x_t, y)\n    else:\n        self._learn(x_t, y)\n</code></pre>"},{"location":"reference/classification/zoo/#deep_river.classification.zoo.LogisticRegression.learn_one","title":"learn_one","text":"<pre><code>learn_one(x: dict, y: ClfTarget) -&gt; None\n</code></pre> <p>Learn from a single instance.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>dict</code> <p>Feature dictionary.</p> required <code>y</code> <code>hashable</code> <p>Class label.</p> required Source code in <code>deep_river/classification/classifier.py</code> <pre><code>def learn_one(self, x: dict, y: base.typing.ClfTarget) -&gt; None:\n    \"\"\"Learn from a single instance.\n\n    Parameters\n    ----------\n    x : dict\n        Feature dictionary.\n    y : hashable\n        Class label.\n    \"\"\"\n    self._update_observed_features(x)\n    self._update_observed_targets(y)\n    x_t = self._dict2tensor(x)\n    if self.loss_fn == \"cross_entropy\":\n        self._classification_step_cross_entropy(x_t, y)\n    else:\n        # One-hot pathway / other losses\n        self._learn(x_t, y)\n</code></pre>"},{"location":"reference/classification/zoo/#deep_river.classification.zoo.LogisticRegression.load","title":"load  <code>classmethod</code>","text":"<pre><code>load(filepath: Union[str, Path])\n</code></pre> <p>Load a previously saved estimator.</p> <p>The method reconstructs the estimator class, its wrapped module, optimiser state and runtime information (feature names, buffers, etc.).</p> Source code in <code>deep_river/base.py</code> <pre><code>@classmethod\ndef load(cls, filepath: Union[str, Path]):\n    \"\"\"Load a previously saved estimator.\n\n    The method reconstructs the estimator class, its wrapped module, optimiser\n    state and runtime information (feature names, buffers, etc.).\n    \"\"\"\n    with open(filepath, \"rb\") as f:\n        state = pickle.load(f)\n\n    estimator_cls = cls._import_from_path(state[\"estimator_class\"])\n    init_params = state[\"init_params\"]\n\n    # Rebuild module if needed\n    if \"module\" in init_params and isinstance(init_params[\"module\"], dict):\n        module_info = init_params.pop(\"module\")\n        module_cls = cls._import_from_path(module_info[\"class\"])\n        module = module_cls(\n            **cls._filter_kwargs(module_cls.__init__, module_info[\"kwargs\"])\n        )\n        if state.get(\"model_state_dict\"):\n            module.load_state_dict(state[\"model_state_dict\"])\n        init_params[\"module\"] = module\n\n    estimator = estimator_cls(\n        **cls._filter_kwargs(estimator_cls.__init__, init_params)\n    )\n\n    if state.get(\"optimizer_state_dict\") and hasattr(estimator, \"optimizer\"):\n        try:\n            estimator.optimizer.load_state_dict(\n                state[\"optimizer_state_dict\"]  # type: ignore[arg-type]\n            )\n        except Exception:  # noqa: E722\n            pass\n\n    estimator._restore_runtime_state(state.get(\"runtime_state\", {}))\n    return estimator\n</code></pre>"},{"location":"reference/classification/zoo/#deep_river.classification.zoo.LogisticRegression.predict_proba_many","title":"predict_proba_many","text":"<pre><code>predict_proba_many(X: DataFrame) -&gt; DataFrame\n</code></pre> <p>Predict probabilities for a batch of instances.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>DataFrame</code> <p>Feature matrix.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>Each row sums to 1 (multi-class) or has two columns for binary.</p> Source code in <code>deep_river/classification/classifier.py</code> <pre><code>def predict_proba_many(self, X: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"Predict probabilities for a batch of instances.\n\n    Parameters\n    ----------\n    X : pandas.DataFrame\n        Feature matrix.\n\n    Returns\n    -------\n    pandas.DataFrame\n        Each row sums to 1 (multi-class) or has two columns for binary.\n    \"\"\"\n    self._update_observed_features(X)\n    x_t = self._df2tensor(X)\n    self.module.eval()\n    with torch.inference_mode():\n        y_preds = self.module(x_t)\n    return pd.DataFrame(\n        output2proba(y_preds, self.observed_classes, self.output_is_logit)\n    )\n</code></pre>"},{"location":"reference/classification/zoo/#deep_river.classification.zoo.LogisticRegression.predict_proba_one","title":"predict_proba_one","text":"<pre><code>predict_proba_one(x: dict) -&gt; dict[ClfTarget, float]\n</code></pre> <p>Predict class membership probabilities for one instance.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>dict</code> <p>Feature dictionary.</p> required <p>Returns:</p> Type Description <code>dict</code> <p>Mapping from label -&gt; probability.</p> Source code in <code>deep_river/classification/classifier.py</code> <pre><code>def predict_proba_one(self, x: dict) -&gt; dict[base.typing.ClfTarget, float]:\n    \"\"\"Predict class membership probabilities for one instance.\n\n    Parameters\n    ----------\n    x : dict\n        Feature dictionary.\n\n    Returns\n    -------\n    dict\n        Mapping from label -&gt; probability.\n    \"\"\"\n    self._update_observed_features(x)\n    x_t = self._dict2tensor(x)\n    self.module.eval()\n    with torch.inference_mode():\n        y_pred = self.module(x_t)\n    raw = output2proba(y_pred, self.observed_classes, self.output_is_logit)[0]\n    return cast(dict[base.typing.ClfTarget, float], raw)\n</code></pre>"},{"location":"reference/classification/zoo/#deep_river.classification.zoo.LogisticRegression.save","title":"save","text":"<pre><code>save(filepath: Union[str, Path]) -&gt; None\n</code></pre> <p>Persist the estimator (architecture, weights, optimiser &amp; runtime state).</p> <p>Parameters:</p> Name Type Description Default <code>filepath</code> <code>str | Path</code> <p>Destination file. Parent directories are created automatically.</p> required Source code in <code>deep_river/base.py</code> <pre><code>def save(self, filepath: Union[str, Path]) -&gt; None:\n    \"\"\"Persist the estimator (architecture, weights, optimiser &amp; runtime state).\n\n    Parameters\n    ----------\n    filepath : str | Path\n        Destination file. Parent directories are created automatically.\n    \"\"\"\n    filepath = Path(filepath)\n    filepath.parent.mkdir(parents=True, exist_ok=True)\n\n    state = {\n        \"estimator_class\": f\"{type(self).__module__}.{type(self).__name__}\",\n        \"init_params\": self._get_all_init_params(),\n        \"model_state_dict\": getattr(self.module, \"state_dict\", lambda: {})(),\n        \"optimizer_state_dict\": getattr(self.optimizer, \"state_dict\", lambda: {})(),\n        \"runtime_state\": self._get_runtime_state(),\n    }\n\n    with open(filepath, \"wb\") as f:\n        pickle.dump(state, f)\n</code></pre>"},{"location":"reference/classification/zoo/#deep_river.classification.zoo.MultiLayerPerceptron","title":"MultiLayerPerceptron","text":"<pre><code>MultiLayerPerceptron(\n    n_features: int = 10,\n    n_width: int = 5,\n    n_layers: int = 5,\n    n_init_classes: int = 2,\n    loss_fn: Union[str, Callable] = \"cross_entropy\",\n    optimizer_fn: Union[str, Type[Optimizer]] = \"sgd\",\n    lr: float = 0.001,\n    output_is_logit: bool = True,\n    is_feature_incremental: bool = False,\n    is_class_incremental: bool = True,\n    device: str = \"cpu\",\n    seed: int = 42,\n    gradient_clip_value: float | None = None,\n    **kwargs\n)\n</code></pre> <p>               Bases: <code>Classifier</code></p> <p>Configurable multi-layer perceptron with dynamic class expansion.</p> <p>Hidden layers use ReLU activations; the output layer emits raw logits.</p> <p>Parameters:</p> Name Type Description Default <code>n_features</code> <code>int</code> <p>Initial number of features.</p> <code>10</code> <code>n_width</code> <code>int</code> <p>Width (units) of each hidden layer.</p> <code>5</code> <code>n_layers</code> <code>int</code> <p>Number of hidden layers (&gt;=1). If 1, only the input layer feeds the output.</p> <code>5</code> <code>n_init_classes</code> <code>int</code> <p>Initial number of classes/output units.</p> <code>2</code> <code>loss_fn</code> <code>Union[str, Callable]</code> <p>is_class_incremental, device, seed, gradient_clip_value, **kwargs See :class:<code>LogisticRegression</code>.</p> <code>'cross_entropy'</code> <code>optimizer_fn</code> <code>Union[str, Callable]</code> <p>is_class_incremental, device, seed, gradient_clip_value, **kwargs See :class:<code>LogisticRegression</code>.</p> <code>'cross_entropy'</code> <code>lr</code> <code>Union[str, Callable]</code> <p>is_class_incremental, device, seed, gradient_clip_value, **kwargs See :class:<code>LogisticRegression</code>.</p> <code>'cross_entropy'</code> <code>output_is_logit</code> <code>Union[str, Callable]</code> <p>is_class_incremental, device, seed, gradient_clip_value, **kwargs See :class:<code>LogisticRegression</code>.</p> <code>'cross_entropy'</code> <code>is_feature_incremental</code> <code>Union[str, Callable]</code> <p>is_class_incremental, device, seed, gradient_clip_value, **kwargs See :class:<code>LogisticRegression</code>.</p> <code>'cross_entropy'</code> <p>Examples:</p> <pre><code>Phishing dataset stream with online Accuracy. The exact value may vary\ndepending on library version and hardware::\n\n&gt;&gt;&gt; import random, numpy as np, torch\n&gt;&gt;&gt; from torch import manual_seed\n&gt;&gt;&gt; from river import datasets, metrics\n&gt;&gt;&gt; from deep_river.classification.zoo import MultiLayerPerceptron\n&gt;&gt;&gt; _ = manual_seed(42); random.seed(42); np.random.seed(42)\n&gt;&gt;&gt; first_x, _ = next(iter(datasets.Phishing()))\n&gt;&gt;&gt; mlp = MultiLayerPerceptron(\n...     n_features=len(first_x), n_width=8, n_layers=2, n_init_classes=2,\n...     optimizer_fn='sgd', lr=5e-3, is_class_incremental=True,\n... )\n&gt;&gt;&gt; acc = metrics.Accuracy()\n&gt;&gt;&gt; for i, (x, y) in enumerate(datasets.Phishing().take(200)):\n...     mlp.learn_one(x, y)\n...     if i &gt; 0:\n...         y_pred = mlp.predict_one(x)\n...         acc.update(y, y_pred)\n&gt;&gt;&gt; assert 0.5 &lt;= acc.get() &lt;= 1.0\n&gt;&gt;&gt; print(f\"Accuracy: {acc.get():.4f}\")  # doctest: +ELLIPSIS\nAccuracy: ...\n</code></pre> <p>Methods:</p> Name Description <code>clone</code> <p>Return a fresh estimator instance with (optionally) copied state.</p> <code>draw</code> <p>Render a (partial) computational graph of the wrapped model.</p> <code>learn_many</code> <p>Learn from a batch of instances.</p> <code>learn_one</code> <p>Learn from a single instance.</p> <code>load</code> <p>Load a previously saved estimator.</p> <code>predict_proba_many</code> <p>Predict probabilities for a batch of instances.</p> <code>predict_proba_one</code> <p>Predict class membership probabilities for one instance.</p> <code>save</code> <p>Persist the estimator (architecture, weights, optimiser &amp; runtime state).</p> Source code in <code>deep_river/classification/zoo.py</code> <pre><code>def __init__(\n    self,\n    n_features: int = 10,\n    n_width: int = 5,\n    n_layers: int = 5,\n    n_init_classes: int = 2,\n    loss_fn: Union[str, Callable] = \"cross_entropy\",\n    optimizer_fn: Union[str, Type[optim.Optimizer]] = \"sgd\",\n    lr: float = 1e-3,\n    output_is_logit: bool = True,\n    is_feature_incremental: bool = False,\n    is_class_incremental: bool = True,\n    device: str = \"cpu\",\n    seed: int = 42,\n    gradient_clip_value: float | None = None,\n    **kwargs,\n):\n    self.n_features = n_features\n    self.n_width = n_width\n    self.n_layers = n_layers\n    self.n_init_classes = n_init_classes\n    module = MultiLayerPerceptron.MLPModule(\n        n_width=n_width,\n        n_layers=n_layers,\n        n_features=n_features,\n        n_init_classes=n_init_classes,\n    )\n    if \"module\" in kwargs:\n        del kwargs[\"module\"]\n    super().__init__(\n        module=module,\n        loss_fn=loss_fn,\n        optimizer_fn=optimizer_fn,\n        output_is_logit=output_is_logit,\n        is_feature_incremental=is_feature_incremental,\n        is_class_incremental=is_class_incremental,\n        device=device,\n        lr=lr,\n        seed=seed,\n        gradient_clip_value=gradient_clip_value,\n        **kwargs,\n    )\n</code></pre>"},{"location":"reference/classification/zoo/#deep_river.classification.zoo.MultiLayerPerceptron.clone","title":"clone","text":"<pre><code>clone(\n    new_params=None,\n    include_attributes: bool = False,\n    copy_weights: bool = False,\n)\n</code></pre> <p>Return a fresh estimator instance with (optionally) copied state.</p> <p>Parameters:</p> Name Type Description Default <code>new_params</code> <code>dict | None</code> <p>Parameter overrides for the cloned instance.</p> <code>None</code> <code>include_attributes</code> <code>bool</code> <p>If True, runtime state (observed features, buffers) is also copied.</p> <code>False</code> <code>copy_weights</code> <code>bool</code> <p>If True, model weights are copied (otherwise the module is re\u2011initialised).</p> <code>False</code> Source code in <code>deep_river/base.py</code> <pre><code>def clone(\n    self,\n    new_params=None,\n    include_attributes: bool = False,\n    copy_weights: bool = False,\n):\n    \"\"\"Return a fresh estimator instance with (optionally) copied state.\n\n    Parameters\n    ----------\n    new_params : dict | None\n        Parameter overrides for the cloned instance.\n    include_attributes : bool, default=False\n        If True, runtime state (observed features, buffers) is also copied.\n    copy_weights : bool, default=False\n        If True, model weights are copied (otherwise the module is re\u2011initialised).\n    \"\"\"\n    new_params = new_params or {}\n    copy_weights = new_params.pop(\"copy_weights\", copy_weights)\n\n    params = {**self._get_all_init_params(), **new_params}\n\n    if \"module\" not in new_params:\n        params[\"module\"] = self._rebuild_module()\n\n    new_est = self.__class__(**self._filter_kwargs(self.__class__.__init__, params))\n\n    if copy_weights and hasattr(self.module, \"state_dict\"):\n        new_est.module.load_state_dict(self.module.state_dict())\n\n    if include_attributes:\n        new_est._restore_runtime_state(self._get_runtime_state())\n\n    return new_est\n</code></pre>"},{"location":"reference/classification/zoo/#deep_river.classification.zoo.MultiLayerPerceptron.draw","title":"draw","text":"<pre><code>draw()\n</code></pre> <p>Render a (partial) computational graph of the wrapped model.</p> <p>Imports <code>graphviz</code> and <code>torchviz</code> lazily. Raises an informative ImportError if the optional dependencies are not installed.</p> Source code in <code>deep_river/base.py</code> <pre><code>def draw(self):  # type: ignore[override]\n    \"\"\"Render a (partial) computational graph of the wrapped model.\n\n    Imports ``graphviz`` and ``torchviz`` lazily. Raises an informative\n    ImportError if the optional dependencies are not installed.\n    \"\"\"\n    try:  # pragma: no cover\n        from torchviz import make_dot  # type: ignore\n    except Exception as err:  # noqa: BLE001\n        raise ImportError(\n            \"graphviz and torchviz must be installed to draw the model.\"\n        ) from err\n\n    first_parameter = next(self.module.parameters())\n    input_shape = first_parameter.size()\n    y_pred = self.module(torch.rand(input_shape))\n    return make_dot(y_pred.mean(), params=dict(self.module.named_parameters()))\n</code></pre>"},{"location":"reference/classification/zoo/#deep_river.classification.zoo.MultiLayerPerceptron.learn_many","title":"learn_many","text":"<pre><code>learn_many(X: DataFrame, y: Series) -&gt; None\n</code></pre> <p>Learn from a batch of instances.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>DataFrame</code> <p>Batch of feature rows.</p> required <code>y</code> <code>Series</code> <p>Corresponding labels.</p> required Source code in <code>deep_river/classification/classifier.py</code> <pre><code>def learn_many(self, X: pd.DataFrame, y: pd.Series) -&gt; None:\n    \"\"\"Learn from a batch of instances.\n\n    Parameters\n    ----------\n    X : pandas.DataFrame\n        Batch of feature rows.\n    y : pandas.Series\n        Corresponding labels.\n    \"\"\"\n    self._update_observed_features(X)\n    self._update_observed_targets(y)\n    x_t = self._df2tensor(X)\n    if self.loss_fn == \"cross_entropy\":\n        self._classification_step_cross_entropy(x_t, y)\n    else:\n        self._learn(x_t, y)\n</code></pre>"},{"location":"reference/classification/zoo/#deep_river.classification.zoo.MultiLayerPerceptron.learn_one","title":"learn_one","text":"<pre><code>learn_one(x: dict, y: ClfTarget) -&gt; None\n</code></pre> <p>Learn from a single instance.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>dict</code> <p>Feature dictionary.</p> required <code>y</code> <code>hashable</code> <p>Class label.</p> required Source code in <code>deep_river/classification/classifier.py</code> <pre><code>def learn_one(self, x: dict, y: base.typing.ClfTarget) -&gt; None:\n    \"\"\"Learn from a single instance.\n\n    Parameters\n    ----------\n    x : dict\n        Feature dictionary.\n    y : hashable\n        Class label.\n    \"\"\"\n    self._update_observed_features(x)\n    self._update_observed_targets(y)\n    x_t = self._dict2tensor(x)\n    if self.loss_fn == \"cross_entropy\":\n        self._classification_step_cross_entropy(x_t, y)\n    else:\n        # One-hot pathway / other losses\n        self._learn(x_t, y)\n</code></pre>"},{"location":"reference/classification/zoo/#deep_river.classification.zoo.MultiLayerPerceptron.load","title":"load  <code>classmethod</code>","text":"<pre><code>load(filepath: Union[str, Path])\n</code></pre> <p>Load a previously saved estimator.</p> <p>The method reconstructs the estimator class, its wrapped module, optimiser state and runtime information (feature names, buffers, etc.).</p> Source code in <code>deep_river/base.py</code> <pre><code>@classmethod\ndef load(cls, filepath: Union[str, Path]):\n    \"\"\"Load a previously saved estimator.\n\n    The method reconstructs the estimator class, its wrapped module, optimiser\n    state and runtime information (feature names, buffers, etc.).\n    \"\"\"\n    with open(filepath, \"rb\") as f:\n        state = pickle.load(f)\n\n    estimator_cls = cls._import_from_path(state[\"estimator_class\"])\n    init_params = state[\"init_params\"]\n\n    # Rebuild module if needed\n    if \"module\" in init_params and isinstance(init_params[\"module\"], dict):\n        module_info = init_params.pop(\"module\")\n        module_cls = cls._import_from_path(module_info[\"class\"])\n        module = module_cls(\n            **cls._filter_kwargs(module_cls.__init__, module_info[\"kwargs\"])\n        )\n        if state.get(\"model_state_dict\"):\n            module.load_state_dict(state[\"model_state_dict\"])\n        init_params[\"module\"] = module\n\n    estimator = estimator_cls(\n        **cls._filter_kwargs(estimator_cls.__init__, init_params)\n    )\n\n    if state.get(\"optimizer_state_dict\") and hasattr(estimator, \"optimizer\"):\n        try:\n            estimator.optimizer.load_state_dict(\n                state[\"optimizer_state_dict\"]  # type: ignore[arg-type]\n            )\n        except Exception:  # noqa: E722\n            pass\n\n    estimator._restore_runtime_state(state.get(\"runtime_state\", {}))\n    return estimator\n</code></pre>"},{"location":"reference/classification/zoo/#deep_river.classification.zoo.MultiLayerPerceptron.predict_proba_many","title":"predict_proba_many","text":"<pre><code>predict_proba_many(X: DataFrame) -&gt; DataFrame\n</code></pre> <p>Predict probabilities for a batch of instances.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>DataFrame</code> <p>Feature matrix.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>Each row sums to 1 (multi-class) or has two columns for binary.</p> Source code in <code>deep_river/classification/classifier.py</code> <pre><code>def predict_proba_many(self, X: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"Predict probabilities for a batch of instances.\n\n    Parameters\n    ----------\n    X : pandas.DataFrame\n        Feature matrix.\n\n    Returns\n    -------\n    pandas.DataFrame\n        Each row sums to 1 (multi-class) or has two columns for binary.\n    \"\"\"\n    self._update_observed_features(X)\n    x_t = self._df2tensor(X)\n    self.module.eval()\n    with torch.inference_mode():\n        y_preds = self.module(x_t)\n    return pd.DataFrame(\n        output2proba(y_preds, self.observed_classes, self.output_is_logit)\n    )\n</code></pre>"},{"location":"reference/classification/zoo/#deep_river.classification.zoo.MultiLayerPerceptron.predict_proba_one","title":"predict_proba_one","text":"<pre><code>predict_proba_one(x: dict) -&gt; dict[ClfTarget, float]\n</code></pre> <p>Predict class membership probabilities for one instance.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>dict</code> <p>Feature dictionary.</p> required <p>Returns:</p> Type Description <code>dict</code> <p>Mapping from label -&gt; probability.</p> Source code in <code>deep_river/classification/classifier.py</code> <pre><code>def predict_proba_one(self, x: dict) -&gt; dict[base.typing.ClfTarget, float]:\n    \"\"\"Predict class membership probabilities for one instance.\n\n    Parameters\n    ----------\n    x : dict\n        Feature dictionary.\n\n    Returns\n    -------\n    dict\n        Mapping from label -&gt; probability.\n    \"\"\"\n    self._update_observed_features(x)\n    x_t = self._dict2tensor(x)\n    self.module.eval()\n    with torch.inference_mode():\n        y_pred = self.module(x_t)\n    raw = output2proba(y_pred, self.observed_classes, self.output_is_logit)[0]\n    return cast(dict[base.typing.ClfTarget, float], raw)\n</code></pre>"},{"location":"reference/classification/zoo/#deep_river.classification.zoo.MultiLayerPerceptron.save","title":"save","text":"<pre><code>save(filepath: Union[str, Path]) -&gt; None\n</code></pre> <p>Persist the estimator (architecture, weights, optimiser &amp; runtime state).</p> <p>Parameters:</p> Name Type Description Default <code>filepath</code> <code>str | Path</code> <p>Destination file. Parent directories are created automatically.</p> required Source code in <code>deep_river/base.py</code> <pre><code>def save(self, filepath: Union[str, Path]) -&gt; None:\n    \"\"\"Persist the estimator (architecture, weights, optimiser &amp; runtime state).\n\n    Parameters\n    ----------\n    filepath : str | Path\n        Destination file. Parent directories are created automatically.\n    \"\"\"\n    filepath = Path(filepath)\n    filepath.parent.mkdir(parents=True, exist_ok=True)\n\n    state = {\n        \"estimator_class\": f\"{type(self).__module__}.{type(self).__name__}\",\n        \"init_params\": self._get_all_init_params(),\n        \"model_state_dict\": getattr(self.module, \"state_dict\", lambda: {})(),\n        \"optimizer_state_dict\": getattr(self.optimizer, \"state_dict\", lambda: {})(),\n        \"runtime_state\": self._get_runtime_state(),\n    }\n\n    with open(filepath, \"wb\") as f:\n        pickle.dump(state, f)\n</code></pre>"},{"location":"reference/classification/zoo/#deep_river.classification.zoo.RNNClassifier","title":"RNNClassifier","text":"<pre><code>RNNClassifier(\n    n_features: int = 10,\n    hidden_size: int = 16,\n    num_layers: int = 1,\n    nonlinearity: str = \"tanh\",\n    n_init_classes: int = 2,\n    loss_fn: Union[str, Callable] = \"cross_entropy\",\n    optimizer_fn: Union[str, Type[Optimizer]] = \"adam\",\n    lr: float = 0.001,\n    output_is_logit: bool = True,\n    is_feature_incremental: bool = False,\n    is_class_incremental: bool = True,\n    device: str = \"cpu\",\n    seed: int = 42,\n    gradient_clip_value: float | None = None,\n    **kwargs\n)\n</code></pre> <p>               Bases: <code>RollingClassifier</code></p> <p>Rolling RNN classifier with dynamic class expansion.</p> <p>Uses a (stacked) <code>nn.RNN</code> backbone followed by a linear head that produces raw logits. Designed for streaming sequential data via a fixed-size rolling window handled by :class:<code>RollingClassifier</code>.</p> <p>Parameters:</p> Name Type Description Default <code>n_features</code> <code>int</code> <p>Number of input features per timestep.</p> <code>10</code> <code>hidden_size</code> <code>int</code> <p>Hidden state dimensionality of the RNN.</p> <code>16</code> <code>num_layers</code> <code>int</code> <p>Number of stacked RNN layers.</p> <code>1</code> <code>nonlinearity</code> <code>str</code> <p>Non-linearity used inside the RNN ('tanh' or 'relu').</p> <code>'tanh'</code> <code>n_init_classes</code> <code>int</code> <p>Initial number of classes/output units.</p> <code>2</code> <code>loss_fn</code> <code>str | Callable</code> <p>Training loss.</p> <code>'cross_entropy'</code> <code>optimizer_fn</code> <code>str | type</code> <p>Optimizer specification.</p> <code>'sgd'</code> <code>lr</code> <code>float</code> <p>Learning rate.</p> <code>1e-3</code> <code>output_is_logit</code> <code>bool</code> <p>Indicates outputs are logits (enables proper conversion in <code>predict_proba</code>).</p> <code>True</code> <code>is_feature_incremental</code> <code>bool</code> <p>Whether to dynamically expand the input layer when new features appear.</p> <code>False</code> <code>is_class_incremental</code> <code>bool</code> <p>Whether to expand the output layer for new class labels.</p> <code>True</code> <code>device</code> <code>str</code> <p>Torch device.</p> <code>'cpu'</code> <code>seed</code> <code>int</code> <p>Random seed.</p> <code>42</code> <code>gradient_clip_value</code> <code>float | None</code> <p>Optional gradient norm clipping value.</p> <code>None</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import torch, random, numpy as np\n&gt;&gt;&gt; from torch import manual_seed\n&gt;&gt;&gt; from river import metrics\n&gt;&gt;&gt; from river import datasets\n&gt;&gt;&gt; from deep_river.classification.zoo import RNNClassifier\n&gt;&gt;&gt; _ = manual_seed(42); random.seed(42); np.random.seed(42)\n&gt;&gt;&gt; stream = datasets.Phishing()\n&gt;&gt;&gt; samples = {}\n&gt;&gt;&gt; for x, y in stream:\n...     if y not in samples:\n...         samples[y] = x\n...     if len(samples) == 2:\n...         break\n&gt;&gt;&gt; x0, x1 = samples[0], samples[1]\n&gt;&gt;&gt; n_features = len(x0)\n&gt;&gt;&gt; rnn_clf = RNNClassifier(n_features=n_features, hidden_size=3, n_init_classes=2,\n...                         is_class_incremental=False, is_feature_incremental=False)\n&gt;&gt;&gt; acc = metrics.Accuracy()\n&gt;&gt;&gt; for i, (x, y) in enumerate(datasets.Phishing().take(200)):\n...     rnn_clf.learn_one(x, y)\n...     if i &gt; 0:\n...         y_pred = rnn_clf.predict_one(x)\n...         acc.update(y, y_pred)\n&gt;&gt;&gt; assert 0.0 &lt;= acc.get() &lt;= 1.0\n&gt;&gt;&gt; print(f\"Accuracy: {acc.get():.4f}\")  # doctest: +ELLIPSIS\nAccuracy: ...\n</code></pre> <p>Methods:</p> Name Description <code>clone</code> <p>Return a fresh estimator instance with (optionally) copied state.</p> <code>draw</code> <p>Render a (partial) computational graph of the wrapped model.</p> <code>learn_many</code> <p>Batch update: extend window with rows of X and perform a step.</p> <code>learn_one</code> <p>Learn from a single (x, y) updating the rolling window.</p> <code>load</code> <p>Load a previously saved estimator.</p> <code>predict_proba_many</code> <p>Return probability DataFrame for multiple samples with rolling context.</p> <code>predict_proba_one</code> <p>Return class probability mapping for one sample using rolling context.</p> <code>save</code> <p>Persist the estimator (architecture, weights, optimiser &amp; runtime state).</p> Source code in <code>deep_river/classification/zoo.py</code> <pre><code>def __init__(\n    self,\n    n_features: int = 10,\n    hidden_size: int = 16,\n    num_layers: int = 1,\n    nonlinearity: str = \"tanh\",\n    n_init_classes: int = 2,\n    loss_fn: Union[str, Callable] = \"cross_entropy\",\n    optimizer_fn: Union[str, Type[optim.Optimizer]] = \"adam\",\n    lr: float = 1e-3,\n    output_is_logit: bool = True,\n    is_feature_incremental: bool = False,\n    is_class_incremental: bool = True,\n    device: str = \"cpu\",\n    seed: int = 42,\n    gradient_clip_value: float | None = None,\n    **kwargs,\n):\n    self.n_features = n_features\n    self.hidden_size = hidden_size\n    self.num_layers = num_layers\n    self.nonlinearity = nonlinearity\n    self.n_init_classes = n_init_classes\n    module = RNNClassifier.RNNModule(\n        n_features=n_features,\n        hidden_size=hidden_size,\n        num_layers=num_layers,\n        nonlinearity=nonlinearity,\n        n_init_classes=n_init_classes,\n    )\n    if \"module\" in kwargs:\n        del kwargs[\"module\"]\n    super().__init__(\n        module=module,\n        loss_fn=loss_fn,\n        optimizer_fn=optimizer_fn,\n        output_is_logit=output_is_logit,\n        is_feature_incremental=is_feature_incremental,\n        is_class_incremental=is_class_incremental,\n        device=device,\n        lr=lr,\n        seed=seed,\n        gradient_clip_value=gradient_clip_value,\n        **kwargs,\n    )\n</code></pre>"},{"location":"reference/classification/zoo/#deep_river.classification.zoo.RNNClassifier.clone","title":"clone","text":"<pre><code>clone(\n    new_params=None,\n    include_attributes: bool = False,\n    copy_weights: bool = False,\n)\n</code></pre> <p>Return a fresh estimator instance with (optionally) copied state.</p> <p>Parameters:</p> Name Type Description Default <code>new_params</code> <code>dict | None</code> <p>Parameter overrides for the cloned instance.</p> <code>None</code> <code>include_attributes</code> <code>bool</code> <p>If True, runtime state (observed features, buffers) is also copied.</p> <code>False</code> <code>copy_weights</code> <code>bool</code> <p>If True, model weights are copied (otherwise the module is re\u2011initialised).</p> <code>False</code> Source code in <code>deep_river/base.py</code> <pre><code>def clone(\n    self,\n    new_params=None,\n    include_attributes: bool = False,\n    copy_weights: bool = False,\n):\n    \"\"\"Return a fresh estimator instance with (optionally) copied state.\n\n    Parameters\n    ----------\n    new_params : dict | None\n        Parameter overrides for the cloned instance.\n    include_attributes : bool, default=False\n        If True, runtime state (observed features, buffers) is also copied.\n    copy_weights : bool, default=False\n        If True, model weights are copied (otherwise the module is re\u2011initialised).\n    \"\"\"\n    new_params = new_params or {}\n    copy_weights = new_params.pop(\"copy_weights\", copy_weights)\n\n    params = {**self._get_all_init_params(), **new_params}\n\n    if \"module\" not in new_params:\n        params[\"module\"] = self._rebuild_module()\n\n    new_est = self.__class__(**self._filter_kwargs(self.__class__.__init__, params))\n\n    if copy_weights and hasattr(self.module, \"state_dict\"):\n        new_est.module.load_state_dict(self.module.state_dict())\n\n    if include_attributes:\n        new_est._restore_runtime_state(self._get_runtime_state())\n\n    return new_est\n</code></pre>"},{"location":"reference/classification/zoo/#deep_river.classification.zoo.RNNClassifier.draw","title":"draw","text":"<pre><code>draw()\n</code></pre> <p>Render a (partial) computational graph of the wrapped model.</p> <p>Imports <code>graphviz</code> and <code>torchviz</code> lazily. Raises an informative ImportError if the optional dependencies are not installed.</p> Source code in <code>deep_river/base.py</code> <pre><code>def draw(self):  # type: ignore[override]\n    \"\"\"Render a (partial) computational graph of the wrapped model.\n\n    Imports ``graphviz`` and ``torchviz`` lazily. Raises an informative\n    ImportError if the optional dependencies are not installed.\n    \"\"\"\n    try:  # pragma: no cover\n        from torchviz import make_dot  # type: ignore\n    except Exception as err:  # noqa: BLE001\n        raise ImportError(\n            \"graphviz and torchviz must be installed to draw the model.\"\n        ) from err\n\n    first_parameter = next(self.module.parameters())\n    input_shape = first_parameter.size()\n    y_pred = self.module(torch.rand(input_shape))\n    return make_dot(y_pred.mean(), params=dict(self.module.named_parameters()))\n</code></pre>"},{"location":"reference/classification/zoo/#deep_river.classification.zoo.RNNClassifier.learn_many","title":"learn_many","text":"<pre><code>learn_many(X: DataFrame, y: Series) -&gt; None\n</code></pre> <p>Batch update: extend window with rows of X and perform a step.</p> Source code in <code>deep_river/classification/rolling_classifier.py</code> <pre><code>def learn_many(self, X: pd.DataFrame, y: pd.Series) -&gt; None:\n    \"\"\"Batch update: extend window with rows of X and perform a step.\"\"\"\n    self._update_observed_targets(y)\n    self._update_observed_features(X)\n    X = X[list(self.observed_features)]\n    self._x_window.extend(X.values.tolist())\n    X_t = self._deque2rolling_tensor(self._x_window)\n    self._learn(x=X_t, y=y)\n</code></pre>"},{"location":"reference/classification/zoo/#deep_river.classification.zoo.RNNClassifier.learn_one","title":"learn_one","text":"<pre><code>learn_one(x: dict, y: ClfTarget, **kwargs) -&gt; None\n</code></pre> <p>Learn from a single (x, y) updating the rolling window.</p> Source code in <code>deep_river/classification/rolling_classifier.py</code> <pre><code>def learn_one(self, x: dict, y: ClfTarget, **kwargs) -&gt; None:\n    \"\"\"Learn from a single (x, y) updating the rolling window.\"\"\"\n    self._update_observed_features(x)\n    self._update_observed_targets(y)\n    self._x_window.append([x.get(feature, 0) for feature in self.observed_features])\n    x_t = self._deque2rolling_tensor(self._x_window)\n    self._learn(x=x_t, y=y)\n</code></pre>"},{"location":"reference/classification/zoo/#deep_river.classification.zoo.RNNClassifier.load","title":"load  <code>classmethod</code>","text":"<pre><code>load(filepath: Union[str, Path])\n</code></pre> <p>Load a previously saved estimator.</p> <p>The method reconstructs the estimator class, its wrapped module, optimiser state and runtime information (feature names, buffers, etc.).</p> Source code in <code>deep_river/base.py</code> <pre><code>@classmethod\ndef load(cls, filepath: Union[str, Path]):\n    \"\"\"Load a previously saved estimator.\n\n    The method reconstructs the estimator class, its wrapped module, optimiser\n    state and runtime information (feature names, buffers, etc.).\n    \"\"\"\n    with open(filepath, \"rb\") as f:\n        state = pickle.load(f)\n\n    estimator_cls = cls._import_from_path(state[\"estimator_class\"])\n    init_params = state[\"init_params\"]\n\n    # Rebuild module if needed\n    if \"module\" in init_params and isinstance(init_params[\"module\"], dict):\n        module_info = init_params.pop(\"module\")\n        module_cls = cls._import_from_path(module_info[\"class\"])\n        module = module_cls(\n            **cls._filter_kwargs(module_cls.__init__, module_info[\"kwargs\"])\n        )\n        if state.get(\"model_state_dict\"):\n            module.load_state_dict(state[\"model_state_dict\"])\n        init_params[\"module\"] = module\n\n    estimator = estimator_cls(\n        **cls._filter_kwargs(estimator_cls.__init__, init_params)\n    )\n\n    if state.get(\"optimizer_state_dict\") and hasattr(estimator, \"optimizer\"):\n        try:\n            estimator.optimizer.load_state_dict(\n                state[\"optimizer_state_dict\"]  # type: ignore[arg-type]\n            )\n        except Exception:  # noqa: E722\n            pass\n\n    estimator._restore_runtime_state(state.get(\"runtime_state\", {}))\n    return estimator\n</code></pre>"},{"location":"reference/classification/zoo/#deep_river.classification.zoo.RNNClassifier.predict_proba_many","title":"predict_proba_many","text":"<pre><code>predict_proba_many(X: DataFrame) -&gt; DataFrame\n</code></pre> <p>Return probability DataFrame for multiple samples with rolling context.</p> Source code in <code>deep_river/classification/rolling_classifier.py</code> <pre><code>def predict_proba_many(self, X: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"Return probability DataFrame for multiple samples with rolling context.\"\"\"\n    self._update_observed_features(X)\n    X = X[list(self.observed_features)]\n    x_win = self._x_window.copy()\n    x_win.extend(X.values.tolist())\n    if self.append_predict:\n        self._x_window = x_win\n    self.module.eval()\n    with torch.inference_mode():\n        x_t = self._deque2rolling_tensor(x_win)\n        probas = self.module(x_t).detach().tolist()\n    return pd.DataFrame(probas)\n</code></pre>"},{"location":"reference/classification/zoo/#deep_river.classification.zoo.RNNClassifier.predict_proba_one","title":"predict_proba_one","text":"<pre><code>predict_proba_one(x: dict) -&gt; Dict[ClfTarget, float]\n</code></pre> <p>Return class probability mapping for one sample using rolling context.</p> Source code in <code>deep_river/classification/rolling_classifier.py</code> <pre><code>def predict_proba_one(self, x: dict) -&gt; Dict[ClfTarget, float]:\n    \"\"\"Return class probability mapping for one sample using rolling context.\"\"\"\n    self._update_observed_features(x)\n    x_win = self._x_window.copy()\n    x_win.append([x.get(feature, 0) for feature in self.observed_features])\n    if self.append_predict:\n        self._x_window = x_win\n    self.module.eval()\n    with torch.inference_mode():\n        x_t = self._deque2rolling_tensor(x_win)\n        y_pred = self.module(x_t)\n        proba = output2proba(y_pred, self.observed_classes, self.output_is_logit)\n    return cast(Dict[ClfTarget, float], proba[0])\n</code></pre>"},{"location":"reference/classification/zoo/#deep_river.classification.zoo.RNNClassifier.save","title":"save","text":"<pre><code>save(filepath: Union[str, Path]) -&gt; None\n</code></pre> <p>Persist the estimator (architecture, weights, optimiser &amp; runtime state).</p> <p>Parameters:</p> Name Type Description Default <code>filepath</code> <code>str | Path</code> <p>Destination file. Parent directories are created automatically.</p> required Source code in <code>deep_river/base.py</code> <pre><code>def save(self, filepath: Union[str, Path]) -&gt; None:\n    \"\"\"Persist the estimator (architecture, weights, optimiser &amp; runtime state).\n\n    Parameters\n    ----------\n    filepath : str | Path\n        Destination file. Parent directories are created automatically.\n    \"\"\"\n    filepath = Path(filepath)\n    filepath.parent.mkdir(parents=True, exist_ok=True)\n\n    state = {\n        \"estimator_class\": f\"{type(self).__module__}.{type(self).__name__}\",\n        \"init_params\": self._get_all_init_params(),\n        \"model_state_dict\": getattr(self.module, \"state_dict\", lambda: {})(),\n        \"optimizer_state_dict\": getattr(self.optimizer, \"state_dict\", lambda: {})(),\n        \"runtime_state\": self._get_runtime_state(),\n    }\n\n    with open(filepath, \"wb\") as f:\n        pickle.dump(state, f)\n</code></pre>"},{"location":"reference/regression/multioutput/","title":"multioutput","text":""},{"location":"reference/regression/multioutput/#deep_river.regression.multioutput","title":"multioutput","text":"<p>Classes:</p> Name Description <code>MultiTargetRegressor</code> <p>Incremental multi-target regression wrapper for PyTorch modules.</p>"},{"location":"reference/regression/multioutput/#deep_river.regression.multioutput.MultiTargetRegressor","title":"MultiTargetRegressor","text":"<pre><code>MultiTargetRegressor(\n    module: Module,\n    loss_fn: Union[str, Callable] = \"mse\",\n    optimizer_fn: Union[str, Callable] = \"sgd\",\n    is_feature_incremental: bool = False,\n    is_target_incremental: bool = False,\n    lr: float = 0.001,\n    device: str = \"cpu\",\n    seed: int = 42,\n    **kwargs\n)\n</code></pre> <p>               Bases: <code>MultiTargetRegressor</code>, <code>DeepEstimator</code></p> <p>Incremental multi-target regression wrapper for PyTorch modules.</p> <p>This estimator adapts a <code>torch.nn.Module</code> to the :mod:<code>river</code> streaming API for multi\u2011target (a.k.a. multi\u2011output) regression. It optionally supports feature\u2011incremental learning (dynamic growth of the input layer when new feature names appear) as provided by :class:<code>deep_river.base.DeepEstimator</code> and additionally (optionally) target\u2011incremental learning: if new target names appear during the stream, the output layer can be expanded on\u2011the\u2011fly so the model natively handles the enlarged target vector.</p> <p>Targets are tracked via an ordered :class:<code>~sortedcontainers.SortedSet</code> to guarantee deterministic ordering between training and prediction. Incoming target dictionaries / frames are converted into dense tensors with columns arranged according to the observed target name order. Missing targets (when the model has been expanded but a prior sample omits some target) are imputed with <code>0.0</code>.</p> <p>Parameters:</p> Name Type Description Default <code>module</code> <code>Module</code> <p>PyTorch module producing an output tensor of shape <code>(N, T)</code> where <code>T</code> is the current number of target variables.</p> required <code>loss_fn</code> <code>str | Callable</code> <p>Loss identifier or custom callable passed through :func:<code>deep_river.utils.get_loss_fn</code>.</p> <code>'mse'</code> <code>optimizer_fn</code> <code>str | Callable</code> <p>Optimizer identifier (e.g. <code>'adam'</code>, <code>'sgd'</code>) or factory / class.</p> <code>'sgd'</code> <code>is_feature_incremental</code> <code>bool</code> <p>If True, unseen feature names trigger expansion of the first trainable layer (see :class:<code>DeepEstimator</code>).</p> <code>False</code> <code>is_target_incremental</code> <code>bool</code> <p>If True, unseen target names trigger expansion of the last trainable layer. Expansion preserves existing weights and initialises new units with small random values.</p> <code>False</code> <code>lr</code> <code>float</code> <p>Learning rate.</p> <code>1e-3</code> <code>device</code> <code>str</code> <p>Torch device (e.g. <code>'cuda'</code>).</p> <code>'cpu'</code> <code>seed</code> <code>int</code> <p>Random seed for reproducibility.</p> <code>42</code> <code>**kwargs</code> <p>Extra arguments stored for persistence / cloning.</p> <code>{}</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from torch import nn\n&gt;&gt;&gt; from deep_river.regression.multioutput import MultiTargetRegressor\n&gt;&gt;&gt; class TinyMultiNet(nn.Module):\n...     def __init__(self, n_features, n_outputs):\n...         super().__init__()\n...         self.net = nn.Sequential(\n...             nn.Linear(n_features, 8),\n...             nn.ReLU(),\n...             nn.Linear(8, n_outputs)\n...         )\n...     def forward(self, x):\n...         return self.net(x)\n&gt;&gt;&gt; model = MultiTargetRegressor(\n...     module=TinyMultiNet(3, 2),\n...     loss_fn='mse',\n...     optimizer_fn='sgd',\n...     is_feature_incremental=True,\n...     is_target_incremental=True,\n... )\n&gt;&gt;&gt; x = {'a': 1.0, 'b': 2.0, 'c': 3.0}\n&gt;&gt;&gt; y = {'y1': 10.0, 'y2': 20.0}\n&gt;&gt;&gt; _ = model.learn_one(x, y)\n&gt;&gt;&gt; model.predict_one(x)\n{'y1': ..., 'y2': ...}\n</code></pre> Notes <ul> <li>The module's last trainable leaf layer is treated as output layer for</li> <li>If <code>is_target_incremental</code> is disabled, the number of outputs is fixed   and encountering a new target name will only register it internally (the   tensor conversion will still allocate a slot, but the model's output layer   size will not change, possibly causing a mismatch). Therefore, enabling   target incrementality is recommended for truly open\u2011world streams.</li> </ul> <p>Methods:</p> Name Description <code>clone</code> <p>Return a fresh estimator instance with (optionally) copied state.</p> <code>draw</code> <p>Render a (partial) computational graph of the wrapped model.</p> <code>learn_many</code> <p>Learn from a batch of multi-target instances.</p> <code>learn_one</code> <p>Learn from a single multi-target instance.</p> <code>load</code> <p>Load a previously saved estimator.</p> <code>predict_many</code> <p>Predict target values for multiple instances.</p> <code>predict_one</code> <p>Predict a dictionary of target values for a single instance.</p> <code>save</code> <p>Persist the estimator (architecture, weights, optimiser &amp; runtime state).</p> Source code in <code>deep_river/regression/multioutput.py</code> <pre><code>def __init__(\n    self,\n    module: torch.nn.Module,\n    loss_fn: Union[str, Callable] = \"mse\",\n    optimizer_fn: Union[str, Callable] = \"sgd\",\n    is_feature_incremental: bool = False,\n    is_target_incremental: bool = False,\n    lr: float = 1e-3,\n    device: str = \"cpu\",\n    seed: int = 42,\n    **kwargs,\n):\n    super().__init__(\n        module=module,\n        loss_fn=loss_fn,\n        optimizer_fn=optimizer_fn,\n        lr=lr,\n        device=device,\n        seed=seed,\n        is_feature_incremental=is_feature_incremental,\n        **kwargs,\n    )\n    self.is_target_incremental = is_target_incremental\n    self.observed_targets: SortedSet[FeatureName] = SortedSet()\n</code></pre>"},{"location":"reference/regression/multioutput/#deep_river.regression.multioutput.MultiTargetRegressor.clone","title":"clone","text":"<pre><code>clone(\n    new_params=None,\n    include_attributes: bool = False,\n    copy_weights: bool = False,\n)\n</code></pre> <p>Return a fresh estimator instance with (optionally) copied state.</p> <p>Parameters:</p> Name Type Description Default <code>new_params</code> <code>dict | None</code> <p>Parameter overrides for the cloned instance.</p> <code>None</code> <code>include_attributes</code> <code>bool</code> <p>If True, runtime state (observed features, buffers) is also copied.</p> <code>False</code> <code>copy_weights</code> <code>bool</code> <p>If True, model weights are copied (otherwise the module is re\u2011initialised).</p> <code>False</code> Source code in <code>deep_river/base.py</code> <pre><code>def clone(\n    self,\n    new_params=None,\n    include_attributes: bool = False,\n    copy_weights: bool = False,\n):\n    \"\"\"Return a fresh estimator instance with (optionally) copied state.\n\n    Parameters\n    ----------\n    new_params : dict | None\n        Parameter overrides for the cloned instance.\n    include_attributes : bool, default=False\n        If True, runtime state (observed features, buffers) is also copied.\n    copy_weights : bool, default=False\n        If True, model weights are copied (otherwise the module is re\u2011initialised).\n    \"\"\"\n    new_params = new_params or {}\n    copy_weights = new_params.pop(\"copy_weights\", copy_weights)\n\n    params = {**self._get_all_init_params(), **new_params}\n\n    if \"module\" not in new_params:\n        params[\"module\"] = self._rebuild_module()\n\n    new_est = self.__class__(**self._filter_kwargs(self.__class__.__init__, params))\n\n    if copy_weights and hasattr(self.module, \"state_dict\"):\n        new_est.module.load_state_dict(self.module.state_dict())\n\n    if include_attributes:\n        new_est._restore_runtime_state(self._get_runtime_state())\n\n    return new_est\n</code></pre>"},{"location":"reference/regression/multioutput/#deep_river.regression.multioutput.MultiTargetRegressor.draw","title":"draw","text":"<pre><code>draw()\n</code></pre> <p>Render a (partial) computational graph of the wrapped model.</p> <p>Imports <code>graphviz</code> and <code>torchviz</code> lazily. Raises an informative ImportError if the optional dependencies are not installed.</p> Source code in <code>deep_river/base.py</code> <pre><code>def draw(self):  # type: ignore[override]\n    \"\"\"Render a (partial) computational graph of the wrapped model.\n\n    Imports ``graphviz`` and ``torchviz`` lazily. Raises an informative\n    ImportError if the optional dependencies are not installed.\n    \"\"\"\n    try:  # pragma: no cover\n        from torchviz import make_dot  # type: ignore\n    except Exception as err:  # noqa: BLE001\n        raise ImportError(\n            \"graphviz and torchviz must be installed to draw the model.\"\n        ) from err\n\n    first_parameter = next(self.module.parameters())\n    input_shape = first_parameter.size()\n    y_pred = self.module(torch.rand(input_shape))\n    return make_dot(y_pred.mean(), params=dict(self.module.named_parameters()))\n</code></pre>"},{"location":"reference/regression/multioutput/#deep_river.regression.multioutput.MultiTargetRegressor.learn_many","title":"learn_many","text":"<pre><code>learn_many(\n    X: DataFrame,\n    y: Union[\n        DataFrame, Series, Mapping[str, Sequence[RegTarget]]\n    ],\n) -&gt; None\n</code></pre> <p>Learn from a batch of multi-target instances.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>DataFrame</code> <p>Feature matrix (rows are samples, columns are feature names).</p> required <code>y</code> <code>DataFrame | Series | mapping</code> <p>Target matrix. Preferred is a DataFrame with one column per target. A Series is interpreted as one target. A mapping of <code>name -&gt; list</code> is converted into a DataFrame first.</p> required Source code in <code>deep_river/regression/multioutput.py</code> <pre><code>def learn_many(\n    self,\n    X: pd.DataFrame,\n    y: Union[pd.DataFrame, pd.Series, Mapping[str, Sequence[RegTarget]]],\n) -&gt; None:\n    \"\"\"Learn from a batch of multi-target instances.\n\n    Parameters\n    ----------\n    X : pandas.DataFrame\n        Feature matrix (rows are samples, columns are feature names).\n    y : pandas.DataFrame | pandas.Series | mapping\n        Target matrix. Preferred is a DataFrame with one column per target.\n        A Series is interpreted as *one* target. A mapping of ``name -&gt; list``\n        is converted into a DataFrame first.\n    \"\"\"\n    self._update_observed_features(X)\n    y_df = self._coerce_targets_to_frame(y)\n    self._update_observed_targets(y_df)\n\n    x_t = self._df2tensor(X)\n    y_t = self._multi_target_frame_to_tensor(y_df)\n    self._learn(x_t, y_t)\n</code></pre>"},{"location":"reference/regression/multioutput/#deep_river.regression.multioutput.MultiTargetRegressor.learn_one","title":"learn_one","text":"<pre><code>learn_one(\n    x: dict, y: dict[FeatureName, RegTarget], **kwargs\n) -&gt; None\n</code></pre> <p>Learn from a single multi-target instance.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>dict[str, float]</code> <p>Feature mapping.</p> required <code>y</code> <code>dict[str, float]</code> <p>Mapping of target name -&gt; target value.</p> required <code>**kwargs</code> <p>Ignored (kept for signature compatibility / future hooks).</p> <code>{}</code> Source code in <code>deep_river/regression/multioutput.py</code> <pre><code>def learn_one(\n    self,\n    x: dict,\n    y: dict[FeatureName, RegTarget],\n    **kwargs,\n) -&gt; None:\n    \"\"\"Learn from a single multi-target instance.\n\n    Parameters\n    ----------\n    x : dict[str, float]\n        Feature mapping.\n    y : dict[str, float]\n        Mapping of target name -&gt; target value.\n    **kwargs\n        Ignored (kept for signature compatibility / future hooks).\n    \"\"\"\n    self._update_observed_features(x)\n    self._update_observed_targets(y)\n    x_t = self._dict2tensor(dict(x))\n    y_t = self._single_target_dict_to_tensor(y)\n    self._learn(x_t, y_t)\n</code></pre>"},{"location":"reference/regression/multioutput/#deep_river.regression.multioutput.MultiTargetRegressor.load","title":"load  <code>classmethod</code>","text":"<pre><code>load(filepath: Union[str, Path])\n</code></pre> <p>Load a previously saved estimator.</p> <p>The method reconstructs the estimator class, its wrapped module, optimiser state and runtime information (feature names, buffers, etc.).</p> Source code in <code>deep_river/base.py</code> <pre><code>@classmethod\ndef load(cls, filepath: Union[str, Path]):\n    \"\"\"Load a previously saved estimator.\n\n    The method reconstructs the estimator class, its wrapped module, optimiser\n    state and runtime information (feature names, buffers, etc.).\n    \"\"\"\n    with open(filepath, \"rb\") as f:\n        state = pickle.load(f)\n\n    estimator_cls = cls._import_from_path(state[\"estimator_class\"])\n    init_params = state[\"init_params\"]\n\n    # Rebuild module if needed\n    if \"module\" in init_params and isinstance(init_params[\"module\"], dict):\n        module_info = init_params.pop(\"module\")\n        module_cls = cls._import_from_path(module_info[\"class\"])\n        module = module_cls(\n            **cls._filter_kwargs(module_cls.__init__, module_info[\"kwargs\"])\n        )\n        if state.get(\"model_state_dict\"):\n            module.load_state_dict(state[\"model_state_dict\"])\n        init_params[\"module\"] = module\n\n    estimator = estimator_cls(\n        **cls._filter_kwargs(estimator_cls.__init__, init_params)\n    )\n\n    if state.get(\"optimizer_state_dict\") and hasattr(estimator, \"optimizer\"):\n        try:\n            estimator.optimizer.load_state_dict(\n                state[\"optimizer_state_dict\"]  # type: ignore[arg-type]\n            )\n        except Exception:  # noqa: E722\n            pass\n\n    estimator._restore_runtime_state(state.get(\"runtime_state\", {}))\n    return estimator\n</code></pre>"},{"location":"reference/regression/multioutput/#deep_river.regression.multioutput.MultiTargetRegressor.predict_many","title":"predict_many","text":"<pre><code>predict_many(X: DataFrame) -&gt; DataFrame\n</code></pre> <p>Predict target values for multiple instances.</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>DataFrame whose columns follow the ordering of <code>observed_targets</code>.</p> Source code in <code>deep_river/regression/multioutput.py</code> <pre><code>def predict_many(self, X: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"Predict target values for multiple instances.\n\n    Returns\n    -------\n    pandas.DataFrame\n        DataFrame whose columns follow the ordering of ``observed_targets``.\n    \"\"\"\n    self._update_observed_features(X)\n    x_t = self._df2tensor(X)\n    self.module.eval()\n    with torch.inference_mode():\n        y_pred = self.module(x_t)\n        if y_pred.is_cuda:\n            y_pred = y_pred.cpu()\n    # Ensure 2D\n    if y_pred.dim() == 1:\n        y_pred = y_pred.view(-1, 1)\n    cols = list(self.observed_targets)\n    # Truncate or pad columns if dimensions drift (defensive)\n    if y_pred.shape[1] &lt; len(cols):\n        pad = torch.zeros(\n            (y_pred.shape[0], len(cols) - y_pred.shape[1]),\n            dtype=y_pred.dtype,\n        )\n        y_pred = torch.cat([y_pred, pad], dim=1)\n    elif y_pred.shape[1] &gt; len(cols):\n        extra = [f\"__extra_{i}\" for i in range(y_pred.shape[1] - len(cols))]\n        cols = cols + extra\n    return pd.DataFrame(y_pred.numpy(), columns=cols)\n</code></pre>"},{"location":"reference/regression/multioutput/#deep_river.regression.multioutput.MultiTargetRegressor.predict_one","title":"predict_one","text":"<pre><code>predict_one(x: dict) -&gt; dict[FeatureName, RegTarget]\n</code></pre> <p>Predict a dictionary of target values for a single instance.</p> Source code in <code>deep_river/regression/multioutput.py</code> <pre><code>def predict_one(self, x: dict) -&gt; dict[FeatureName, RegTarget]:\n    \"\"\"Predict a dictionary of target values for a single instance.\"\"\"\n    self._update_observed_features(x)\n    x_t = self._dict2tensor(dict(x))\n    self.module.eval()\n    with torch.inference_mode():\n        y_pred_t = self.module(x_t).squeeze(0)\n        if y_pred_t.dim() == 0:  # single value fallback\n            y_pred_t = y_pred_t.view(1)\n        if y_pred_t.is_cuda:\n            y_pred_t = y_pred_t.cpu()\n        y_list: list[float] = [float(v) for v in y_pred_t.tolist()]\n    return {\n        cast(FeatureName, t): cast(\n            RegTarget, (y_list[i] if i &lt; len(y_list) else float(\"nan\"))\n        )\n        for i, t in enumerate(self.observed_targets)\n    }\n</code></pre>"},{"location":"reference/regression/multioutput/#deep_river.regression.multioutput.MultiTargetRegressor.save","title":"save","text":"<pre><code>save(filepath: Union[str, Path]) -&gt; None\n</code></pre> <p>Persist the estimator (architecture, weights, optimiser &amp; runtime state).</p> <p>Parameters:</p> Name Type Description Default <code>filepath</code> <code>str | Path</code> <p>Destination file. Parent directories are created automatically.</p> required Source code in <code>deep_river/base.py</code> <pre><code>def save(self, filepath: Union[str, Path]) -&gt; None:\n    \"\"\"Persist the estimator (architecture, weights, optimiser &amp; runtime state).\n\n    Parameters\n    ----------\n    filepath : str | Path\n        Destination file. Parent directories are created automatically.\n    \"\"\"\n    filepath = Path(filepath)\n    filepath.parent.mkdir(parents=True, exist_ok=True)\n\n    state = {\n        \"estimator_class\": f\"{type(self).__module__}.{type(self).__name__}\",\n        \"init_params\": self._get_all_init_params(),\n        \"model_state_dict\": getattr(self.module, \"state_dict\", lambda: {})(),\n        \"optimizer_state_dict\": getattr(self.optimizer, \"state_dict\", lambda: {})(),\n        \"runtime_state\": self._get_runtime_state(),\n    }\n\n    with open(filepath, \"wb\") as f:\n        pickle.dump(state, f)\n</code></pre>"},{"location":"reference/regression/regressor/","title":"regressor","text":""},{"location":"reference/regression/regressor/#deep_river.regression.regressor","title":"regressor","text":"<p>Classes:</p> Name Description <code>Regressor</code> <p>Incremental wrapper for PyTorch regression models.</p>"},{"location":"reference/regression/regressor/#deep_river.regression.regressor.Regressor","title":"Regressor","text":"<pre><code>Regressor(\n    module: Module,\n    loss_fn: Union[str, Callable],\n    optimizer_fn: Union[str, Type[Optimizer]],\n    lr: float = 0.001,\n    is_feature_incremental: bool = False,\n    device: str = \"cpu\",\n    seed: int = 42,\n    **kwargs\n)\n</code></pre> <p>               Bases: <code>DeepEstimator</code>, <code>MiniBatchRegressor</code></p> <p>Incremental wrapper for PyTorch regression models.</p> <p>Provides feature-incremental learning (optional) by expanding the first trainable layer on-the-fly when unseen feature names are encountered. Suitable for streaming / online regression tasks using the :mod:<code>river</code> API.</p> <p>Parameters:</p> Name Type Description Default <code>module</code> <code>Module</code> <p>PyTorch module that outputs a numeric prediction (shape (N, 1) or (N,)).</p> required <code>loss_fn</code> <code>str | Callable</code> <p>Loss identifier or callable (e.g. <code>'mse'</code>).</p> required <code>optimizer_fn</code> <code>str | Type[Optimizer]</code> <p>Optimizer spec (<code>'adam'</code>, <code>'sgd'</code> or optimizer class).</p> required <code>lr</code> <code>float</code> <p>Learning rate.</p> <code>1e-3</code> <code>is_feature_incremental</code> <code>bool</code> <p>If True, expands the input layer for new feature names.</p> <code>False</code> <code>device</code> <code>str</code> <p>Torch device.</p> <code>'cpu'</code> <code>seed</code> <code>int</code> <p>Random seed for reproducibility.</p> <code>42</code> <code>**kwargs</code> <p>Extra args stored for cloning/persistence.</p> <code>{}</code> <p>Examples:</p> <pre><code>Real-world streaming regression on the Bikes dataset from :mod:`river`.\nWe retain only numeric features (discarding timestamps/strings) to build\ndense tensors. We maintain an online MAE; the exact value may vary depending\non library version and hardware.\n</code></pre> <pre><code>&gt;&gt;&gt; import random, numpy as np\n&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from torch import nn, manual_seed\n&gt;&gt;&gt; from river import datasets, metrics\n&gt;&gt;&gt; from deep_river.regression import Regressor\n&gt;&gt;&gt; _ = manual_seed(42); random.seed(42); np.random.seed(42)\n&gt;&gt;&gt; first_x, _ = next(iter(datasets.Bikes()))\n&gt;&gt;&gt; numeric_keys = sorted([k for k, v in first_x.items() if isinstance(v, (int, float))])\n&gt;&gt;&gt; class SmallNet(nn.Module):\n...     def __init__(self, n_features):\n...         super().__init__()\n...         self.net = nn.Sequential(\n...             nn.Linear(n_features, 8),\n...             nn.ReLU(),\n...             nn.Linear(8, 1)\n...         )\n...     def forward(self, x):\n...         return self.net(x)\n&gt;&gt;&gt; model = Regressor(module=SmallNet(len(numeric_keys)), loss_fn='mse',\n...                     optimizer_fn='sgd', lr=1e-2)\n&gt;&gt;&gt; mae = metrics.MAE()\n&gt;&gt;&gt; for i, (x, y) in enumerate(datasets.Bikes().take(200)):\n...     x_num = {k: x[k] for k in numeric_keys}\n...     y_pred = model.predict_one(x_num)\n...     model.learn_one(x_num, y)\n...     mae.update(y, y_pred)\n&gt;&gt;&gt; print(f\"MAE: {mae.get():.4f}\")\nMAE: ...\n</code></pre> <p>Methods:</p> Name Description <code>clone</code> <p>Return a fresh estimator instance with (optionally) copied state.</p> <code>draw</code> <p>Render a (partial) computational graph of the wrapped model.</p> <code>load</code> <p>Load a previously saved estimator.</p> <code>predict_many</code> <p>Predict target values for multiple instances (returns single-column DataFrame).</p> <code>predict_one</code> <p>Predict target value for a single instance.</p> <code>save</code> <p>Persist the estimator (architecture, weights, optimiser &amp; runtime state).</p> Source code in <code>deep_river/regression/regressor.py</code> <pre><code>def __init__(\n    self,\n    module: nn.Module,\n    loss_fn: Union[str, Callable],\n    optimizer_fn: Union[str, Type[optim.Optimizer]],\n    lr: float = 0.001,\n    is_feature_incremental: bool = False,\n    device: str = \"cpu\",\n    seed: int = 42,\n    **kwargs,\n):\n    super().__init__(\n        module=module,\n        loss_fn=loss_fn,\n        optimizer_fn=optimizer_fn,\n        device=device,\n        lr=lr,\n        is_feature_incremental=is_feature_incremental,\n        seed=seed,\n        **kwargs,\n    )\n</code></pre>"},{"location":"reference/regression/regressor/#deep_river.regression.regressor.Regressor.clone","title":"clone","text":"<pre><code>clone(\n    new_params=None,\n    include_attributes: bool = False,\n    copy_weights: bool = False,\n)\n</code></pre> <p>Return a fresh estimator instance with (optionally) copied state.</p> <p>Parameters:</p> Name Type Description Default <code>new_params</code> <code>dict | None</code> <p>Parameter overrides for the cloned instance.</p> <code>None</code> <code>include_attributes</code> <code>bool</code> <p>If True, runtime state (observed features, buffers) is also copied.</p> <code>False</code> <code>copy_weights</code> <code>bool</code> <p>If True, model weights are copied (otherwise the module is re\u2011initialised).</p> <code>False</code> Source code in <code>deep_river/base.py</code> <pre><code>def clone(\n    self,\n    new_params=None,\n    include_attributes: bool = False,\n    copy_weights: bool = False,\n):\n    \"\"\"Return a fresh estimator instance with (optionally) copied state.\n\n    Parameters\n    ----------\n    new_params : dict | None\n        Parameter overrides for the cloned instance.\n    include_attributes : bool, default=False\n        If True, runtime state (observed features, buffers) is also copied.\n    copy_weights : bool, default=False\n        If True, model weights are copied (otherwise the module is re\u2011initialised).\n    \"\"\"\n    new_params = new_params or {}\n    copy_weights = new_params.pop(\"copy_weights\", copy_weights)\n\n    params = {**self._get_all_init_params(), **new_params}\n\n    if \"module\" not in new_params:\n        params[\"module\"] = self._rebuild_module()\n\n    new_est = self.__class__(**self._filter_kwargs(self.__class__.__init__, params))\n\n    if copy_weights and hasattr(self.module, \"state_dict\"):\n        new_est.module.load_state_dict(self.module.state_dict())\n\n    if include_attributes:\n        new_est._restore_runtime_state(self._get_runtime_state())\n\n    return new_est\n</code></pre>"},{"location":"reference/regression/regressor/#deep_river.regression.regressor.Regressor.draw","title":"draw","text":"<pre><code>draw()\n</code></pre> <p>Render a (partial) computational graph of the wrapped model.</p> <p>Imports <code>graphviz</code> and <code>torchviz</code> lazily. Raises an informative ImportError if the optional dependencies are not installed.</p> Source code in <code>deep_river/base.py</code> <pre><code>def draw(self):  # type: ignore[override]\n    \"\"\"Render a (partial) computational graph of the wrapped model.\n\n    Imports ``graphviz`` and ``torchviz`` lazily. Raises an informative\n    ImportError if the optional dependencies are not installed.\n    \"\"\"\n    try:  # pragma: no cover\n        from torchviz import make_dot  # type: ignore\n    except Exception as err:  # noqa: BLE001\n        raise ImportError(\n            \"graphviz and torchviz must be installed to draw the model.\"\n        ) from err\n\n    first_parameter = next(self.module.parameters())\n    input_shape = first_parameter.size()\n    y_pred = self.module(torch.rand(input_shape))\n    return make_dot(y_pred.mean(), params=dict(self.module.named_parameters()))\n</code></pre>"},{"location":"reference/regression/regressor/#deep_river.regression.regressor.Regressor.load","title":"load  <code>classmethod</code>","text":"<pre><code>load(filepath: Union[str, Path])\n</code></pre> <p>Load a previously saved estimator.</p> <p>The method reconstructs the estimator class, its wrapped module, optimiser state and runtime information (feature names, buffers, etc.).</p> Source code in <code>deep_river/base.py</code> <pre><code>@classmethod\ndef load(cls, filepath: Union[str, Path]):\n    \"\"\"Load a previously saved estimator.\n\n    The method reconstructs the estimator class, its wrapped module, optimiser\n    state and runtime information (feature names, buffers, etc.).\n    \"\"\"\n    with open(filepath, \"rb\") as f:\n        state = pickle.load(f)\n\n    estimator_cls = cls._import_from_path(state[\"estimator_class\"])\n    init_params = state[\"init_params\"]\n\n    # Rebuild module if needed\n    if \"module\" in init_params and isinstance(init_params[\"module\"], dict):\n        module_info = init_params.pop(\"module\")\n        module_cls = cls._import_from_path(module_info[\"class\"])\n        module = module_cls(\n            **cls._filter_kwargs(module_cls.__init__, module_info[\"kwargs\"])\n        )\n        if state.get(\"model_state_dict\"):\n            module.load_state_dict(state[\"model_state_dict\"])\n        init_params[\"module\"] = module\n\n    estimator = estimator_cls(\n        **cls._filter_kwargs(estimator_cls.__init__, init_params)\n    )\n\n    if state.get(\"optimizer_state_dict\") and hasattr(estimator, \"optimizer\"):\n        try:\n            estimator.optimizer.load_state_dict(\n                state[\"optimizer_state_dict\"]  # type: ignore[arg-type]\n            )\n        except Exception:  # noqa: E722\n            pass\n\n    estimator._restore_runtime_state(state.get(\"runtime_state\", {}))\n    return estimator\n</code></pre>"},{"location":"reference/regression/regressor/#deep_river.regression.regressor.Regressor.predict_many","title":"predict_many","text":"<pre><code>predict_many(X: DataFrame) -&gt; DataFrame\n</code></pre> <p>Predict target values for multiple instances (returns single-column DataFrame).</p> Source code in <code>deep_river/regression/regressor.py</code> <pre><code>def predict_many(self, X: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"Predict target values for multiple instances (returns single-column DataFrame).\"\"\"\n    self._update_observed_features(X)\n    x_t = self._df2tensor(X)\n    self.module.eval()\n    with torch.inference_mode():\n        y_preds = self.module(x_t)\n    return pd.DataFrame(y_preds if not y_preds.is_cuda else y_preds.cpu().numpy())\n</code></pre>"},{"location":"reference/regression/regressor/#deep_river.regression.regressor.Regressor.predict_one","title":"predict_one","text":"<pre><code>predict_one(x: dict) -&gt; RegTarget\n</code></pre> <p>Predict target value for a single instance.</p> Source code in <code>deep_river/regression/regressor.py</code> <pre><code>def predict_one(self, x: dict) -&gt; RegTarget:\n    \"\"\"Predict target value for a single instance.\"\"\"\n    self._update_observed_features(x)\n    x_t = self._dict2tensor(x)\n    self.module.eval()\n    with torch.inference_mode():\n        y_pred = self.module(x_t).item()\n    return y_pred\n</code></pre>"},{"location":"reference/regression/regressor/#deep_river.regression.regressor.Regressor.save","title":"save","text":"<pre><code>save(filepath: Union[str, Path]) -&gt; None\n</code></pre> <p>Persist the estimator (architecture, weights, optimiser &amp; runtime state).</p> <p>Parameters:</p> Name Type Description Default <code>filepath</code> <code>str | Path</code> <p>Destination file. Parent directories are created automatically.</p> required Source code in <code>deep_river/base.py</code> <pre><code>def save(self, filepath: Union[str, Path]) -&gt; None:\n    \"\"\"Persist the estimator (architecture, weights, optimiser &amp; runtime state).\n\n    Parameters\n    ----------\n    filepath : str | Path\n        Destination file. Parent directories are created automatically.\n    \"\"\"\n    filepath = Path(filepath)\n    filepath.parent.mkdir(parents=True, exist_ok=True)\n\n    state = {\n        \"estimator_class\": f\"{type(self).__module__}.{type(self).__name__}\",\n        \"init_params\": self._get_all_init_params(),\n        \"model_state_dict\": getattr(self.module, \"state_dict\", lambda: {})(),\n        \"optimizer_state_dict\": getattr(self.optimizer, \"state_dict\", lambda: {})(),\n        \"runtime_state\": self._get_runtime_state(),\n    }\n\n    with open(filepath, \"wb\") as f:\n        pickle.dump(state, f)\n</code></pre>"},{"location":"reference/regression/rolling_regressor/","title":"rolling_regressor","text":""},{"location":"reference/regression/rolling_regressor/#deep_river.regression.rolling_regressor","title":"rolling_regressor","text":"<p>Classes:</p> Name Description <code>RollingRegressor</code> <p>Incremental regressor with a fixed-size rolling window.</p>"},{"location":"reference/regression/rolling_regressor/#deep_river.regression.rolling_regressor.RollingRegressor","title":"RollingRegressor","text":"<pre><code>RollingRegressor(\n    module: Module,\n    loss_fn: Union[str, Callable] = \"mse\",\n    optimizer_fn: Union[str, Type[Optimizer]] = \"sgd\",\n    lr: float = 0.001,\n    is_feature_incremental: bool = False,\n    device: str = \"cpu\",\n    seed: int = 42,\n    window_size: int = 10,\n    append_predict: bool = False,\n    **kwargs\n)\n</code></pre> <p>               Bases: <code>RollingDeepEstimator</code>, <code>Regressor</code></p> <p>Incremental regressor with a fixed-size rolling window.</p> <p>Maintains the most recent <code>window_size</code> observations in a deque and feeds them as a (sequence_length, batch=1, n_features) tensor to the wrapped PyTorch module. This enables simple sequence style conditioning for models such as RNN/LSTM/GRU without storing the full historical stream.</p> <p>Parameters:</p> Name Type Description Default <code>module</code> <code>Module</code> <p>Wrapped regression module (expects rolling tensor input shape).</p> required <code>loss_fn</code> <code>str | Callable</code> <p>Loss used for optimisation.</p> <code>'mse'</code> <code>optimizer_fn</code> <code>str | type</code> <p>Optimizer specification.</p> <code>'sgd'</code> <code>lr</code> <code>float</code> <p>Learning rate.</p> <code>1e-3</code> <code>is_feature_incremental</code> <code>bool</code> <p>Whether to expand the first trainable layer when new feature names appear.</p> <code>False</code> <code>device</code> <code>str</code> <p>Torch device.</p> <code>'cpu'</code> <code>seed</code> <code>int</code> <p>Random seed.</p> <code>42</code> <code>window_size</code> <code>int</code> <p>Number of most recent samples kept in the rolling buffer.</p> <code>10</code> <code>append_predict</code> <code>bool</code> <p>If True, predicted samples (during prediction) are appended to the window enabling simple autoregressive rollouts.</p> <code>False</code> <code>**kwargs</code> <p>Forwarded to :class:<code>~deep_river.base.RollingDeepEstimator</code>.</p> <code>{}</code> <p>Examples:</p> <pre><code>Real-world regression example using the Bikes dataset from river. We keep only\nthe numeric features so the rolling tensor construction succeeds. A small GRU\nis trained online and we track a running MAE. The exact value may vary across\nlibrary versions and hardware.\n</code></pre> <pre><code>&gt;&gt;&gt; import random, numpy as np\n&gt;&gt;&gt; from torch import nn, manual_seed\n&gt;&gt;&gt; from river import datasets, metrics\n&gt;&gt;&gt; from deep_river.regression.rolling_regressor import RollingRegressor\n&gt;&gt;&gt; _ = manual_seed(42)\n&gt;&gt;&gt; random.seed(42)\n&gt;&gt;&gt; np.random.seed(42)\n&gt;&gt;&gt; first_x, _ = next(iter(datasets.Bikes()))\n&gt;&gt;&gt; numeric_keys = sorted([k for k, v in first_x.items() if isinstance(v, (int, float))])\n&gt;&gt;&gt; class TinySeq(nn.Module):\n...     def __init__(self, n_features):\n...         super().__init__()\n...         self.rnn = nn.GRU(n_features, 8)\n...         self.head = nn.Linear(8, 1)\n...     def forward(self, x):\n...         out, _ = self.rnn(x)\n...         return self.head(out[-1])\n&gt;&gt;&gt; model = RollingRegressor(module=TinySeq(len(numeric_keys)), window_size=8)\n&gt;&gt;&gt; mae = metrics.MAE()\n&gt;&gt;&gt; window_size = 8\n&gt;&gt;&gt; for i, (x, y) in enumerate(datasets.Bikes().take(200)):\n...     x_num = {k: x[k] for k in numeric_keys}\n...     if i &gt;= window_size:\n...         y_pred = model.predict_one(x_num)\n...         mae.update(y, y_pred)\n...     model.learn_one(x_num, y)\n&gt;&gt;&gt; assert 0.0 &lt;= mae.get() &lt; 15.0\n&gt;&gt;&gt; print(f\"MAE: {mae.get():.4f}\")\nMAE: ...\n</code></pre> <p>Methods:</p> Name Description <code>clone</code> <p>Return a fresh estimator instance with (optionally) copied state.</p> <code>draw</code> <p>Render a (partial) computational graph of the wrapped model.</p> <code>learn_many</code> <p>Batch update with multiple samples using the rolling window.</p> <code>learn_one</code> <p>Update model using a single (x, y) and current rolling window.</p> <code>load</code> <p>Load a previously saved estimator.</p> <code>predict_many</code> <p>Predict targets for multiple samples (appends to a copy of the window).</p> <code>predict_one</code> <p>Predict a single regression target using rolling context.</p> <code>save</code> <p>Persist the estimator (architecture, weights, optimiser &amp; runtime state).</p> Source code in <code>deep_river/regression/rolling_regressor.py</code> <pre><code>def __init__(\n    self,\n    module: torch.nn.Module,\n    loss_fn: Union[str, Callable] = \"mse\",\n    optimizer_fn: Union[str, Type[optim.Optimizer]] = \"sgd\",\n    lr: float = 1e-3,\n    is_feature_incremental: bool = False,\n    device: str = \"cpu\",\n    seed: int = 42,\n    window_size: int = 10,\n    append_predict: bool = False,\n    **kwargs,\n):\n    super().__init__(\n        module=module,\n        loss_fn=loss_fn,\n        optimizer_fn=optimizer_fn,\n        lr=lr,\n        is_feature_incremental=is_feature_incremental,\n        device=device,\n        seed=seed,\n        window_size=window_size,\n        append_predict=append_predict,\n        **kwargs,\n    )\n</code></pre>"},{"location":"reference/regression/rolling_regressor/#deep_river.regression.rolling_regressor.RollingRegressor.clone","title":"clone","text":"<pre><code>clone(\n    new_params=None,\n    include_attributes: bool = False,\n    copy_weights: bool = False,\n)\n</code></pre> <p>Return a fresh estimator instance with (optionally) copied state.</p> <p>Parameters:</p> Name Type Description Default <code>new_params</code> <code>dict | None</code> <p>Parameter overrides for the cloned instance.</p> <code>None</code> <code>include_attributes</code> <code>bool</code> <p>If True, runtime state (observed features, buffers) is also copied.</p> <code>False</code> <code>copy_weights</code> <code>bool</code> <p>If True, model weights are copied (otherwise the module is re\u2011initialised).</p> <code>False</code> Source code in <code>deep_river/base.py</code> <pre><code>def clone(\n    self,\n    new_params=None,\n    include_attributes: bool = False,\n    copy_weights: bool = False,\n):\n    \"\"\"Return a fresh estimator instance with (optionally) copied state.\n\n    Parameters\n    ----------\n    new_params : dict | None\n        Parameter overrides for the cloned instance.\n    include_attributes : bool, default=False\n        If True, runtime state (observed features, buffers) is also copied.\n    copy_weights : bool, default=False\n        If True, model weights are copied (otherwise the module is re\u2011initialised).\n    \"\"\"\n    new_params = new_params or {}\n    copy_weights = new_params.pop(\"copy_weights\", copy_weights)\n\n    params = {**self._get_all_init_params(), **new_params}\n\n    if \"module\" not in new_params:\n        params[\"module\"] = self._rebuild_module()\n\n    new_est = self.__class__(**self._filter_kwargs(self.__class__.__init__, params))\n\n    if copy_weights and hasattr(self.module, \"state_dict\"):\n        new_est.module.load_state_dict(self.module.state_dict())\n\n    if include_attributes:\n        new_est._restore_runtime_state(self._get_runtime_state())\n\n    return new_est\n</code></pre>"},{"location":"reference/regression/rolling_regressor/#deep_river.regression.rolling_regressor.RollingRegressor.draw","title":"draw","text":"<pre><code>draw()\n</code></pre> <p>Render a (partial) computational graph of the wrapped model.</p> <p>Imports <code>graphviz</code> and <code>torchviz</code> lazily. Raises an informative ImportError if the optional dependencies are not installed.</p> Source code in <code>deep_river/base.py</code> <pre><code>def draw(self):  # type: ignore[override]\n    \"\"\"Render a (partial) computational graph of the wrapped model.\n\n    Imports ``graphviz`` and ``torchviz`` lazily. Raises an informative\n    ImportError if the optional dependencies are not installed.\n    \"\"\"\n    try:  # pragma: no cover\n        from torchviz import make_dot  # type: ignore\n    except Exception as err:  # noqa: BLE001\n        raise ImportError(\n            \"graphviz and torchviz must be installed to draw the model.\"\n        ) from err\n\n    first_parameter = next(self.module.parameters())\n    input_shape = first_parameter.size()\n    y_pred = self.module(torch.rand(input_shape))\n    return make_dot(y_pred.mean(), params=dict(self.module.named_parameters()))\n</code></pre>"},{"location":"reference/regression/rolling_regressor/#deep_river.regression.rolling_regressor.RollingRegressor.learn_many","title":"learn_many","text":"<pre><code>learn_many(X: DataFrame, y: Series) -&gt; None\n</code></pre> <p>Batch update with multiple samples using the rolling window.</p> <p>Only performs an optimisation step once the internal window has reached <code>window_size</code> length to ensure a full sequence is available.</p> Source code in <code>deep_river/regression/rolling_regressor.py</code> <pre><code>def learn_many(self, X: pd.DataFrame, y: pd.Series) -&gt; None:\n    \"\"\"Batch update with multiple samples using the rolling window.\n\n    Only performs an optimisation step once the internal window has reached\n    ``window_size`` length to ensure a full sequence is available.\n    \"\"\"\n    self._update_observed_features(X)\n\n    X = X[list(self.observed_features)]\n    self._x_window.extend(X.values.tolist())\n\n    if len(self._x_window) == self.window_size:\n        X_t = self._deque2rolling_tensor(self._x_window)\n\n        # Convert y to tensor (ensuring proper shape for regression)\n        y_t = torch.tensor(y.values, dtype=torch.float32, device=self.device).view(\n            -1, 1\n        )\n\n        self._learn(x=X_t, y=y_t)\n</code></pre>"},{"location":"reference/regression/rolling_regressor/#deep_river.regression.rolling_regressor.RollingRegressor.learn_one","title":"learn_one","text":"<pre><code>learn_one(x: dict, y: RegTarget, **kwargs) -&gt; None\n</code></pre> <p>Update model using a single (x, y) and current rolling window.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>dict</code> <p>Feature mapping.</p> required <code>y</code> <code>float</code> <p>Target value.</p> required Source code in <code>deep_river/regression/rolling_regressor.py</code> <pre><code>def learn_one(self, x: dict, y: base.typing.RegTarget, **kwargs) -&gt; None:\n    \"\"\"Update model using a single (x, y) and current rolling window.\n\n    Parameters\n    ----------\n    x : dict\n        Feature mapping.\n    y : float\n        Target value.\n    \"\"\"\n    self._update_observed_features(x)\n\n    self._x_window.append([x.get(feature, 0) for feature in self.observed_features])\n\n    x_t = self._deque2rolling_tensor(self._x_window)\n\n    # Convert y to tensor (ensuring proper shape for regression)\n    y_t = torch.tensor([y], dtype=torch.float32, device=self.device).view(-1, 1)\n\n    self._learn(x=x_t, y=y_t)\n</code></pre>"},{"location":"reference/regression/rolling_regressor/#deep_river.regression.rolling_regressor.RollingRegressor.load","title":"load  <code>classmethod</code>","text":"<pre><code>load(filepath: Union[str, Path])\n</code></pre> <p>Load a previously saved estimator.</p> <p>The method reconstructs the estimator class, its wrapped module, optimiser state and runtime information (feature names, buffers, etc.).</p> Source code in <code>deep_river/base.py</code> <pre><code>@classmethod\ndef load(cls, filepath: Union[str, Path]):\n    \"\"\"Load a previously saved estimator.\n\n    The method reconstructs the estimator class, its wrapped module, optimiser\n    state and runtime information (feature names, buffers, etc.).\n    \"\"\"\n    with open(filepath, \"rb\") as f:\n        state = pickle.load(f)\n\n    estimator_cls = cls._import_from_path(state[\"estimator_class\"])\n    init_params = state[\"init_params\"]\n\n    # Rebuild module if needed\n    if \"module\" in init_params and isinstance(init_params[\"module\"], dict):\n        module_info = init_params.pop(\"module\")\n        module_cls = cls._import_from_path(module_info[\"class\"])\n        module = module_cls(\n            **cls._filter_kwargs(module_cls.__init__, module_info[\"kwargs\"])\n        )\n        if state.get(\"model_state_dict\"):\n            module.load_state_dict(state[\"model_state_dict\"])\n        init_params[\"module\"] = module\n\n    estimator = estimator_cls(\n        **cls._filter_kwargs(estimator_cls.__init__, init_params)\n    )\n\n    if state.get(\"optimizer_state_dict\") and hasattr(estimator, \"optimizer\"):\n        try:\n            estimator.optimizer.load_state_dict(\n                state[\"optimizer_state_dict\"]  # type: ignore[arg-type]\n            )\n        except Exception:  # noqa: E722\n            pass\n\n    estimator._restore_runtime_state(state.get(\"runtime_state\", {}))\n    return estimator\n</code></pre>"},{"location":"reference/regression/rolling_regressor/#deep_river.regression.rolling_regressor.RollingRegressor.predict_many","title":"predict_many","text":"<pre><code>predict_many(X: DataFrame) -&gt; DataFrame\n</code></pre> <p>Predict targets for multiple samples (appends to a copy of the window).</p> <p>Returns a single-column DataFrame named <code>'y_pred'</code>.</p> Source code in <code>deep_river/regression/rolling_regressor.py</code> <pre><code>def predict_many(self, X: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"Predict targets for multiple samples (appends to a copy of the window).\n\n    Returns a single-column DataFrame named ``'y_pred'``.\n    \"\"\"\n\n    self._update_observed_features(X)\n    X = X[list(self.observed_features)]\n    x_win = self._x_window.copy()\n    x_win.extend(X.values.tolist())\n    if self.append_predict:\n        self._x_window = x_win\n\n    self.module.eval()\n    with torch.inference_mode():\n        x_t = self._deque2rolling_tensor(x_win)\n        y_preds = self.module(x_t)\n        if isinstance(y_preds, torch.Tensor):\n            y_preds = y_preds.detach().cpu().view(-1).numpy().tolist()\n\n    return pd.DataFrame({\"y_pred\": y_preds})\n</code></pre>"},{"location":"reference/regression/rolling_regressor/#deep_river.regression.rolling_regressor.RollingRegressor.predict_one","title":"predict_one","text":"<pre><code>predict_one(x: dict) -&gt; RegTarget\n</code></pre> <p>Predict a single regression target using rolling context.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>dict</code> <p>Feature mapping.</p> required <p>Returns:</p> Type Description <code>float</code> <p>Predicted target value.</p> Source code in <code>deep_river/regression/rolling_regressor.py</code> <pre><code>def predict_one(self, x: dict) -&gt; base.typing.RegTarget:\n    \"\"\"Predict a single regression target using rolling context.\n\n    Parameters\n    ----------\n    x : dict\n        Feature mapping.\n\n    Returns\n    -------\n    float\n        Predicted target value.\n    \"\"\"\n    self._update_observed_features(x)\n\n    x_win = self._x_window.copy()\n    x_win.append([x.get(feature, 0) for feature in self.observed_features])\n    if self.append_predict:\n        self._x_window = x_win\n\n    self.module.eval()\n    with torch.inference_mode():\n        x_t = self._deque2rolling_tensor(x_win)\n        y_pred = self.module(x_t)\n        if isinstance(y_pred, torch.Tensor):\n            y_pred = y_pred.detach().view(-1)[-1].cpu().numpy().item()\n        else:\n            y_pred = float(y_pred)\n\n    return y_pred\n</code></pre>"},{"location":"reference/regression/rolling_regressor/#deep_river.regression.rolling_regressor.RollingRegressor.save","title":"save","text":"<pre><code>save(filepath: Union[str, Path]) -&gt; None\n</code></pre> <p>Persist the estimator (architecture, weights, optimiser &amp; runtime state).</p> <p>Parameters:</p> Name Type Description Default <code>filepath</code> <code>str | Path</code> <p>Destination file. Parent directories are created automatically.</p> required Source code in <code>deep_river/base.py</code> <pre><code>def save(self, filepath: Union[str, Path]) -&gt; None:\n    \"\"\"Persist the estimator (architecture, weights, optimiser &amp; runtime state).\n\n    Parameters\n    ----------\n    filepath : str | Path\n        Destination file. Parent directories are created automatically.\n    \"\"\"\n    filepath = Path(filepath)\n    filepath.parent.mkdir(parents=True, exist_ok=True)\n\n    state = {\n        \"estimator_class\": f\"{type(self).__module__}.{type(self).__name__}\",\n        \"init_params\": self._get_all_init_params(),\n        \"model_state_dict\": getattr(self.module, \"state_dict\", lambda: {})(),\n        \"optimizer_state_dict\": getattr(self.optimizer, \"state_dict\", lambda: {})(),\n        \"runtime_state\": self._get_runtime_state(),\n    }\n\n    with open(filepath, \"wb\") as f:\n        pickle.dump(state, f)\n</code></pre>"},{"location":"reference/regression/zoo/","title":"zoo","text":""},{"location":"reference/regression/zoo/#deep_river.regression.zoo","title":"zoo","text":"<p>Classes:</p> Name Description <code>LSTMRegressor</code> <p>Rolling LSTM regressor for sequential / time-series data.</p> <code>LinearRegression</code> <p>Incremental linear regression with optional feature growth and gradient clipping.</p> <code>MultiLayerPerceptron</code> <p>Multi-layer perceptron regressor with optional feature growth.</p> <code>RNNRegressor</code> <p>Rolling RNN regressor for sequential / time-series data.</p>"},{"location":"reference/regression/zoo/#deep_river.regression.zoo.LSTMRegressor","title":"LSTMRegressor","text":"<pre><code>LSTMRegressor(\n    n_features: int = 10,\n    hidden_size: int = 32,\n    num_layers: int = 1,\n    dropout: float = 0.0,\n    gradient_clip_value: float | None = 1.0,\n    loss_fn: Union[str, Callable] = \"mse\",\n    optimizer_fn: Union[str, Type[Optimizer]] = \"adam\",\n    lr: float = 0.001,\n    is_feature_incremental: bool = False,\n    device: str = \"cpu\",\n    seed: int = 42,\n    **kwargs\n)\n</code></pre> <p>               Bases: <code>RollingRegressor</code></p> <p>Rolling LSTM regressor for sequential / time-series data.</p> <p>Improves over a na\u00efve single-unit LSTM by separating the hidden representation (<code>hidden_size</code>) from the 1D regression output head. Supports optional dropout and multiple LSTM layers. Designed to work with a rolling window maintained by :class:<code>~deep_river.base.RollingDeepEstimator</code>.</p> <p>Parameters:</p> Name Type Description Default <code>n_features</code> <code>int</code> <p>Number of input features per timestep (may grow if feature-incremental).</p> <code>10</code> <code>hidden_size</code> <code>int</code> <p>Dimensionality of the LSTM hidden state.</p> <code>32</code> <code>num_layers</code> <code>int</code> <p>Number of stacked LSTM layers.</p> <code>1</code> <code>dropout</code> <code>float</code> <p>Dropout probability applied after the LSTM (and internally by PyTorch if <code>num_layers &gt; 1</code>). Capped internally for safety.</p> <code>0.0</code> <code>gradient_clip_value</code> <code>float | None</code> <p>Gradient norm clipping threshold (helps stability). <code>None</code> disables it.</p> <code>1.0</code> <code>loss_fn</code> <code>Union[str, Callable]</code> <p>Standard configuration.</p> <code>'mse'</code> <code>optimizer_fn</code> <code>Union[str, Callable]</code> <p>Standard configuration.</p> <code>'mse'</code> <code>lr</code> <code>Union[str, Callable]</code> <p>Standard configuration.</p> <code>'mse'</code> <code>is_feature_incremental</code> <code>Union[str, Callable]</code> <p>Standard configuration.</p> <code>'mse'</code> <code>device</code> <code>Union[str, Callable]</code> <p>Standard configuration.</p> <code>'mse'</code> <code>seed</code> <code>Union[str, Callable]</code> <p>Standard configuration.</p> <code>'mse'</code> <code>**kwargs</code> <code>Union[str, Callable]</code> <p>Standard configuration.</p> <code>'mse'</code> <p>Examples:</p> <p>Streaming regression on the Bikes dataset (only numeric features kept). The exact MAE value may vary depending on library version and hardware::</p> <pre><code>&gt;&gt;&gt; import random, numpy as np, torch\n&gt;&gt;&gt; from torch import manual_seed\n&gt;&gt;&gt; from river import datasets, metrics\n&gt;&gt;&gt; from deep_river.regression.zoo import LSTMRegressor\n&gt;&gt;&gt; _ = manual_seed(42); random.seed(42); np.random.seed(42)\n&gt;&gt;&gt; first_x, _ = next(iter(datasets.Bikes()))\n&gt;&gt;&gt; numeric_keys = sorted([k for k,v in first_x.items() if isinstance(v,(int,float))])\n&gt;&gt;&gt; reg = LSTMRegressor(\n...     n_features=len(numeric_keys), hidden_size=8, num_layers=1,\n...     optimizer_fn='sgd', lr=1e-2, is_feature_incremental=True,\n... )\n&gt;&gt;&gt; mae = metrics.MAE()\n&gt;&gt;&gt; for i, (x, y) in enumerate(datasets.Bikes().take(200)):\n...     x_num = {k: x[k] for k in numeric_keys}\n...     if i &gt; 0:\n...         y_pred = reg.predict_one(x_num)\n...         mae.update(y, y_pred)\n...     reg.learn_one(x_num, y)\n&gt;&gt;&gt; assert 0.0 &lt;= mae.get() &lt; 20.0\n&gt;&gt;&gt; print(f\"MAE: {mae.get():.4f}\")  # doctest: +ELLIPSIS\nMAE: ...\n</code></pre> <p>Methods:</p> Name Description <code>clone</code> <p>Return a fresh estimator instance with (optionally) copied state.</p> <code>draw</code> <p>Render a (partial) computational graph of the wrapped model.</p> <code>learn_many</code> <p>Batch update with multiple samples using the rolling window.</p> <code>learn_one</code> <p>Update model using a single (x, y) and current rolling window.</p> <code>load</code> <p>Load a previously saved estimator.</p> <code>predict_many</code> <p>Predict targets for multiple samples (appends to a copy of the window).</p> <code>predict_one</code> <p>Predict a single regression target using rolling context.</p> <code>save</code> <p>Persist the estimator (architecture, weights, optimiser &amp; runtime state).</p> Source code in <code>deep_river/regression/zoo.py</code> <pre><code>def __init__(\n    self,\n    n_features: int = 10,\n    hidden_size: int = 32,\n    num_layers: int = 1,\n    dropout: float = 0.0,\n    gradient_clip_value: float | None = 1.0,\n    loss_fn: Union[str, Callable] = \"mse\",\n    optimizer_fn: Union[str, Type[optim.Optimizer]] = \"adam\",\n    lr: float = 1e-3,\n    is_feature_incremental: bool = False,\n    device: str = \"cpu\",\n    seed: int = 42,\n    **kwargs,\n):\n    self.n_features = n_features\n    self.hidden_size = hidden_size\n    self.num_layers = num_layers\n    self.dropout = dropout\n    self.gradient_clip_value = gradient_clip_value\n    module = LSTMRegressor.LSTMModule(\n        n_features=n_features,\n        hidden_size=hidden_size,\n        num_layers=num_layers,\n        dropout=dropout,\n    )\n    if \"module\" in kwargs:\n        del kwargs[\"module\"]\n    super().__init__(\n        module=module,\n        loss_fn=loss_fn,\n        optimizer_fn=optimizer_fn,\n        is_feature_incremental=is_feature_incremental,\n        device=device,\n        lr=lr,\n        seed=seed,\n        gradient_clip_value=gradient_clip_value,\n        **kwargs,\n    )\n</code></pre>"},{"location":"reference/regression/zoo/#deep_river.regression.zoo.LSTMRegressor.clone","title":"clone","text":"<pre><code>clone(\n    new_params=None,\n    include_attributes: bool = False,\n    copy_weights: bool = False,\n)\n</code></pre> <p>Return a fresh estimator instance with (optionally) copied state.</p> <p>Parameters:</p> Name Type Description Default <code>new_params</code> <code>dict | None</code> <p>Parameter overrides for the cloned instance.</p> <code>None</code> <code>include_attributes</code> <code>bool</code> <p>If True, runtime state (observed features, buffers) is also copied.</p> <code>False</code> <code>copy_weights</code> <code>bool</code> <p>If True, model weights are copied (otherwise the module is re\u2011initialised).</p> <code>False</code> Source code in <code>deep_river/base.py</code> <pre><code>def clone(\n    self,\n    new_params=None,\n    include_attributes: bool = False,\n    copy_weights: bool = False,\n):\n    \"\"\"Return a fresh estimator instance with (optionally) copied state.\n\n    Parameters\n    ----------\n    new_params : dict | None\n        Parameter overrides for the cloned instance.\n    include_attributes : bool, default=False\n        If True, runtime state (observed features, buffers) is also copied.\n    copy_weights : bool, default=False\n        If True, model weights are copied (otherwise the module is re\u2011initialised).\n    \"\"\"\n    new_params = new_params or {}\n    copy_weights = new_params.pop(\"copy_weights\", copy_weights)\n\n    params = {**self._get_all_init_params(), **new_params}\n\n    if \"module\" not in new_params:\n        params[\"module\"] = self._rebuild_module()\n\n    new_est = self.__class__(**self._filter_kwargs(self.__class__.__init__, params))\n\n    if copy_weights and hasattr(self.module, \"state_dict\"):\n        new_est.module.load_state_dict(self.module.state_dict())\n\n    if include_attributes:\n        new_est._restore_runtime_state(self._get_runtime_state())\n\n    return new_est\n</code></pre>"},{"location":"reference/regression/zoo/#deep_river.regression.zoo.LSTMRegressor.draw","title":"draw","text":"<pre><code>draw()\n</code></pre> <p>Render a (partial) computational graph of the wrapped model.</p> <p>Imports <code>graphviz</code> and <code>torchviz</code> lazily. Raises an informative ImportError if the optional dependencies are not installed.</p> Source code in <code>deep_river/base.py</code> <pre><code>def draw(self):  # type: ignore[override]\n    \"\"\"Render a (partial) computational graph of the wrapped model.\n\n    Imports ``graphviz`` and ``torchviz`` lazily. Raises an informative\n    ImportError if the optional dependencies are not installed.\n    \"\"\"\n    try:  # pragma: no cover\n        from torchviz import make_dot  # type: ignore\n    except Exception as err:  # noqa: BLE001\n        raise ImportError(\n            \"graphviz and torchviz must be installed to draw the model.\"\n        ) from err\n\n    first_parameter = next(self.module.parameters())\n    input_shape = first_parameter.size()\n    y_pred = self.module(torch.rand(input_shape))\n    return make_dot(y_pred.mean(), params=dict(self.module.named_parameters()))\n</code></pre>"},{"location":"reference/regression/zoo/#deep_river.regression.zoo.LSTMRegressor.learn_many","title":"learn_many","text":"<pre><code>learn_many(X: DataFrame, y: Series) -&gt; None\n</code></pre> <p>Batch update with multiple samples using the rolling window.</p> <p>Only performs an optimisation step once the internal window has reached <code>window_size</code> length to ensure a full sequence is available.</p> Source code in <code>deep_river/regression/rolling_regressor.py</code> <pre><code>def learn_many(self, X: pd.DataFrame, y: pd.Series) -&gt; None:\n    \"\"\"Batch update with multiple samples using the rolling window.\n\n    Only performs an optimisation step once the internal window has reached\n    ``window_size`` length to ensure a full sequence is available.\n    \"\"\"\n    self._update_observed_features(X)\n\n    X = X[list(self.observed_features)]\n    self._x_window.extend(X.values.tolist())\n\n    if len(self._x_window) == self.window_size:\n        X_t = self._deque2rolling_tensor(self._x_window)\n\n        # Convert y to tensor (ensuring proper shape for regression)\n        y_t = torch.tensor(y.values, dtype=torch.float32, device=self.device).view(\n            -1, 1\n        )\n\n        self._learn(x=X_t, y=y_t)\n</code></pre>"},{"location":"reference/regression/zoo/#deep_river.regression.zoo.LSTMRegressor.learn_one","title":"learn_one","text":"<pre><code>learn_one(x: dict, y: RegTarget, **kwargs) -&gt; None\n</code></pre> <p>Update model using a single (x, y) and current rolling window.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>dict</code> <p>Feature mapping.</p> required <code>y</code> <code>float</code> <p>Target value.</p> required Source code in <code>deep_river/regression/rolling_regressor.py</code> <pre><code>def learn_one(self, x: dict, y: base.typing.RegTarget, **kwargs) -&gt; None:\n    \"\"\"Update model using a single (x, y) and current rolling window.\n\n    Parameters\n    ----------\n    x : dict\n        Feature mapping.\n    y : float\n        Target value.\n    \"\"\"\n    self._update_observed_features(x)\n\n    self._x_window.append([x.get(feature, 0) for feature in self.observed_features])\n\n    x_t = self._deque2rolling_tensor(self._x_window)\n\n    # Convert y to tensor (ensuring proper shape for regression)\n    y_t = torch.tensor([y], dtype=torch.float32, device=self.device).view(-1, 1)\n\n    self._learn(x=x_t, y=y_t)\n</code></pre>"},{"location":"reference/regression/zoo/#deep_river.regression.zoo.LSTMRegressor.load","title":"load  <code>classmethod</code>","text":"<pre><code>load(filepath: Union[str, Path])\n</code></pre> <p>Load a previously saved estimator.</p> <p>The method reconstructs the estimator class, its wrapped module, optimiser state and runtime information (feature names, buffers, etc.).</p> Source code in <code>deep_river/base.py</code> <pre><code>@classmethod\ndef load(cls, filepath: Union[str, Path]):\n    \"\"\"Load a previously saved estimator.\n\n    The method reconstructs the estimator class, its wrapped module, optimiser\n    state and runtime information (feature names, buffers, etc.).\n    \"\"\"\n    with open(filepath, \"rb\") as f:\n        state = pickle.load(f)\n\n    estimator_cls = cls._import_from_path(state[\"estimator_class\"])\n    init_params = state[\"init_params\"]\n\n    # Rebuild module if needed\n    if \"module\" in init_params and isinstance(init_params[\"module\"], dict):\n        module_info = init_params.pop(\"module\")\n        module_cls = cls._import_from_path(module_info[\"class\"])\n        module = module_cls(\n            **cls._filter_kwargs(module_cls.__init__, module_info[\"kwargs\"])\n        )\n        if state.get(\"model_state_dict\"):\n            module.load_state_dict(state[\"model_state_dict\"])\n        init_params[\"module\"] = module\n\n    estimator = estimator_cls(\n        **cls._filter_kwargs(estimator_cls.__init__, init_params)\n    )\n\n    if state.get(\"optimizer_state_dict\") and hasattr(estimator, \"optimizer\"):\n        try:\n            estimator.optimizer.load_state_dict(\n                state[\"optimizer_state_dict\"]  # type: ignore[arg-type]\n            )\n        except Exception:  # noqa: E722\n            pass\n\n    estimator._restore_runtime_state(state.get(\"runtime_state\", {}))\n    return estimator\n</code></pre>"},{"location":"reference/regression/zoo/#deep_river.regression.zoo.LSTMRegressor.predict_many","title":"predict_many","text":"<pre><code>predict_many(X: DataFrame) -&gt; DataFrame\n</code></pre> <p>Predict targets for multiple samples (appends to a copy of the window).</p> <p>Returns a single-column DataFrame named <code>'y_pred'</code>.</p> Source code in <code>deep_river/regression/rolling_regressor.py</code> <pre><code>def predict_many(self, X: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"Predict targets for multiple samples (appends to a copy of the window).\n\n    Returns a single-column DataFrame named ``'y_pred'``.\n    \"\"\"\n\n    self._update_observed_features(X)\n    X = X[list(self.observed_features)]\n    x_win = self._x_window.copy()\n    x_win.extend(X.values.tolist())\n    if self.append_predict:\n        self._x_window = x_win\n\n    self.module.eval()\n    with torch.inference_mode():\n        x_t = self._deque2rolling_tensor(x_win)\n        y_preds = self.module(x_t)\n        if isinstance(y_preds, torch.Tensor):\n            y_preds = y_preds.detach().cpu().view(-1).numpy().tolist()\n\n    return pd.DataFrame({\"y_pred\": y_preds})\n</code></pre>"},{"location":"reference/regression/zoo/#deep_river.regression.zoo.LSTMRegressor.predict_one","title":"predict_one","text":"<pre><code>predict_one(x: dict) -&gt; RegTarget\n</code></pre> <p>Predict a single regression target using rolling context.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>dict</code> <p>Feature mapping.</p> required <p>Returns:</p> Type Description <code>float</code> <p>Predicted target value.</p> Source code in <code>deep_river/regression/rolling_regressor.py</code> <pre><code>def predict_one(self, x: dict) -&gt; base.typing.RegTarget:\n    \"\"\"Predict a single regression target using rolling context.\n\n    Parameters\n    ----------\n    x : dict\n        Feature mapping.\n\n    Returns\n    -------\n    float\n        Predicted target value.\n    \"\"\"\n    self._update_observed_features(x)\n\n    x_win = self._x_window.copy()\n    x_win.append([x.get(feature, 0) for feature in self.observed_features])\n    if self.append_predict:\n        self._x_window = x_win\n\n    self.module.eval()\n    with torch.inference_mode():\n        x_t = self._deque2rolling_tensor(x_win)\n        y_pred = self.module(x_t)\n        if isinstance(y_pred, torch.Tensor):\n            y_pred = y_pred.detach().view(-1)[-1].cpu().numpy().item()\n        else:\n            y_pred = float(y_pred)\n\n    return y_pred\n</code></pre>"},{"location":"reference/regression/zoo/#deep_river.regression.zoo.LSTMRegressor.save","title":"save","text":"<pre><code>save(filepath: Union[str, Path]) -&gt; None\n</code></pre> <p>Persist the estimator (architecture, weights, optimiser &amp; runtime state).</p> <p>Parameters:</p> Name Type Description Default <code>filepath</code> <code>str | Path</code> <p>Destination file. Parent directories are created automatically.</p> required Source code in <code>deep_river/base.py</code> <pre><code>def save(self, filepath: Union[str, Path]) -&gt; None:\n    \"\"\"Persist the estimator (architecture, weights, optimiser &amp; runtime state).\n\n    Parameters\n    ----------\n    filepath : str | Path\n        Destination file. Parent directories are created automatically.\n    \"\"\"\n    filepath = Path(filepath)\n    filepath.parent.mkdir(parents=True, exist_ok=True)\n\n    state = {\n        \"estimator_class\": f\"{type(self).__module__}.{type(self).__name__}\",\n        \"init_params\": self._get_all_init_params(),\n        \"model_state_dict\": getattr(self.module, \"state_dict\", lambda: {})(),\n        \"optimizer_state_dict\": getattr(self.optimizer, \"state_dict\", lambda: {})(),\n        \"runtime_state\": self._get_runtime_state(),\n    }\n\n    with open(filepath, \"wb\") as f:\n        pickle.dump(state, f)\n</code></pre>"},{"location":"reference/regression/zoo/#deep_river.regression.zoo.LinearRegression","title":"LinearRegression","text":"<pre><code>LinearRegression(\n    n_features: int = 10,\n    loss_fn: Union[str, Callable] = \"mse\",\n    optimizer_fn: Union[str, Type[Optimizer]] = \"sgd\",\n    lr: float = 0.001,\n    is_feature_incremental: bool = False,\n    device: str = \"cpu\",\n    seed: int = 42,\n    gradient_clip_value: float | None = 1.0,\n    **kwargs\n)\n</code></pre> <p>               Bases: <code>Regressor</code></p> <p>Incremental linear regression with optional feature growth and gradient clipping.</p> <p>A thin wrapper that instantiates a single linear layer and enables dynamic feature expansion when <code>is_feature_incremental=True</code>. The model outputs a single continuous target value.</p> <p>Parameters:</p> Name Type Description Default <code>n_features</code> <code>int</code> <p>Initial number of input features (columns). The input layer can expand if feature incrementality is enabled and new feature names appear.</p> <code>10</code> <code>loss_fn</code> <code>str | Callable</code> <p>Loss used for optimisation.</p> <code>'mse'</code> <code>optimizer_fn</code> <code>str | type</code> <p>Optimizer specification.</p> <code>'sgd'</code> <code>lr</code> <code>float</code> <p>Learning rate.</p> <code>1e-3</code> <code>is_feature_incremental</code> <code>bool</code> <p>Whether to expand the input layer when new features appear.</p> <code>False</code> <code>device</code> <code>str</code> <p>Torch device.</p> <code>'cpu'</code> <code>seed</code> <code>int</code> <p>Random seed.</p> <code>42</code> <code>gradient_clip_value</code> <code>float | None</code> <p>Gradient norm clipping threshold. Disabled if <code>None</code>.</p> <code>None</code> <code>**kwargs</code> <p>Forwarded to :class:<code>~deep_river.base.DeepEstimator</code>.</p> <code>{}</code> <p>Examples:</p> <pre><code>Streaming regression on the Bikes dataset (only numeric features kept).\nThe exact MAE value may vary depending on library version and hardware::\n\n&gt;&gt;&gt; import random, numpy as np, torch\n&gt;&gt;&gt; from torch import manual_seed\n&gt;&gt;&gt; from river import datasets, metrics\n&gt;&gt;&gt; from deep_river.regression.zoo import LinearRegression\n&gt;&gt;&gt; _ = manual_seed(42); random.seed(42); np.random.seed(42)\n&gt;&gt;&gt; first_x, _ = next(iter(datasets.Bikes()))\n&gt;&gt;&gt; numeric_keys = sorted([k for k,v in first_x.items() if isinstance(v,(int,float))])\n&gt;&gt;&gt; reg = LinearRegression(n_features=len(numeric_keys),\n...                        loss_fn='mse', lr=1e-2,\n...                        is_feature_incremental=True)\n&gt;&gt;&gt; mae = metrics.MAE()\n&gt;&gt;&gt; for i, (x, y) in enumerate(datasets.Bikes().take(200)):\n...     x_num = {k: x[k] for k in numeric_keys}\n...     if i &gt; 0:\n...         y_pred = reg.predict_one(x_num)\n...         mae.update(y, y_pred)\n...     reg.learn_one(x_num, y)\n&gt;&gt;&gt; assert 0.0 &lt;= mae.get() &lt; 20.0\n&gt;&gt;&gt; print(f\"MAE: {mae.get():.4f}\")  # doctest: +ELLIPSIS\nMAE: ...\n</code></pre> <p>Methods:</p> Name Description <code>clone</code> <p>Return a fresh estimator instance with (optionally) copied state.</p> <code>draw</code> <p>Render a (partial) computational graph of the wrapped model.</p> <code>load</code> <p>Load a previously saved estimator.</p> <code>predict_many</code> <p>Predict target values for multiple instances (returns single-column DataFrame).</p> <code>predict_one</code> <p>Predict target value for a single instance.</p> <code>save</code> <p>Persist the estimator (architecture, weights, optimiser &amp; runtime state).</p> Source code in <code>deep_river/regression/zoo.py</code> <pre><code>def __init__(\n    self,\n    n_features: int = 10,\n    loss_fn: Union[str, Callable] = \"mse\",\n    optimizer_fn: Union[str, Type[optim.Optimizer]] = \"sgd\",\n    lr: float = 1e-3,\n    is_feature_incremental: bool = False,\n    device: str = \"cpu\",\n    seed: int = 42,\n    gradient_clip_value: float | None = 1.0,\n    **kwargs,\n):\n    self.n_features = n_features\n    module = LinearRegression.LRModule(n_features=n_features)\n    if \"module\" in kwargs:\n        del kwargs[\"module\"]\n    super().__init__(\n        module=module,\n        loss_fn=loss_fn,\n        optimizer_fn=optimizer_fn,\n        is_feature_incremental=is_feature_incremental,\n        device=device,\n        lr=lr,\n        seed=seed,\n        gradient_clip_value=gradient_clip_value,\n        **kwargs,\n    )\n</code></pre>"},{"location":"reference/regression/zoo/#deep_river.regression.zoo.LinearRegression.clone","title":"clone","text":"<pre><code>clone(\n    new_params=None,\n    include_attributes: bool = False,\n    copy_weights: bool = False,\n)\n</code></pre> <p>Return a fresh estimator instance with (optionally) copied state.</p> <p>Parameters:</p> Name Type Description Default <code>new_params</code> <code>dict | None</code> <p>Parameter overrides for the cloned instance.</p> <code>None</code> <code>include_attributes</code> <code>bool</code> <p>If True, runtime state (observed features, buffers) is also copied.</p> <code>False</code> <code>copy_weights</code> <code>bool</code> <p>If True, model weights are copied (otherwise the module is re\u2011initialised).</p> <code>False</code> Source code in <code>deep_river/base.py</code> <pre><code>def clone(\n    self,\n    new_params=None,\n    include_attributes: bool = False,\n    copy_weights: bool = False,\n):\n    \"\"\"Return a fresh estimator instance with (optionally) copied state.\n\n    Parameters\n    ----------\n    new_params : dict | None\n        Parameter overrides for the cloned instance.\n    include_attributes : bool, default=False\n        If True, runtime state (observed features, buffers) is also copied.\n    copy_weights : bool, default=False\n        If True, model weights are copied (otherwise the module is re\u2011initialised).\n    \"\"\"\n    new_params = new_params or {}\n    copy_weights = new_params.pop(\"copy_weights\", copy_weights)\n\n    params = {**self._get_all_init_params(), **new_params}\n\n    if \"module\" not in new_params:\n        params[\"module\"] = self._rebuild_module()\n\n    new_est = self.__class__(**self._filter_kwargs(self.__class__.__init__, params))\n\n    if copy_weights and hasattr(self.module, \"state_dict\"):\n        new_est.module.load_state_dict(self.module.state_dict())\n\n    if include_attributes:\n        new_est._restore_runtime_state(self._get_runtime_state())\n\n    return new_est\n</code></pre>"},{"location":"reference/regression/zoo/#deep_river.regression.zoo.LinearRegression.draw","title":"draw","text":"<pre><code>draw()\n</code></pre> <p>Render a (partial) computational graph of the wrapped model.</p> <p>Imports <code>graphviz</code> and <code>torchviz</code> lazily. Raises an informative ImportError if the optional dependencies are not installed.</p> Source code in <code>deep_river/base.py</code> <pre><code>def draw(self):  # type: ignore[override]\n    \"\"\"Render a (partial) computational graph of the wrapped model.\n\n    Imports ``graphviz`` and ``torchviz`` lazily. Raises an informative\n    ImportError if the optional dependencies are not installed.\n    \"\"\"\n    try:  # pragma: no cover\n        from torchviz import make_dot  # type: ignore\n    except Exception as err:  # noqa: BLE001\n        raise ImportError(\n            \"graphviz and torchviz must be installed to draw the model.\"\n        ) from err\n\n    first_parameter = next(self.module.parameters())\n    input_shape = first_parameter.size()\n    y_pred = self.module(torch.rand(input_shape))\n    return make_dot(y_pred.mean(), params=dict(self.module.named_parameters()))\n</code></pre>"},{"location":"reference/regression/zoo/#deep_river.regression.zoo.LinearRegression.load","title":"load  <code>classmethod</code>","text":"<pre><code>load(filepath: Union[str, Path])\n</code></pre> <p>Load a previously saved estimator.</p> <p>The method reconstructs the estimator class, its wrapped module, optimiser state and runtime information (feature names, buffers, etc.).</p> Source code in <code>deep_river/base.py</code> <pre><code>@classmethod\ndef load(cls, filepath: Union[str, Path]):\n    \"\"\"Load a previously saved estimator.\n\n    The method reconstructs the estimator class, its wrapped module, optimiser\n    state and runtime information (feature names, buffers, etc.).\n    \"\"\"\n    with open(filepath, \"rb\") as f:\n        state = pickle.load(f)\n\n    estimator_cls = cls._import_from_path(state[\"estimator_class\"])\n    init_params = state[\"init_params\"]\n\n    # Rebuild module if needed\n    if \"module\" in init_params and isinstance(init_params[\"module\"], dict):\n        module_info = init_params.pop(\"module\")\n        module_cls = cls._import_from_path(module_info[\"class\"])\n        module = module_cls(\n            **cls._filter_kwargs(module_cls.__init__, module_info[\"kwargs\"])\n        )\n        if state.get(\"model_state_dict\"):\n            module.load_state_dict(state[\"model_state_dict\"])\n        init_params[\"module\"] = module\n\n    estimator = estimator_cls(\n        **cls._filter_kwargs(estimator_cls.__init__, init_params)\n    )\n\n    if state.get(\"optimizer_state_dict\") and hasattr(estimator, \"optimizer\"):\n        try:\n            estimator.optimizer.load_state_dict(\n                state[\"optimizer_state_dict\"]  # type: ignore[arg-type]\n            )\n        except Exception:  # noqa: E722\n            pass\n\n    estimator._restore_runtime_state(state.get(\"runtime_state\", {}))\n    return estimator\n</code></pre>"},{"location":"reference/regression/zoo/#deep_river.regression.zoo.LinearRegression.predict_many","title":"predict_many","text":"<pre><code>predict_many(X: DataFrame) -&gt; DataFrame\n</code></pre> <p>Predict target values for multiple instances (returns single-column DataFrame).</p> Source code in <code>deep_river/regression/regressor.py</code> <pre><code>def predict_many(self, X: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"Predict target values for multiple instances (returns single-column DataFrame).\"\"\"\n    self._update_observed_features(X)\n    x_t = self._df2tensor(X)\n    self.module.eval()\n    with torch.inference_mode():\n        y_preds = self.module(x_t)\n    return pd.DataFrame(y_preds if not y_preds.is_cuda else y_preds.cpu().numpy())\n</code></pre>"},{"location":"reference/regression/zoo/#deep_river.regression.zoo.LinearRegression.predict_one","title":"predict_one","text":"<pre><code>predict_one(x: dict) -&gt; RegTarget\n</code></pre> <p>Predict target value for a single instance.</p> Source code in <code>deep_river/regression/regressor.py</code> <pre><code>def predict_one(self, x: dict) -&gt; RegTarget:\n    \"\"\"Predict target value for a single instance.\"\"\"\n    self._update_observed_features(x)\n    x_t = self._dict2tensor(x)\n    self.module.eval()\n    with torch.inference_mode():\n        y_pred = self.module(x_t).item()\n    return y_pred\n</code></pre>"},{"location":"reference/regression/zoo/#deep_river.regression.zoo.LinearRegression.save","title":"save","text":"<pre><code>save(filepath: Union[str, Path]) -&gt; None\n</code></pre> <p>Persist the estimator (architecture, weights, optimiser &amp; runtime state).</p> <p>Parameters:</p> Name Type Description Default <code>filepath</code> <code>str | Path</code> <p>Destination file. Parent directories are created automatically.</p> required Source code in <code>deep_river/base.py</code> <pre><code>def save(self, filepath: Union[str, Path]) -&gt; None:\n    \"\"\"Persist the estimator (architecture, weights, optimiser &amp; runtime state).\n\n    Parameters\n    ----------\n    filepath : str | Path\n        Destination file. Parent directories are created automatically.\n    \"\"\"\n    filepath = Path(filepath)\n    filepath.parent.mkdir(parents=True, exist_ok=True)\n\n    state = {\n        \"estimator_class\": f\"{type(self).__module__}.{type(self).__name__}\",\n        \"init_params\": self._get_all_init_params(),\n        \"model_state_dict\": getattr(self.module, \"state_dict\", lambda: {})(),\n        \"optimizer_state_dict\": getattr(self.optimizer, \"state_dict\", lambda: {})(),\n        \"runtime_state\": self._get_runtime_state(),\n    }\n\n    with open(filepath, \"wb\") as f:\n        pickle.dump(state, f)\n</code></pre>"},{"location":"reference/regression/zoo/#deep_river.regression.zoo.MultiLayerPerceptron","title":"MultiLayerPerceptron","text":"<pre><code>MultiLayerPerceptron(\n    n_features: int = 10,\n    n_width: int = 5,\n    n_layers: int = 5,\n    loss_fn: Union[str, Callable] = \"mse\",\n    optimizer_fn: Union[str, Type[Optimizer]] = \"sgd\",\n    lr: float = 0.001,\n    is_feature_incremental: bool = False,\n    device: str = \"cpu\",\n    seed: int = 42,\n    gradient_clip_value: float | None = None,\n    **kwargs\n)\n</code></pre> <p>               Bases: <code>Regressor</code></p> <p>Multi-layer perceptron regressor with optional feature growth.</p> <p>Stacks <code>n_layers</code> fully connected layers of width <code>n_width</code> with a sigmoid non-linearity (kept for backward compatibility) followed by a single output unit. Can expand its input layer when new feature names appear.</p> <p>Parameters:</p> Name Type Description Default <code>n_features</code> <code>int</code> <p>Initial number of input features.</p> <code>10</code> <code>n_width</code> <code>int</code> <p>Hidden layer width.</p> <code>5</code> <code>n_layers</code> <code>int</code> <p>Number of hidden layers. Must be &gt;=1.</p> <code>5</code> <code>loss_fn</code> <code>Union[str, Callable]</code> <p>Standard estimator configuration.</p> <code>'mse'</code> <code>optimizer_fn</code> <code>Union[str, Callable]</code> <p>Standard estimator configuration.</p> <code>'mse'</code> <code>lr</code> <code>Union[str, Callable]</code> <p>Standard estimator configuration.</p> <code>'mse'</code> <code>is_feature_incremental</code> <code>Union[str, Callable]</code> <p>Standard estimator configuration.</p> <code>'mse'</code> <code>device</code> <code>Union[str, Callable]</code> <p>Standard estimator configuration.</p> <code>'mse'</code> <code>seed</code> <code>Union[str, Callable]</code> <p>Standard estimator configuration.</p> <code>'mse'</code> <code>gradient_clip_value</code> <code>Union[str, Callable]</code> <p>Standard estimator configuration.</p> <code>'mse'</code> <code>**kwargs</code> <code>Union[str, Callable]</code> <p>Standard estimator configuration.</p> <code>'mse'</code> Notes <p>The use of <code>sigmoid</code> after each hidden layer can cause saturation; for deeper networks consider replacing with ReLU or GELU in a custom module.</p> <p>Examples:</p> <p>Streaming regression on the Bikes dataset (only numeric features kept). The exact MAE value may vary depending on library version and hardware::</p> <pre><code>&gt;&gt;&gt; import random, numpy as np, torch\n&gt;&gt;&gt; from torch import manual_seed\n&gt;&gt;&gt; from river import datasets, metrics\n&gt;&gt;&gt; from deep_river.regression.zoo import MultiLayerPerceptron\n&gt;&gt;&gt; _ = manual_seed(42); random.seed(42); np.random.seed(42)\n&gt;&gt;&gt; first_x, _ = next(iter(datasets.Bikes()))\n&gt;&gt;&gt; numeric_keys = sorted([k for k,v in first_x.items() if isinstance(v,(int,float))])\n&gt;&gt;&gt; reg = MultiLayerPerceptron(\n...     n_features=len(numeric_keys), n_width=8, n_layers=2,\n...     optimizer_fn='sgd', lr=1e-2, is_feature_incremental=True,\n... )\n&gt;&gt;&gt; mae = metrics.MAE()\n&gt;&gt;&gt; for i, (x, y) in enumerate(datasets.Bikes().take(200)):\n...     x_num = {k: x[k] for k in numeric_keys}\n...     if i &gt; 0:\n...         y_pred = reg.predict_one(x_num)\n...         mae.update(y, y_pred)\n...     reg.learn_one(x_num, y)\n&gt;&gt;&gt; assert 0.0 &lt;= mae.get() &lt; 20.0\n&gt;&gt;&gt; print(f\"MAE: {mae.get():.4f}\")  # doctest: +ELLIPSIS\nMAE: ...\n</code></pre> <p>Methods:</p> Name Description <code>clone</code> <p>Return a fresh estimator instance with (optionally) copied state.</p> <code>draw</code> <p>Render a (partial) computational graph of the wrapped model.</p> <code>load</code> <p>Load a previously saved estimator.</p> <code>predict_many</code> <p>Predict target values for multiple instances (returns single-column DataFrame).</p> <code>predict_one</code> <p>Predict target value for a single instance.</p> <code>save</code> <p>Persist the estimator (architecture, weights, optimiser &amp; runtime state).</p> Source code in <code>deep_river/regression/zoo.py</code> <pre><code>def __init__(\n    self,\n    n_features: int = 10,\n    n_width: int = 5,\n    n_layers: int = 5,\n    loss_fn: Union[str, Callable] = \"mse\",\n    optimizer_fn: Union[str, Type[optim.Optimizer]] = \"sgd\",\n    lr: float = 1e-3,\n    is_feature_incremental: bool = False,\n    device: str = \"cpu\",\n    seed: int = 42,\n    gradient_clip_value: float | None = None,\n    **kwargs,\n):\n    self.n_features = n_features\n    self.n_width = n_width\n    self.n_layers = n_layers\n    module = MultiLayerPerceptron.MLPModule(\n        n_features=n_features, n_layers=n_layers, n_width=n_width\n    )\n    if \"module\" in kwargs:\n        del kwargs[\"module\"]\n    super().__init__(\n        module=module,\n        loss_fn=loss_fn,\n        optimizer_fn=optimizer_fn,\n        is_feature_incremental=is_feature_incremental,\n        device=device,\n        lr=lr,\n        seed=seed,\n        gradient_clip_value=gradient_clip_value,\n        **kwargs,\n    )\n</code></pre>"},{"location":"reference/regression/zoo/#deep_river.regression.zoo.MultiLayerPerceptron.clone","title":"clone","text":"<pre><code>clone(\n    new_params=None,\n    include_attributes: bool = False,\n    copy_weights: bool = False,\n)\n</code></pre> <p>Return a fresh estimator instance with (optionally) copied state.</p> <p>Parameters:</p> Name Type Description Default <code>new_params</code> <code>dict | None</code> <p>Parameter overrides for the cloned instance.</p> <code>None</code> <code>include_attributes</code> <code>bool</code> <p>If True, runtime state (observed features, buffers) is also copied.</p> <code>False</code> <code>copy_weights</code> <code>bool</code> <p>If True, model weights are copied (otherwise the module is re\u2011initialised).</p> <code>False</code> Source code in <code>deep_river/base.py</code> <pre><code>def clone(\n    self,\n    new_params=None,\n    include_attributes: bool = False,\n    copy_weights: bool = False,\n):\n    \"\"\"Return a fresh estimator instance with (optionally) copied state.\n\n    Parameters\n    ----------\n    new_params : dict | None\n        Parameter overrides for the cloned instance.\n    include_attributes : bool, default=False\n        If True, runtime state (observed features, buffers) is also copied.\n    copy_weights : bool, default=False\n        If True, model weights are copied (otherwise the module is re\u2011initialised).\n    \"\"\"\n    new_params = new_params or {}\n    copy_weights = new_params.pop(\"copy_weights\", copy_weights)\n\n    params = {**self._get_all_init_params(), **new_params}\n\n    if \"module\" not in new_params:\n        params[\"module\"] = self._rebuild_module()\n\n    new_est = self.__class__(**self._filter_kwargs(self.__class__.__init__, params))\n\n    if copy_weights and hasattr(self.module, \"state_dict\"):\n        new_est.module.load_state_dict(self.module.state_dict())\n\n    if include_attributes:\n        new_est._restore_runtime_state(self._get_runtime_state())\n\n    return new_est\n</code></pre>"},{"location":"reference/regression/zoo/#deep_river.regression.zoo.MultiLayerPerceptron.draw","title":"draw","text":"<pre><code>draw()\n</code></pre> <p>Render a (partial) computational graph of the wrapped model.</p> <p>Imports <code>graphviz</code> and <code>torchviz</code> lazily. Raises an informative ImportError if the optional dependencies are not installed.</p> Source code in <code>deep_river/base.py</code> <pre><code>def draw(self):  # type: ignore[override]\n    \"\"\"Render a (partial) computational graph of the wrapped model.\n\n    Imports ``graphviz`` and ``torchviz`` lazily. Raises an informative\n    ImportError if the optional dependencies are not installed.\n    \"\"\"\n    try:  # pragma: no cover\n        from torchviz import make_dot  # type: ignore\n    except Exception as err:  # noqa: BLE001\n        raise ImportError(\n            \"graphviz and torchviz must be installed to draw the model.\"\n        ) from err\n\n    first_parameter = next(self.module.parameters())\n    input_shape = first_parameter.size()\n    y_pred = self.module(torch.rand(input_shape))\n    return make_dot(y_pred.mean(), params=dict(self.module.named_parameters()))\n</code></pre>"},{"location":"reference/regression/zoo/#deep_river.regression.zoo.MultiLayerPerceptron.load","title":"load  <code>classmethod</code>","text":"<pre><code>load(filepath: Union[str, Path])\n</code></pre> <p>Load a previously saved estimator.</p> <p>The method reconstructs the estimator class, its wrapped module, optimiser state and runtime information (feature names, buffers, etc.).</p> Source code in <code>deep_river/base.py</code> <pre><code>@classmethod\ndef load(cls, filepath: Union[str, Path]):\n    \"\"\"Load a previously saved estimator.\n\n    The method reconstructs the estimator class, its wrapped module, optimiser\n    state and runtime information (feature names, buffers, etc.).\n    \"\"\"\n    with open(filepath, \"rb\") as f:\n        state = pickle.load(f)\n\n    estimator_cls = cls._import_from_path(state[\"estimator_class\"])\n    init_params = state[\"init_params\"]\n\n    # Rebuild module if needed\n    if \"module\" in init_params and isinstance(init_params[\"module\"], dict):\n        module_info = init_params.pop(\"module\")\n        module_cls = cls._import_from_path(module_info[\"class\"])\n        module = module_cls(\n            **cls._filter_kwargs(module_cls.__init__, module_info[\"kwargs\"])\n        )\n        if state.get(\"model_state_dict\"):\n            module.load_state_dict(state[\"model_state_dict\"])\n        init_params[\"module\"] = module\n\n    estimator = estimator_cls(\n        **cls._filter_kwargs(estimator_cls.__init__, init_params)\n    )\n\n    if state.get(\"optimizer_state_dict\") and hasattr(estimator, \"optimizer\"):\n        try:\n            estimator.optimizer.load_state_dict(\n                state[\"optimizer_state_dict\"]  # type: ignore[arg-type]\n            )\n        except Exception:  # noqa: E722\n            pass\n\n    estimator._restore_runtime_state(state.get(\"runtime_state\", {}))\n    return estimator\n</code></pre>"},{"location":"reference/regression/zoo/#deep_river.regression.zoo.MultiLayerPerceptron.predict_many","title":"predict_many","text":"<pre><code>predict_many(X: DataFrame) -&gt; DataFrame\n</code></pre> <p>Predict target values for multiple instances (returns single-column DataFrame).</p> Source code in <code>deep_river/regression/regressor.py</code> <pre><code>def predict_many(self, X: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"Predict target values for multiple instances (returns single-column DataFrame).\"\"\"\n    self._update_observed_features(X)\n    x_t = self._df2tensor(X)\n    self.module.eval()\n    with torch.inference_mode():\n        y_preds = self.module(x_t)\n    return pd.DataFrame(y_preds if not y_preds.is_cuda else y_preds.cpu().numpy())\n</code></pre>"},{"location":"reference/regression/zoo/#deep_river.regression.zoo.MultiLayerPerceptron.predict_one","title":"predict_one","text":"<pre><code>predict_one(x: dict) -&gt; RegTarget\n</code></pre> <p>Predict target value for a single instance.</p> Source code in <code>deep_river/regression/regressor.py</code> <pre><code>def predict_one(self, x: dict) -&gt; RegTarget:\n    \"\"\"Predict target value for a single instance.\"\"\"\n    self._update_observed_features(x)\n    x_t = self._dict2tensor(x)\n    self.module.eval()\n    with torch.inference_mode():\n        y_pred = self.module(x_t).item()\n    return y_pred\n</code></pre>"},{"location":"reference/regression/zoo/#deep_river.regression.zoo.MultiLayerPerceptron.save","title":"save","text":"<pre><code>save(filepath: Union[str, Path]) -&gt; None\n</code></pre> <p>Persist the estimator (architecture, weights, optimiser &amp; runtime state).</p> <p>Parameters:</p> Name Type Description Default <code>filepath</code> <code>str | Path</code> <p>Destination file. Parent directories are created automatically.</p> required Source code in <code>deep_river/base.py</code> <pre><code>def save(self, filepath: Union[str, Path]) -&gt; None:\n    \"\"\"Persist the estimator (architecture, weights, optimiser &amp; runtime state).\n\n    Parameters\n    ----------\n    filepath : str | Path\n        Destination file. Parent directories are created automatically.\n    \"\"\"\n    filepath = Path(filepath)\n    filepath.parent.mkdir(parents=True, exist_ok=True)\n\n    state = {\n        \"estimator_class\": f\"{type(self).__module__}.{type(self).__name__}\",\n        \"init_params\": self._get_all_init_params(),\n        \"model_state_dict\": getattr(self.module, \"state_dict\", lambda: {})(),\n        \"optimizer_state_dict\": getattr(self.optimizer, \"state_dict\", lambda: {})(),\n        \"runtime_state\": self._get_runtime_state(),\n    }\n\n    with open(filepath, \"wb\") as f:\n        pickle.dump(state, f)\n</code></pre>"},{"location":"reference/regression/zoo/#deep_river.regression.zoo.RNNRegressor","title":"RNNRegressor","text":"<pre><code>RNNRegressor(\n    n_features: int = 10,\n    hidden_size: int = 32,\n    num_layers: int = 1,\n    nonlinearity: str = \"tanh\",\n    dropout: float = 0.0,\n    gradient_clip_value: float | None = 1.0,\n    loss_fn: Union[str, Callable] = \"mse\",\n    optimizer_fn: Union[str, Type[Optimizer]] = \"adam\",\n    lr: float = 0.001,\n    is_feature_incremental: bool = False,\n    device: str = \"cpu\",\n    seed: int = 42,\n    **kwargs\n)\n</code></pre> <p>               Bases: <code>RollingRegressor</code></p> <p>Rolling RNN regressor for sequential / time-series data.</p> <p>Uses a <code>nn.RNN</code> backbone and a linear head to output a single continuous target. Leverages the rolling window maintained by :class:<code>RollingRegressor</code> to feed the last <code>window_size</code> observations as a sequence.</p> <p>Parameters:</p> Name Type Description Default <code>n_features</code> <code>int</code> <p>Number of input features per timestep.</p> <code>10</code> <code>hidden_size</code> <code>int</code> <p>Hidden state dimensionality of the RNN.</p> <code>32</code> <code>num_layers</code> <code>int</code> <p>Number of stacked RNN layers.</p> <code>1</code> <code>nonlinearity</code> <code>str</code> <p>Non-linearity used inside the RNN (<code>'tanh'</code> or <code>'relu'</code>).</p> <code>'tanh'</code> <code>dropout</code> <code>float</code> <p>Dropout applied after extracting the last hidden state (no internal RNN dropout).</p> <code>0.0</code> <code>gradient_clip_value</code> <code>float | None</code> <p>Gradient norm clipping threshold. <code>None</code> disables clipping.</p> <code>1.0</code> <code>loss_fn</code> <code>Union[str, Callable]</code> <p>Standard configuration as in other regressors.</p> <code>'mse'</code> <code>optimizer_fn</code> <code>Union[str, Callable]</code> <p>Standard configuration as in other regressors.</p> <code>'mse'</code> <code>lr</code> <code>Union[str, Callable]</code> <p>Standard configuration as in other regressors.</p> <code>'mse'</code> <code>is_feature_incremental</code> <code>Union[str, Callable]</code> <p>Standard configuration as in other regressors.</p> <code>'mse'</code> <code>device</code> <code>Union[str, Callable]</code> <p>Standard configuration as in other regressors.</p> <code>'mse'</code> <code>seed</code> <code>Union[str, Callable]</code> <p>Standard configuration as in other regressors.</p> <code>'mse'</code> <code>**kwargs</code> <code>Union[str, Callable]</code> <p>Standard configuration as in other regressors.</p> <code>'mse'</code> <p>Examples:</p> <p>Streaming regression on the Bikes dataset (only numeric features kept). The exact MAE value may vary depending on library version and hardware::</p> <pre><code>&gt;&gt;&gt; import random, numpy as np, torch\n&gt;&gt;&gt; from torch import manual_seed\n&gt;&gt;&gt; from river import datasets, metrics\n&gt;&gt;&gt; from deep_river.regression.zoo import RNNRegressor\n&gt;&gt;&gt; _ = manual_seed(42); random.seed(42); np.random.seed(42)\n&gt;&gt;&gt; first_x, _ = next(iter(datasets.Bikes()))\n&gt;&gt;&gt; numeric_keys = sorted([k for k,v in first_x.items() if isinstance(v,(int,float))])\n&gt;&gt;&gt; reg = RNNRegressor(\n...     n_features=len(numeric_keys), hidden_size=8, num_layers=1,\n...     optimizer_fn='sgd', lr=1e-2, is_feature_incremental=True,\n... )\n&gt;&gt;&gt; mae = metrics.MAE()\n&gt;&gt;&gt; for i, (x, y) in enumerate(datasets.Bikes().take(200)):\n...     x_num = {k: x[k] for k in numeric_keys}\n...     if i &gt; 0:\n...         y_pred = reg.predict_one(x_num)\n...         mae.update(y, y_pred)\n...     reg.learn_one(x_num, y)\n&gt;&gt;&gt; assert 0.0 &lt;= mae.get() &lt; 20.0\n&gt;&gt;&gt; print(f\"MAE: {mae.get():.4f}\")  # doctest: +ELLIPSIS\nMAE: ...\n</code></pre> <p>Methods:</p> Name Description <code>clone</code> <p>Return a fresh estimator instance with (optionally) copied state.</p> <code>draw</code> <p>Render a (partial) computational graph of the wrapped model.</p> <code>learn_many</code> <p>Batch update with multiple samples using the rolling window.</p> <code>learn_one</code> <p>Update model using a single (x, y) and current rolling window.</p> <code>load</code> <p>Load a previously saved estimator.</p> <code>predict_many</code> <p>Predict targets for multiple samples (appends to a copy of the window).</p> <code>predict_one</code> <p>Predict a single regression target using rolling context.</p> <code>save</code> <p>Persist the estimator (architecture, weights, optimiser &amp; runtime state).</p> Source code in <code>deep_river/regression/zoo.py</code> <pre><code>def __init__(\n    self,\n    n_features: int = 10,\n    hidden_size: int = 32,\n    num_layers: int = 1,\n    nonlinearity: str = \"tanh\",\n    dropout: float = 0.0,\n    gradient_clip_value: float | None = 1.0,\n    loss_fn: Union[str, Callable] = \"mse\",\n    optimizer_fn: Union[str, Type[optim.Optimizer]] = \"adam\",\n    lr: float = 1e-3,\n    is_feature_incremental: bool = False,\n    device: str = \"cpu\",\n    seed: int = 42,\n    **kwargs,\n):\n    self.n_features = n_features\n    self.hidden_size = hidden_size\n    self.num_layers = num_layers\n    self.nonlinearity = nonlinearity\n    self.dropout = dropout\n    module = RNNRegressor.RNNModule(\n        n_features=n_features,\n        hidden_size=hidden_size,\n        num_layers=num_layers,\n        nonlinearity=nonlinearity,\n        dropout=dropout,\n    )\n    if \"module\" in kwargs:\n        del kwargs[\"module\"]\n    super().__init__(\n        module=module,\n        loss_fn=loss_fn,\n        optimizer_fn=optimizer_fn,\n        is_feature_incremental=is_feature_incremental,\n        device=device,\n        lr=lr,\n        seed=seed,\n        gradient_clip_value=gradient_clip_value,\n        **kwargs,\n    )\n</code></pre>"},{"location":"reference/regression/zoo/#deep_river.regression.zoo.RNNRegressor.clone","title":"clone","text":"<pre><code>clone(\n    new_params=None,\n    include_attributes: bool = False,\n    copy_weights: bool = False,\n)\n</code></pre> <p>Return a fresh estimator instance with (optionally) copied state.</p> <p>Parameters:</p> Name Type Description Default <code>new_params</code> <code>dict | None</code> <p>Parameter overrides for the cloned instance.</p> <code>None</code> <code>include_attributes</code> <code>bool</code> <p>If True, runtime state (observed features, buffers) is also copied.</p> <code>False</code> <code>copy_weights</code> <code>bool</code> <p>If True, model weights are copied (otherwise the module is re\u2011initialised).</p> <code>False</code> Source code in <code>deep_river/base.py</code> <pre><code>def clone(\n    self,\n    new_params=None,\n    include_attributes: bool = False,\n    copy_weights: bool = False,\n):\n    \"\"\"Return a fresh estimator instance with (optionally) copied state.\n\n    Parameters\n    ----------\n    new_params : dict | None\n        Parameter overrides for the cloned instance.\n    include_attributes : bool, default=False\n        If True, runtime state (observed features, buffers) is also copied.\n    copy_weights : bool, default=False\n        If True, model weights are copied (otherwise the module is re\u2011initialised).\n    \"\"\"\n    new_params = new_params or {}\n    copy_weights = new_params.pop(\"copy_weights\", copy_weights)\n\n    params = {**self._get_all_init_params(), **new_params}\n\n    if \"module\" not in new_params:\n        params[\"module\"] = self._rebuild_module()\n\n    new_est = self.__class__(**self._filter_kwargs(self.__class__.__init__, params))\n\n    if copy_weights and hasattr(self.module, \"state_dict\"):\n        new_est.module.load_state_dict(self.module.state_dict())\n\n    if include_attributes:\n        new_est._restore_runtime_state(self._get_runtime_state())\n\n    return new_est\n</code></pre>"},{"location":"reference/regression/zoo/#deep_river.regression.zoo.RNNRegressor.draw","title":"draw","text":"<pre><code>draw()\n</code></pre> <p>Render a (partial) computational graph of the wrapped model.</p> <p>Imports <code>graphviz</code> and <code>torchviz</code> lazily. Raises an informative ImportError if the optional dependencies are not installed.</p> Source code in <code>deep_river/base.py</code> <pre><code>def draw(self):  # type: ignore[override]\n    \"\"\"Render a (partial) computational graph of the wrapped model.\n\n    Imports ``graphviz`` and ``torchviz`` lazily. Raises an informative\n    ImportError if the optional dependencies are not installed.\n    \"\"\"\n    try:  # pragma: no cover\n        from torchviz import make_dot  # type: ignore\n    except Exception as err:  # noqa: BLE001\n        raise ImportError(\n            \"graphviz and torchviz must be installed to draw the model.\"\n        ) from err\n\n    first_parameter = next(self.module.parameters())\n    input_shape = first_parameter.size()\n    y_pred = self.module(torch.rand(input_shape))\n    return make_dot(y_pred.mean(), params=dict(self.module.named_parameters()))\n</code></pre>"},{"location":"reference/regression/zoo/#deep_river.regression.zoo.RNNRegressor.learn_many","title":"learn_many","text":"<pre><code>learn_many(X: DataFrame, y: Series) -&gt; None\n</code></pre> <p>Batch update with multiple samples using the rolling window.</p> <p>Only performs an optimisation step once the internal window has reached <code>window_size</code> length to ensure a full sequence is available.</p> Source code in <code>deep_river/regression/rolling_regressor.py</code> <pre><code>def learn_many(self, X: pd.DataFrame, y: pd.Series) -&gt; None:\n    \"\"\"Batch update with multiple samples using the rolling window.\n\n    Only performs an optimisation step once the internal window has reached\n    ``window_size`` length to ensure a full sequence is available.\n    \"\"\"\n    self._update_observed_features(X)\n\n    X = X[list(self.observed_features)]\n    self._x_window.extend(X.values.tolist())\n\n    if len(self._x_window) == self.window_size:\n        X_t = self._deque2rolling_tensor(self._x_window)\n\n        # Convert y to tensor (ensuring proper shape for regression)\n        y_t = torch.tensor(y.values, dtype=torch.float32, device=self.device).view(\n            -1, 1\n        )\n\n        self._learn(x=X_t, y=y_t)\n</code></pre>"},{"location":"reference/regression/zoo/#deep_river.regression.zoo.RNNRegressor.learn_one","title":"learn_one","text":"<pre><code>learn_one(x: dict, y: RegTarget, **kwargs) -&gt; None\n</code></pre> <p>Update model using a single (x, y) and current rolling window.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>dict</code> <p>Feature mapping.</p> required <code>y</code> <code>float</code> <p>Target value.</p> required Source code in <code>deep_river/regression/rolling_regressor.py</code> <pre><code>def learn_one(self, x: dict, y: base.typing.RegTarget, **kwargs) -&gt; None:\n    \"\"\"Update model using a single (x, y) and current rolling window.\n\n    Parameters\n    ----------\n    x : dict\n        Feature mapping.\n    y : float\n        Target value.\n    \"\"\"\n    self._update_observed_features(x)\n\n    self._x_window.append([x.get(feature, 0) for feature in self.observed_features])\n\n    x_t = self._deque2rolling_tensor(self._x_window)\n\n    # Convert y to tensor (ensuring proper shape for regression)\n    y_t = torch.tensor([y], dtype=torch.float32, device=self.device).view(-1, 1)\n\n    self._learn(x=x_t, y=y_t)\n</code></pre>"},{"location":"reference/regression/zoo/#deep_river.regression.zoo.RNNRegressor.load","title":"load  <code>classmethod</code>","text":"<pre><code>load(filepath: Union[str, Path])\n</code></pre> <p>Load a previously saved estimator.</p> <p>The method reconstructs the estimator class, its wrapped module, optimiser state and runtime information (feature names, buffers, etc.).</p> Source code in <code>deep_river/base.py</code> <pre><code>@classmethod\ndef load(cls, filepath: Union[str, Path]):\n    \"\"\"Load a previously saved estimator.\n\n    The method reconstructs the estimator class, its wrapped module, optimiser\n    state and runtime information (feature names, buffers, etc.).\n    \"\"\"\n    with open(filepath, \"rb\") as f:\n        state = pickle.load(f)\n\n    estimator_cls = cls._import_from_path(state[\"estimator_class\"])\n    init_params = state[\"init_params\"]\n\n    # Rebuild module if needed\n    if \"module\" in init_params and isinstance(init_params[\"module\"], dict):\n        module_info = init_params.pop(\"module\")\n        module_cls = cls._import_from_path(module_info[\"class\"])\n        module = module_cls(\n            **cls._filter_kwargs(module_cls.__init__, module_info[\"kwargs\"])\n        )\n        if state.get(\"model_state_dict\"):\n            module.load_state_dict(state[\"model_state_dict\"])\n        init_params[\"module\"] = module\n\n    estimator = estimator_cls(\n        **cls._filter_kwargs(estimator_cls.__init__, init_params)\n    )\n\n    if state.get(\"optimizer_state_dict\") and hasattr(estimator, \"optimizer\"):\n        try:\n            estimator.optimizer.load_state_dict(\n                state[\"optimizer_state_dict\"]  # type: ignore[arg-type]\n            )\n        except Exception:  # noqa: E722\n            pass\n\n    estimator._restore_runtime_state(state.get(\"runtime_state\", {}))\n    return estimator\n</code></pre>"},{"location":"reference/regression/zoo/#deep_river.regression.zoo.RNNRegressor.predict_many","title":"predict_many","text":"<pre><code>predict_many(X: DataFrame) -&gt; DataFrame\n</code></pre> <p>Predict targets for multiple samples (appends to a copy of the window).</p> <p>Returns a single-column DataFrame named <code>'y_pred'</code>.</p> Source code in <code>deep_river/regression/rolling_regressor.py</code> <pre><code>def predict_many(self, X: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"Predict targets for multiple samples (appends to a copy of the window).\n\n    Returns a single-column DataFrame named ``'y_pred'``.\n    \"\"\"\n\n    self._update_observed_features(X)\n    X = X[list(self.observed_features)]\n    x_win = self._x_window.copy()\n    x_win.extend(X.values.tolist())\n    if self.append_predict:\n        self._x_window = x_win\n\n    self.module.eval()\n    with torch.inference_mode():\n        x_t = self._deque2rolling_tensor(x_win)\n        y_preds = self.module(x_t)\n        if isinstance(y_preds, torch.Tensor):\n            y_preds = y_preds.detach().cpu().view(-1).numpy().tolist()\n\n    return pd.DataFrame({\"y_pred\": y_preds})\n</code></pre>"},{"location":"reference/regression/zoo/#deep_river.regression.zoo.RNNRegressor.predict_one","title":"predict_one","text":"<pre><code>predict_one(x: dict) -&gt; RegTarget\n</code></pre> <p>Predict a single regression target using rolling context.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>dict</code> <p>Feature mapping.</p> required <p>Returns:</p> Type Description <code>float</code> <p>Predicted target value.</p> Source code in <code>deep_river/regression/rolling_regressor.py</code> <pre><code>def predict_one(self, x: dict) -&gt; base.typing.RegTarget:\n    \"\"\"Predict a single regression target using rolling context.\n\n    Parameters\n    ----------\n    x : dict\n        Feature mapping.\n\n    Returns\n    -------\n    float\n        Predicted target value.\n    \"\"\"\n    self._update_observed_features(x)\n\n    x_win = self._x_window.copy()\n    x_win.append([x.get(feature, 0) for feature in self.observed_features])\n    if self.append_predict:\n        self._x_window = x_win\n\n    self.module.eval()\n    with torch.inference_mode():\n        x_t = self._deque2rolling_tensor(x_win)\n        y_pred = self.module(x_t)\n        if isinstance(y_pred, torch.Tensor):\n            y_pred = y_pred.detach().view(-1)[-1].cpu().numpy().item()\n        else:\n            y_pred = float(y_pred)\n\n    return y_pred\n</code></pre>"},{"location":"reference/regression/zoo/#deep_river.regression.zoo.RNNRegressor.save","title":"save","text":"<pre><code>save(filepath: Union[str, Path]) -&gt; None\n</code></pre> <p>Persist the estimator (architecture, weights, optimiser &amp; runtime state).</p> <p>Parameters:</p> Name Type Description Default <code>filepath</code> <code>str | Path</code> <p>Destination file. Parent directories are created automatically.</p> required Source code in <code>deep_river/base.py</code> <pre><code>def save(self, filepath: Union[str, Path]) -&gt; None:\n    \"\"\"Persist the estimator (architecture, weights, optimiser &amp; runtime state).\n\n    Parameters\n    ----------\n    filepath : str | Path\n        Destination file. Parent directories are created automatically.\n    \"\"\"\n    filepath = Path(filepath)\n    filepath.parent.mkdir(parents=True, exist_ok=True)\n\n    state = {\n        \"estimator_class\": f\"{type(self).__module__}.{type(self).__name__}\",\n        \"init_params\": self._get_all_init_params(),\n        \"model_state_dict\": getattr(self.module, \"state_dict\", lambda: {})(),\n        \"optimizer_state_dict\": getattr(self.optimizer, \"state_dict\", lambda: {})(),\n        \"runtime_state\": self._get_runtime_state(),\n    }\n\n    with open(filepath, \"wb\") as f:\n        pickle.dump(state, f)\n</code></pre>"}]}